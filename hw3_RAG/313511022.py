# -*- coding: utf-8 -*-
"""HW3_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yV7w9wQpdPGUjlAbw7MNsTfB4cmz4iSI
"""

!pip install pandas==2.2.3 jupyter==1.1.1 langchain==0.3.23 langchain-community==0.3.21 rich==14.0.0 openai==1.71.0 langchain-groq==0.3.2 langchain-ollama==0.3.1 faiss-gpu==1.7.2 numpy<2 rouge-score

!pip install langchain-ollama bitsandbytes

!pip install -q faiss-cpu sentence-transformers transformers

!pip install rank_bm25

!pip install nltk==3.7

import logging
import json

from rich.console import Console
from rich.logging import RichHandler

console = Console(stderr=True, record=True)
log_handler = RichHandler(rich_tracebacks=True, console=console, markup=True)
logging.basicConfig(format="%(message)s",datefmt="[%X]",handlers=[log_handler])
log = logging.getLogger("rich")
# log.setLevel(logging.INFO)
log.setLevel(logging.DEBUG)

DEBUG: bool = False
DATASET_PATH: str = "/content/drive/MyDrive/GAI_HW3/public_dataset.json"

# HuggingFace model
# USING_MODEL: str = "microsoft/Phi-4-mini-instruct"
USING_MODEL: str = "FINGU-AI/Chocolatine-Fusion-14B"

MODEL_TEMPERATURE: float = 0.3
MODEL_MAX_TOKENS: int = 128
RETRIEVE_TOP_K: int = 5

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# 手動載 tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    USING_MODEL,
    trust_remote_code=True
)

# 手動載 model，這裡可以加 max_memory
model = AutoModelForCausalLM.from_pretrained(
    USING_MODEL,
    device_map="auto",
    trust_remote_code=True,
    max_memory={0: "35GiB", "cpu": "30GiB"}
)

# 再自己建 pipeline
qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=MODEL_MAX_TOKENS,
    # temperature=MODEL_TEMPERATURE,
    # do_sample=True
    do_sample=False
)

def call_llm(prompt: str) -> str:
    response = qa_pipeline(prompt)
    return response[0]["generated_text"].strip()

def make_prompt(title: str, question: str, evidence: list[str]) -> str:
    evidence_blocks = "\n\n".join(
        f"Evidence [{i}]:\n{ev}" for i, ev in enumerate(evidence)
    )

    prompt = (
        "You are a natural language processing (NLP) expert.\n"
        "Answer the following question based only on the provided evidence. "
        "If an evidence is not relevant to the question, simply ignore it. "
        "Use only the relevant evidence to compose your answer.\n"
        "Provide your answer in a concise paragraph (2-3 sentences).\n"
        "The evidence is sourced from a research paper.\n\n"
        f"Paper title:\n{title}\n\n"
        f"{evidence_blocks}\n\n"
        f"Question:\n{question}\n\n"
        "Answer:"
    )

    return prompt

with open(DATASET_PATH, "r") as f:
  dataset = json.load(f)

demo_id = 5
demo_title = dataset[demo_id]["title"]
demo_full_text = dataset[demo_id]["full_text"]
demo_question = dataset[demo_id]["question"]
demo_answer = dataset[demo_id]["answer"]
demo_evidence = dataset[demo_id]["evidence"]

# display(dataset[demo_id])
print(f'title: {demo_title}')
print(f'question: {demo_question}')
print(f'answer: {demo_answer}')
print(f'evidence: {demo_evidence}')
# print(f'\nFull text: \n{demo_full_text}')

SEMANTIC_SCORE_PROMPT = """
You are a natural language processing (NLP) expert.

Given the following question and evidence:

Question:
{question}

Evidence:
{evidence}

Instruction:
- If the evidence can help answer the question, output 1.
- If the evidence cannot help answer the question, output 0.

Please follow strictly the output format:
Score (0 or 1): <number>
"""


import re

def extract_score(text):
    match = re.search(r"Score\s*\(1-5\)\s*[:：]?\s*(\d+)", text)
    if match:
        score = int(match.group(1))
        return max(0, min(1, score))  # 保險：強制在 1~5 之間
    else:
        return 0  # 如果模型亂回答，保底最差是1分

def score_evidence(question: str, evidence: str) -> int:
    prompt = SEMANTIC_SCORE_PROMPT.format(question=question, evidence=evidence)
    response = qa_pipeline(prompt)[0]["generated_text"]
    try:
        score = extract_score(response)
        print(f"Evidence Score: {score}")
        return score
    except Exception as e:
        print(f"Error parsing score: {e}")
        print(response)
        return 1  # fallback

from typing import List

def rerank_evidences(question: str, evidences: List[str], top_n=5) -> List[str]:
    scored = []
    for ev in evidences:
        score = score_evidence(question, ev)
        scored.append((ev, score))

    # Sort by score descending
    scored = sorted(scored, key=lambda x: x[1], reverse=True)

    # Keep only those with score >= 4
    top_evidences = [ev for ev, sc in scored if sc >= 1]

    # Limit to top_n evidences
    return top_evidences

from pathlib import Path
import json
import faiss
import torch
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import numpy as np
from typing import List, Dict
from langchain.docstore.document import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 正確下載 punkt
import nltk
nltk.data.path.clear()
nltk.data.path.append("/content/nltk_data")
nltk.download('punkt', download_dir='/content/nltk_data')

# 使用
from nltk.tokenize import sent_tokenize

# Download sentence tokenizer (only needed once)
nltk.download('punkt')

# Settings
CHUNK_SIZE = 256
CHUNK_OVERLAP = 64
RETRIEVE_TOP_K = 5
final_k = 3
EMBED_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"

# Load your full text
documents = demo_full_text.split("\n\n\n")[:-1]

# Split into documents
docs = [Document(page_content=doc) for doc in documents]

# Initialize splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", ". ", " "]
)

# Corpus and Metadata
corpus = []
metadata = []

# Process each document
for doc_id, d in enumerate(docs):
    # --- Split into sentences ---
    sentences = sent_tokenize(d.page_content)

    # --- Join sentences back to form the full doc ---
    full_text = " ".join(sentences)

    # --- Chunking ---
    chunks = text_splitter.split_text(full_text)
    # chunks = sentences

    # --- For each chunk, find which sentences it contains ---
    for chunk in chunks:
        included_sentences = []
        for sent in sentences:
            if chunk in sent:
                included_sentences.append(sent.strip())

        corpus.append(chunk)
        metadata.append({
            "doc_id": doc_id,
            "sentences": included_sentences,   # 💬 Original sentences inside this chunk
            "chunk_text": chunk                 # The actual chunk
        })

# --- Build FAISS index ---
embedder = SentenceTransformer(EMBED_MODEL_NAME)
corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True, show_progress_bar=True)

dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(corpus_embeddings)

print("Finished building index with sentence-level tracking!")

from rank_bm25 import BM25Okapi

# Tokenize your corpus
tokenized_corpus = [chunk.split() for chunk in corpus]

# Build BM25 index
bm25 = BM25Okapi(tokenized_corpus)

def dense_retrieve(query: str, embedder, index, corpus: List[str], metadata: List[Dict], top_k=RETRIEVE_TOP_K):
    query_embedding = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(query_embedding, top_k)
    results = [(metadata[i], D[0][j]) for j, i in enumerate(I[0]) if i != -1]
    return results  # List of (metadata, distance)

def sparse_retrieve(query: str, bm25: BM25Okapi, corpus: List[str], metadata: List[Dict], top_k=RETRIEVE_TOP_K):
    tokenized_query = query.split()
    scores = bm25.get_scores(tokenized_query)
    top_k_indices = np.argsort(scores)[::-1][:top_k]
    results = [(metadata[i], scores[i]) for i in top_k_indices]
    return results  # List of (metadata, BM25 score)

def hybrid_retrieve(query: str, embedder, index, bm25, corpus: List[str], metadata: List[Dict], top_k=RETRIEVE_TOP_K):
    dense_results = dense_retrieve(query, embedder, index, corpus, metadata, top_k)
    sparse_results = sparse_retrieve(query, bm25, corpus, metadata, top_k)

    # Merge by simple union
    all_results = dense_results + sparse_results

    # Deduplicate based on chunk_text
    seen_chunks = set()
    merged = []
    for meta, score in all_results:
        chunk_text = meta["chunk_text"]
        if chunk_text not in seen_chunks:
            merged.append((meta, score))
            seen_chunks.add(chunk_text)

    return merged  # List of (metadata, score)

query = demo_question

retrieved = hybrid_retrieve(query, embedder, index, bm25, corpus, metadata, 5)

final_evidences = []

for meta, score in retrieved:
    if meta["sentences"]:  # If sentences list is not empty
        final_evidences.extend(meta["sentences"])
    else:  # If no sentences recorded
        final_evidences.append(meta["chunk_text"])

print("Final Evidence List:")
for idx, ev in enumerate(final_evidences, 1):
    print(f"Evidence [{idx}]: {ev}")

retrieved = hybrid_retrieve(demo_question, embedder, index, bm25, corpus, metadata, RETRIEVE_TOP_K)

final_evidences = []

for meta, score in retrieved:
    if meta["sentences"]:  # If sentences list is not empty
        final_evidences.extend(meta["sentences"])
    else:  # If no sentences recorded
        final_evidences.append(meta["chunk_text"])

print("Final Evidence List:")
for idx, ev in enumerate(final_evidences, 1):
    print(f"Evidence [{idx}]: {ev}")

# e = rerank_evidences(demo_question, final_evidences, top_n=5)

# prompt = make_prompt(demo_title, demo_question, using_evidence)
prompt = make_prompt(demo_title, demo_question, final_evidences)
answer = call_llm(prompt)

print("模型回答:", answer)
import re

def extract_answer(text: str) -> str:
    match = re.search(r"Answer:\s*(.*)", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    else:
        return ""

ATTRIBUTION_PROMPT = """
You are a natural language processing (NLP) expert.

You are given a list of evidences and an answer.

Please carefully read the answer and the evidences.

Task:
- Identify which evidence(s) directly support the answer.
- Return a list of the evidence numbers that are relevant.

Format:
Evidence used: [list of numbers]

Example:
Evidence used: [1, 3, 5]

---

Evidences:
{evidence_list}

Answer:
{answer}
"""
# Build evidence list
evidence_list = "\n".join(
    [f"Evidence [{i}]: {ev}" for i, ev in enumerate(final_evidences)]
)

your_answer_from_first_step = extract_answer(answer)

# Build attribution prompt
attribution_prompt = ATTRIBUTION_PROMPT.format(
    evidence_list=evidence_list,
    answer=your_answer_from_first_step
)

# Send to LLM again
attribution_response = qa_pipeline(attribution_prompt)[0]["generated_text"]

print(attribution_response)

import re

def extract_answer_and_evidence(text: str):
    # Extract Answer
    answer_match = re.search(r"Answer:\s*(.*?)\s*Evidence used:", text, re.DOTALL)
    answer = answer_match.group(1).strip() if answer_match else ""

    # Extract Evidence numbers: find all digits inside square brackets
    evidence_numbers = re.findall(r"Evidence used:\s*\[([0-9,\s]*)\]", text)
    if evidence_numbers:
        numbers = evidence_numbers[-1]  # take first match
        evidence_list = [int(num.strip()) for num in numbers.split(',') if num.strip()]
    else:
        evidence_list = []

    return evidence_list

def select_used_evidences(evidence_list, final_evidences):
    selected_evidences = []
    for idx in evidence_list:
        if 0 <= idx <= len(final_evidences):
            selected_evidences.append(final_evidences[idx])  # 注意 evidence_list 是從1開始
    return selected_evidences


evidence_list = extract_answer_and_evidence(attribution_response)
selected = select_used_evidences(evidence_list, final_evidences)

prompt = make_prompt(demo_title, demo_question, selected)
answer = call_llm(prompt)

print("模型回答:", answer)

TEST_DATASET_PATH = "/content/drive/MyDrive/GAI_HW3/private_dataset.json"
with open(TEST_DATASET_PATH, "r") as f:
  test_dataset = json.load(f)

predictions = []

for sample in test_dataset:
  demo_title = sample["title"]
  demo_full_text = sample["full_text"]
  demo_question = sample["question"]
  print(f'title: {demo_title}')
  print(f'question: {demo_question}')
  # Settings
  CHUNK_SIZE = 256
  CHUNK_OVERLAP = 64
  RETRIEVE_TOP_K = 8
  final_k = 3
  EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

  # Load your full text
  documents = demo_full_text.split("\n\n\n")[:-1]

  # Split into documents
  docs = [Document(page_content=doc) for doc in documents]

  # Initialize splitter
  text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=CHUNK_SIZE,
      chunk_overlap=CHUNK_OVERLAP,
      separators=["\n\n", "\n", ". ", " "]
  )

  # Corpus and Metadata
  corpus = []
  metadata = []

  # Process each document
  for doc_id, d in enumerate(docs):
      # --- Split into sentences ---
      sentences = sent_tokenize(d.page_content)

      # --- Join sentences back to form the full doc ---
      full_text = " ".join(sentences)

      # --- Chunking ---
      chunks = text_splitter.split_text(full_text)
      # chunks = sentences

      # --- For each chunk, find which sentences it contains ---
      for chunk in chunks:
          included_sentences = []
          for sent in sentences:
              if chunk in sent:
                  included_sentences.append(sent.strip())

          corpus.append(chunk)
          metadata.append({
              "doc_id": doc_id,
              "sentences": included_sentences,   # 💬 Original sentences inside this chunk
              "chunk_text": chunk                 # The actual chunk
          })

  # --- Build FAISS index ---
  embedder = SentenceTransformer(EMBED_MODEL_NAME)
  corpus_embeddings = embedder.encode(corpus, convert_to_numpy=True, show_progress_bar=True)

  dimension = corpus_embeddings.shape[1]
  index = faiss.IndexFlatL2(dimension)
  index.add(corpus_embeddings)

  print("Finished building index with sentence-level tracking!")

  # Tokenize your corpus
  tokenized_corpus = [chunk.split() for chunk in corpus]

  # Build BM25 index
  bm25 = BM25Okapi(tokenized_corpus)

  retrieved = hybrid_retrieve(demo_question, embedder, index, bm25, corpus, metadata, RETRIEVE_TOP_K)

  final_evidences = []

  for meta, score in retrieved:
      if meta["sentences"]:  # If sentences list is not empty
          final_evidences.extend(meta["sentences"])
      else:  # If no sentences recorded
          final_evidences.append(meta["chunk_text"])

  print("Final Evidence List:")
  for idx, ev in enumerate(final_evidences, 1):
      print(f"Evidence [{idx}]: {ev}")

  # e = rerank_evidences(demo_question, final_evidences, top_n=5)

  # prompt = make_prompt(demo_title, demo_question, using_evidence)
  prompt = make_prompt(demo_title, demo_question, final_evidences)
  answer_0 = call_llm(prompt)

  print("模型回答:", answer_0)
  import re

  def extract_answer(text: str) -> str:
      match = re.search(r"Answer:\s*(.*)", text, re.DOTALL)
      if match:
          return match.group(1).strip()
      else:
          return ""
  ATTRIBUTION_PROMPT = """
  You are a natural language processing (NLP) expert.

  You are given a list of evidences and an answer.

  Please carefully read the answer and the evidences.

  Task:
  - Identify which evidence(s) directly support the answer.
  - Return a list of the evidence numbers that are relevant.

  Format:
  Evidence used: [list of numbers]

  Example:
  Evidence used: [1, 3, 5]

  ---

  Evidences:
  {evidence_list}

  Answer:
  {answer}
  """
  # Build evidence list
  evidence_list = "\n".join(
      [f"Evidence [{i}]: {ev}" for i, ev in enumerate(final_evidences)]
  )

  your_answer_from_first_step = extract_answer(answer_0)

  # Build attribution prompt
  attribution_prompt = ATTRIBUTION_PROMPT.format(
      evidence_list=evidence_list,
      answer=your_answer_from_first_step
  )

  # Send to LLM again
  attribution_response = qa_pipeline(attribution_prompt)[0]["generated_text"]

  print(attribution_response)
  import re

  def extract_answer_and_evidence(text: str):
      # Extract Answer
      answer_match = re.search(r"Answer:\s*(.*?)\s*Evidence used:", text, re.DOTALL)
      answer = answer_match.group(1).strip() if answer_match else ""

      # Extract Evidence numbers: find all digits inside square brackets
      evidence_numbers = re.findall(r"Evidence used:\s*\[([0-9,\s]*)\]", text)
      if evidence_numbers:
          numbers = evidence_numbers[-1]  # take first match
          evidence_list = [int(num.strip()) for num in numbers.split(',') if num.strip()]
      else:
          evidence_list = []

      return evidence_list

  def select_used_evidences(evidence_list, final_evidences):
      selected_evidences = []
      for idx in evidence_list:
          if 0 <= idx <= len(final_evidences):
              selected_evidences.append(final_evidences[idx])  # 注意 evidence_list 是從1開始
      return selected_evidences


  evidence_list = extract_answer_and_evidence(attribution_response)
  selected = select_used_evidences(evidence_list, final_evidences)

  prompt = make_prompt(demo_title, demo_question, selected)
  answer = call_llm(prompt)

  a = extract_answer(answer)

  print("模型回答:", answer)

  predictions.append({
            "title": demo_title,
            "answer": a,
            "evidence": selected
        })
  torch.cuda.empty_cache()
  torch.cuda.ipc_collect()

with open("/content/drive/MyDrive/GAI_HW3/submission_v4.json", "w") as f:
    json.dump(predictions, f, indent=2, ensure_ascii=False)

print("✅ Finished! Your submission file is saved at /mnt/data/submission_v4.json")