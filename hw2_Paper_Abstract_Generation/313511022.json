{"paper_id": 408, "abstract": "Despite their impressive ability to synthesize high-fidelity images, Generative Adversarial Networks (GANs) face a significant challenge: they often struggle to maintain high precision when modeling multi-modal distributions. This issue stems from the fact that GANs inherently generate connected distributions, leading to regions within their support that fall outside the true data distribution. Consequently, these problematic areas can manifest as spurious samples during inference. To address this, we delve into the realm of Gaussian isoperimetric inequalities, a mathematical framework designed to determine the partitions of a Gaussian space with the least possible Gaussian-weighted perimeter. Building upon the groundbreaking work of Milman and Neeman (2022), we demonstrate that GANs with latent spaces structured as \"simplicial clusters\" can achieve optimal precision levels, effectively minimizing the measure of out-of-distribution samples. Through rigorous theoretical analysis, we establish both upper and lower bounds on the precision of such GANs, revealing that the gap between these bounds diminishes in the same order: √log m, where m represents the number of modes. Furthermore, we validate our findings through extensive experimentation, illustrating that GANs with higher performance metrics exhibit a latent space organization that aligns closely with the concept of simplicial clusters. In essence, this research not only illuminates a critical aspect of GAN behavior but also provides a pathway toward more precise and reliable generative modeling."}
{"paper_id": 409, "abstract": "In the realm of treatment effect estimation, where the goal is to predict how various interventions might influence outcomes, we confront a critical challenge: crafting a robust loss function that can effectively guide the learning of representations across a multitude of treatment options and dosage levels. In this paper, we embark on a journey to address this pressing issue by developing a novel loss function tailored for treatment effect estimation in settings that encompass multiple treatment types and continuous dosage parameters. Our approach hinges on the establishment of a bound on the generalization error—a bound that not only accounts for the factual error but also incorporates a term that quantifies the statistical interplay between treatment-dosage combinations and observed confounders. This groundbreaking method allows us to navigate the complexities of representation learning in treatment effect estimation without resorting to cumbersome combinatorial calculations or arbitrary binning of dosage values into discrete intervals. To validate our findings, we conduct a series of comprehensive experiments, comparing our method against a variety of baselines. These comparisons reveal that our proposed framework holds significant promise for advancing the field of treatment effect estimation, particularly in the context of multiple treatments and continuous dosages."}
{"paper_id": 410, "abstract": "[Introduction continued]\n\nThe rapid advancements in machine learning, fueled by groundbreaking innovations like transformers and generative models, have undeniably transformed the landscape of applied machine learning in chemistry and biology. These developments have not only enhanced our ability to tackle complex challenges but have also democratized access to powerful tools and techniques. However, the escalating demands for computational power and energy have begun to cast a shadow over the field, raising significant concerns across multiple fronts.\n\nFrom a socioeconomic perspective, the mounting costs of cutting-edge research threaten to exacerbate existing inequalities, creating barriers to entry for many researchers and institutions. This could lead to a concentration of resources and expertise in the hands of a few well-funded entities, potentially stifling innovation and diversity of thought. Moreover, the high financial burden might deter talented individuals from pursuing careers in applied machine learning, particularly if they lack access to substantial funding or infrastructure.\n\nScientifically, the increasing reliance on extensive computational resources risks overshadowing the importance of robust theoretical foundations. As researchers allocate more time and effort to optimizing algorithms and harnessing vast datasets, there is a real danger of neglecting the underlying principles that make machine learning effective. This shift could result in less reproducible research and a diminished emphasis on understanding the fundamental mechanisms at play. Furthermore, the focus on achieving impressive results through sheer computational power might divert attention away from exploring more efficient or innovative approaches that could yield greater insights.\n\nEnvironmental considerations add another layer of complexity to the issue. The substantial carbon footprint associated with large-scale machine learning projects raises critical questions about sustainability and ethical responsibility. As the field continues to grow, the environmental toll becomes increasingly significant, prompting us to consider whether the benefits of advanced machine learning outweigh its ecological costs. This calls for a reevaluation of priorities and a commitment to developing greener solutions that align with our broader goals of sustainability and responsible innovation.\n\nIn light of these multifaceted concerns, it is imperative that we engage in a thoughtful dialogue about the future trajectory of applied machine learning in chemistry and biology. By acknowledging and addressing the challenges posed by escalating costs, we can work toward a more inclusive, sustainable, and scientifically rigorous field that serves both the present and future needs of society.\n\n[Abstract]\n\nThe advent of machine learning has heralded a transformative era in the realms of chemistry and biology, with applications ranging from drug discovery to the deciphering of complex biological systems. Yet, amidst this exhilarating progress, a troubling pattern emerges: the relentless escalation in the computational resources demanded by cutting-edge research. This Abstract delves into the profound implications of this trend,"}
{"paper_id": 411, "abstract": "In the ever-evolving landscape of deep learning, explicit and implicit regularization stand as pillars of optimization, steering models toward enhanced performance. Among the myriad techniques, gradient regularization (GR) emerges as a beacon, leveraging the first-order gradient norm to guide training dynamics toward flatter minima and, consequently, superior generalization. Yet, despite its promise, the intricacies of GR remain shrouded in mystery. In this paper, we embark on a journey to unravel these complexities.  We begin by illuminating the practical advantages of employing finite-difference computations within GR, revealing their computational efficiency and surprising impact on generalization. Our analysis delves into the comparative costs of finite difference versus double backpropagation, uncovering a compelling narrative that resonates with the strengths of forward finite differences. These findings not only offer valuable insights but also pave the way for more effective learning strategies.  Furthermore, we delve into the theoretical underpinnings of GR, crafting a rigorous analysis through the lens of a diagonal linear network (DLN). In doing so, we uncover a profound relationship between gradient norms and the selection of optimal solutions. This revelation not only enhances our understanding of GR's impact on generalization but also reveals the critical role of forward finite differences in amplifying this effect.  Finally, we forge connections between GR and other learning paradigms, such as Sharpness-Aware Minimization (SAM) and the flooding method. Through this exploration, we unveil the inherent finite-difference nature of the flooding method, bridging disparate concepts and enriching our comprehension of regularization techniques.  In summary, this work seeks to illuminate the path forward in the realm of gradient regularization, offering both practical tools and theoretical frameworks to guide future advancements in deep learning."}
{"paper_id": 412, "abstract": "In the realm of machine learning, the unveiling of high-dimensional representations through t-SNE plots has become a cornerstone for understanding and showcasing model performance. Yet, beneath this veneer of transparency lies a perilous truth: these very plots can inadvertently divulge sensitive information about the underlying dataset. This revelation prompts a critical examination into the security vulnerabilities inherent in the publication of t-SNE plots. In this paper, we embark on a groundbreaking exploration of the privacy implications surrounding t-SNE plots, focusing on the potential for adversaries to infer global properties of the dataset. Our findings underscore the pressing need for a more nuanced approach to safeguarding the privacy of our data visualizations. Join us as we navigate the complexities of this emerging challenge and propose strategies to fortify the integrity of our t-SNE plots against unauthorized inference attacks."}
{"paper_id": 413, "abstract": "[BEGIN_ABSTRACT]\n\nIn the realm of machine learning, the specter of backdoor attacks looms large, threatening the integrity of trained models. Traditionally, these attacks have relied on a rare pattern—often a sticker or checkerboard—to masquerade as triggers, a strategy that simplifies the design process yet leaves the model vulnerable to detection. This approach, though seemingly clever, is fraught with pitfalls, as it allows defenders to easily identify and counteract the malevolent influence.\n\nIn this work, we unveil a novel strategy for crafting backdoor attacks, one that shifts the focus from rarity to frequency. By mining trigger patterns directly from benign samples that frequently appear in the target class but remain elusive in non-target classes, we forge a path toward greater stealth and effectiveness. Our method leverages a robust mechanism to embed these carefully chosen triggers into the very fabric of the dataset, ensuring that the poisoned samples blend seamlessly with the genuine crowd.\n\nWe rigorously test our innovative approach across four well-established datasets, and the results speak volumes. Not only does our strategy outperform conventional methods in terms of attack efficacy, but it also demonstrates remarkable resilience against a variety of state-of-the-art defense mechanisms. In essence, we have crafted a more formidable adversary—one that challenges the status quo and pushes the boundaries of what is possible in the ongoing battle for AI security.\n\n[BEGIN_REFERENCE]\n\nChen, Y., Sharma, Y., Zhang, H., Yi, J., & Hsieh, C.-J. (2019). Evasion and Poisoning Attacks against DNNs: A Dynamical System Viewpoint. arXiv preprint arXiv:1908.04564. Doan, T. N., Nguyen, T. Q., & Kim, S. (2021). Adversarial Defense via Data Augmentation. arXiv preprint arXiv:2106.15884. Gu, S., Rigazio, R., & Li, L. (2017). Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733. Huang, X., Hu, G., Liu, W., Wang, S., Chen, Y., & Zhang, X. (2022a). Backdoor Attack Detection via Neural Network Pruning. arXiv preprint arXiv:2201.029"}
{"paper_id": 414, "abstract": "[Benchmarks] The advent of deep learning has revolutionized the field, enabling breakthroughs in a wide array of domains. However, this progress comes with a significant cost: the ever-expanding size of deep learning models. As the complexity of these models grows, so too does the challenge of deploying them efficiently. To address this issue, numerous techniques have emerged, each aiming to compress deep learning models into more manageable forms. Among these, network binarization stands out as one of the most aggressive approaches, reducing the precision of weights and activations to just one bit. This extreme compression not only minimizes memory usage but also accelerates computation, making it an attractive option for practical applications. Yet, despite its promise, the evaluation of binarization algorithms remains fraught with challenges. The current state of the art often fails to provide a comprehensive assessment, focusing narrowly on specific tasks or architectures. Consequently, researchers struggle to determine the true capabilities of these algorithms across diverse scenarios, leading to uncertainty regarding their effectiveness in real-world deployments.  [Contributions] In light of these shortcomings, we introduce BiBench, a groundbreaking network binarization benchmark designed to offer a more complete and nuanced evaluation of binarization algorithms. Our benchmark encompasses multiple dimensions, including different learning tasks, neural architectures, corruption robustness, training efficiency, theoretical complexity, and hardware inference. By evaluating eight representative binarization algorithms across nine deep learning datasets, thirteen distinct neural architectures, two deployment libraries, fourteen hardware chips, and a variety of hyperparameter settings, we have compiled a comprehensive dataset of results. This exhaustive testing required nearly 4 GPU years of computational effort, underscoring the rigor and thoroughness of our approach.   [Key Findings] Through our extensive analysis, we have uncovered several critical insights that shed light on the strengths and limitations of current binarization algorithms. For instance, we observed that binarization methods exhibit varying degrees of success depending on the architecture and task at hand, highlighting the need for more tailored solutions. Additionally, our findings underscore the importance of considering robustness to data corruption, a factor that has been largely overlooked in previous evaluations. Furthermore, our exploration reveals that while binarization algorithms may offer impressive theoretical gains in terms of memory and computation, their practical benefits often fall short due to the lack of optimized hardware support. This gap between theory and practice is a key barrier to realizing the full potential of network binarization in real-world applications.  [Conclusion] In conclusion, BiBench serves as a vital resource for advancing the"}
{"paper_id": 415, "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the robustness of machine learning models, particularly deep neural networks (DNNs), which often struggle to generalize beyond the training data they have encountered. In this work, we introduce a novel approach to OOD detection based on the concept of \"trajectories.\" We define the trajectory of an input sample as its sequential progression through the layers of a DNN, encapsulated within a functional representation. By employing the probability-weighted projection method, we map the multivariate features of each layer into a trajectory space where the statistical properties of the input can be effectively analyzed. The key innovation lies in our redefinition of the OOD detection problem: we seek to identify samples whose trajectories deviate significantly from the normative paths established by the training set. Our method operates without the need for additional OOD or synthetic data, relying solely on the inherent structure of the training data. Through extensive empirical validation, we demonstrate the efficacy of our approach on a mid-sized OOD detection benchmark, showcasing its potential to enhance the reliability of DNNs in real-world applications."}
{"paper_id": 416, "abstract": "In the realm of image super-resolution, a fascinating challenge emerges when faced with unpaired data—how can we craft a high-resolution image from its low-resolution counterpart without the luxury of paired examples? Recent advancements have turned to Generative Adversarial Networks (GANs) as a beacon of hope, yet the path forward remains shrouded in uncertainty. In this work, we illuminate the landscape by examining GAN optimization objectives that are bolstered by content losses—a common practice in unpaired image super-resolution methods. Through rigorous analysis, we reveal that these objectives converge upon an optimal transport map, offering new insights into the nature of GAN training. Our theoretical findings are corroborated by empirical evidence, shedding light on the inherent biases that can arise in this process. Furthermore, we introduce an innovative algorithm designed to fit an unbiased OT map tailored to the perceptual transport cost, and we demonstrate its prowess in the unpaired image super-resolution task. This approach not only bridges the gap between theory and practice but also paves the way for future exploration in this exciting field."}
{"paper_id": 417, "abstract": "In this work, we delve into the realm of unsupervised domain generalization for 3D point clouds, specifically focusing on scenarios where the model must be trained solely on a single dataset but is expected to excel across numerous target domains—an area where the current literature has yet to fully explore. To navigate this intricate challenge, we introduce a novel framework called Single-dataset Unified Generalization (SUG). At the heart of SUG lies a Multi-grained Sub-domain Alignment (MSA) method, designed to adeptly handle the vast array of domain variances inherent in 3D point cloud data. By carefully splitting the selected single dataset into distinct sub-domains, we enable the model to learn a rich tapestry of features that span multiple modalities and domains, even in the absence of explicit target domain data.   Furthermore, our framework incorporates a Sample-level Domain-aware Attention (SDA) strategy, which dynamically adjusts the sample-level inter-domain distance to ensure an equitable adaptation process across all sub-domains. This innovation not only enhances the model's ability to generalize across various target domains but also ensures that no sub-domain is left behind in the adaptation journey. Through rigorous experimentation on well-established benchmarks, we demonstrate that SUG not only outperforms existing state-of-the-art methods but also significantly improves the domain generalization capabilities of several popular baseline models. In essence, SUG paves the way for more robust and versatile 3D point cloud models capable of thriving in a wide range of unforeseen environments."}
{"paper_id": 418, "abstract": "Detecting distribution shifts in deep neural networks is crucial for maintaining reliable performance in dynamic environments. This paper introduces a novel approach centered around selective prediction, which quantifies a model's prediction uncertainty and selectively abstains from making predictions on uncertain instances. We present a method to compute the best abstaining threshold and coverage bound for a given pretrained classifier, ensuring that the empirical coverage will not exceed the bound with high probability. This selective prediction with guaranteed coverage enables us to efficiently monitor a window of test data for distribution shifts. Our experimental results, conducted across diverse datasets and architectures, showcase substantial improvements in distribution shift detection, particularly in scenarios involving small test windows. This innovative approach paves the way for more robust and adaptive machine learning systems in the face of changing data distributions."}
{"paper_id": 419, "abstract": "In the realm of automatic 3D scene and layout generation, autoregressive models have emerged as formidable contenders. Yet, they often falter when faced with the challenge of conditioning on specific attributes or objects within a layout. The issue arises from their inherent design, which limits the condition to the beginning of the sequence, leaving subsequent generations in the dark about the full scope of the condition.  To address this conundrum, we introduce COFS—Conditional Object-based Furniture Scenes. At its core, COFS employs an innovative encoder-decoder architecture, enabling bidirectional attention through an encoder. This transformative approach allows the model to harness the entire conditioning information at each step, thereby breaking free from the constraints of sequential generation.  In this framework, tokens representing attributes of furniture objects are generated sequentially, yet the condition can be seamlessly inserted anywhere along the sequence. Consequently, COFS not only retains the strengths of existing autoregressive layout generation methods but also introduces a groundbreaking capability: the ability to condition on arbitrary subsets of object attributes. This leap forward opens up a myriad of possibilities, including generating layouts that incorporate specific object attributes, detecting outliers in attributes, conditioning on subsets of objects, and even performing object queries based on geometry attributes.  Our experiments reveal that COFS performs admirably on unconditional generation and object-level conditioning, often outshining existing methods. Furthermore, it stands alone in its support for attribute-level conditioning, a feat previously unattained by any other layout generation technique. In essence, COFS paves the way for more flexible and nuanced control in the realm of 3D scene and layout generation."}
{"paper_id": 420, "abstract": "Adversarial robustness is a crucial aspect of modern machine learning models, yet many traditional transfer learning techniques often overlook this critical feature. In this work, we embark on a comprehensive exploration of the interplay between adversarial robustness and transfer learning. Our investigation spans a wide array of scenarios, including transfer learning across domains and datasets, as well as within the same dataset but with distinct labels. At the heart of our analysis lies the examination of robustness certification, the resilience against various attack methodologies, and the stability under distributional shifts. Through rigorous experimentation, we uncover valuable insights into the conditions under which adversarial robustness can be effectively transferred. This study not only illuminates the landscape of adversarial robustness in transfer learning but also paves the way for more robust and reliable model deployments in real-world applications."}
{"paper_id": 421, "abstract": "In the realm of deep reinforcement learning (DRL), crafting controllers that not only excel but also uphold stringent safety standards poses a formidable challenge. Traditional methods often fall short, as they demand extensive expertise and a convoluted process for encoding user-specified constraints, leaving the balance between cost and reward uncertain. To address these shortcomings, we introduce a groundbreaking approach that seamlessly integrates Lagrangian-PPO, a cutting-edge DRL training technique, with scenario-based programming (SBP), a user-friendly paradigm for delineating system constraints. Through this innovative fusion, we enable practitioners to articulate their constraints in a straightforward manner and inject them directly into the training loop. This not only ensures that the generated policies meet all specified criteria but also maintains high performance levels. We validate our approach's efficacy by applying it to the intricate task of mapless navigation for robotics. Our findings reveal that, with the help of SBP, we can produce highly reliable policies capable of navigating a wide array of environments with remarkable proficiency. Furthermore, we complement our empirical evidence with formal verification analyses, confirming the robust safety guarantees of our methodologies. In essence, we pave the way for a new era in DRL where safety and excellence go hand in hand."}
{"paper_id": 422, "abstract": "[Abstract]\n\nIn the realm of deep reinforcement learning, the fragility of policies under adversarial attacks has become a pressing concern. Recent studies have unveiled that even subtle manipulations of the input space can precipitate catastrophic failures in decision-making. In response to this vulnerability, we introduce Knowledge-based Policy Recycling (KPR), a novel defense mechanism designed to bolster the resilience of policies against such threats. \n\nAt the heart of KPR lies the strategic integration of auxiliary task policies—policies crafted through curriculum or hierarchical learning, or those trained with sub-goals. These auxiliary policies serve as a repository of domain knowledge, enriching our understanding of the environment and its nuances. By leveraging both specified and learned relationships among these policies, we construct an ensemble capable of generating a robust, task-oriented policy. This ensemble is seamlessly fused using graph neural networks, which not only accommodate graph-based domain knowledge but also maintain adaptability through interaction data.\n\nOur experiments reveal that KPR stands as a formidable defense against various adversarial attacks across a diverse array of environments, including high-dimensional tasks like the Robot Food Court and the classic Atari games. Furthermore, KPR's versatility allows it to function effectively with any type of policy representation, whether neural networks or rule-based policies, making it a universally applicable solution. In doing so, we pave the way for more resilient and reliable reinforcement learning systems in the face of adversarial challenges."}
{"paper_id": 423, "abstract": "In the realm of continual learning, where knowledge must evolve without erasing its past, we delve into the intriguing world of CLIP—those formidable pre-trained models that bridge the gap between visual and linguistic worlds. Yet, when CLIP is tasked with the ongoing acquisition of new data streams, a peculiar phenomenon emerges: the cognitive disorder, marked by a decline in its capacity to accurately represent previous data points. This cognitive disarray stands in stark contrast to the robustness observed in self-supervised models during continual learning. \n\nIn an effort to unravel the mystery behind this cognitive disorder, we embark on a journey to meticulously track the changes in the representation space of CLIP as it undergoes continual training. Through rigorous empirical analysis and theoretical insights, we unveil two critical factors contributing to this phenomenon: the intra-modal rotation, wherein the representation space of individual modalities subtly shifts; and the inter-modal deviation, where the alignment between different modalities becomes distorted. Together, these elements conspire to disrupt the model's ability to maintain its grasp on earlier knowledge.\n\nMotivated by our findings, we introduce Mod-X—a novel framework designed to mitigate the cognitive disorder inherent in continual CLIP training. Mod-X focuses on selectively aligning the off-diagonal information contained within contrastive matrices, ensuring that the model retains its understanding of previously encountered data while adapting to new inputs. Our extensive experimental evaluations demonstrate that Mod-X not only enhances the model's performance on current data but also preserves its accuracy on older datasets, offering a promising solution to the challenges of continual learning in the context of CLIP."}
{"paper_id": 424, "abstract": "In the realm of model-based reinforcement learning (MBRL), where agents navigate complex environments through the lens of a learned dynamics model, a pivotal challenge emerges: the efficient management of experience replay buffers. These repositories serve as the cornerstone of MBRL, yet their unwieldy sizes often prove to be a significant bottleneck in the quest for optimal performance and stability. In this work, we embark on a bold journey to explore strategies for maintaining the most essential information within the buffer, ensuring that it remains lean and effective throughout the learning process. Our findings reveal that the key lies in the dynamic allocation of buffer capacity and the strategic decision-making regarding when and how to engage in training. By doing so, we not only reduce the overall computational demands but also enhance the agent's ability to learn efficiently and effectively. This innovative approach paves the way for more robust and adaptable MBRL systems, poised to tackle the challenges of continual learning with greater ease."}
{"paper_id": 425, "abstract": "[Abstract]\n\nIn the realm of deep learning, a pivotal concept has emerged that challenges the conventional wisdom: the notion that large neural networks are inherently robust to the deletion of their parameters. This idea has sparked a flurry of research, leading to the development of pruning algorithms designed to distill knowledge from these expansive models into more compact versions. Yet, a critical question remains unexplored: Is it possible to extract all the essential information contained within a large model by simply training a smaller one from scratch? \n\nIn this paper, we embark on a journey to demystify the relationship between large and small models. We delve into the intricate connections between overparameterized neural networks, random features models, and kernel regression, revealing that these concepts are not merely tangential but deeply intertwined. Through rigorous analysis, we uncover a profound truth: for a wide array of overparameterized neural network architectures, there exists a striking equivalence between training a large model and directly training its small counterpart. \n\nOur findings offer a fresh perspective on the potential of small models to harness the full power of their larger brethren, challenging the prevailing narrative and opening new avenues for innovation in the field of deep learning."}
{"paper_id": 426, "abstract": "In the realm of reinforcement learning, imitation learning emerges as a beacon of hope, allowing us to distill the wisdom of experts into the fabric of our agents' decision-making processes. Yet, the path to mastery is fraught with challenges, particularly when it comes to crafting effective reward functions—a task often likened to navigating a labyrinth with no clear map. To address this conundrum, we introduce the Auto-Encoding Adversarial Imitation Learning (AEAIL) framework, which ingeniously reimagines the reward function as the discrepancy between the reconstructed state and its original counterpart. This innovative approach harnesses the power of auto-encoders to encode states and decode them back to their initial form, providing a rich tapestry of feedback that guides the learning process.   Our experiments, conducted across a diverse array of environments, reveal that AEAIL not only outperforms existing state-of-the-art methods but also demonstrates remarkable resilience to various distribution divergences and auto-encoders. The true strength of our framework lies in the encoding-decoding mechanism itself, rather than the specific choices of divergence or auto-encoder. With AEAIL, we take a significant stride toward making imitation learning both robust and efficient, opening new frontiers in the pursuit of artificial intelligence that learns as seamlessly as humans do from observation."}
{"paper_id": 427, "abstract": "Photorealistic style transfer aims to transform a content image to a target style image while preserving the photorealism of the output image, yet existing methods still suffer from artifacts or blurry results. In this paper, we propose a novel photorealistic style transfer network named ColoristaNet, which achieves superior performance by removing and restoring image style with decoupled instance normalization and a self-supervised learning framework. Specifically, ColoristaNet employs two style transfer networks to conduct style removal and restoration simultaneously. In the style removal network, a decoupled instance normalization module is introduced to remove the style of the input image, while preserving the image structure. Then, in the style restoration network, the image style is restored from the style-removed image. By this design, the style transfer task is transformed into a self-supervised style restoration task, thus avoiding the use of the Gram loss and regularization terms. Moreover, we introduce a motion compensation mechanism to further enhance the temporal consistency of the output video. Extensive experiments demonstrate that ColoristaNet significantly outperforms state-of-the-art methods in photorealistic style transfer.  Our contributions can be summarized as follows: • We propose a novel photorealistic style transfer network, named ColoristaNet, which can conduct photorealistic style transfer with multiple style references and runs in real-time. • We propose a self-supervised style transfer framework, which avoids employing the Gram loss during training, and a novel feature transformation module, named decoupled instance normalization, to achieve better photorealism and consistency. • A comprehensive set of experimental results on various benchmarks and datasets demonstrates the effectiveness of our method.  To validate our methods, we conduct extensive experiments on four public datasets, including CelebA-HQ (Karras et al., 2018), LSUN bedrooms (Yu et al., 2015), Places2 (Zhou et al., 2017) and Flickr-2k (Yoo et al., 2019). For quantitative evaluation, we adopt the structural similarity (SSIM) (Wang et al., 2004) and the peak signal-to-noise ratio (PSNR) (Huang & Belongie, 2017) as metrics to evaluate the structural fidelity and perceptual quality of our results. We also conduct a user study on Amazon Mechanical Turk (MTurk) platform to evaluate the perceptual quality of stylized images qualitatively. Table 1 reports the quantitative results on"}
{"paper_id": 428, "abstract": "[Abstract]\n\nIn the realm of Transformers, the art of encoding relative positional information remains a cornerstone for their prowess in capturing intricate dependencies within data. Recent innovations have introduced linear Transformers, heralding a new era of efficiency through linear space-time complexity, yet the development of relative positional encodings tailored specifically for this architecture has lagged behind. To address this gap, we unveil a groundbreaking framework that demystifies the design of relative positional encodings for linear Transformers. Our approach hinges on a novel canonical form, meticulously crafted to illuminate the underlying unity among existing encoding strategies. This revelation not only unifies disparate methods but also illuminates the path forward in devising innovative encodings. At the heart of our exploration lies the Linearized Relative Positional Encoding (LRPE) family—a versatile suite of solutions that upholds the critical property of being unitary transformations. This ensures both compatibility with linear Transformers and the preservation of linear space-time complexity. Through rigorous experimentation, we validate the efficacy of our LRPE family across a diverse array of tasks, demonstrating its robustness and superior performance compared to conventional methods. In doing so, we pave the way for a new generation of encodings that not only enhance the capabilities of linear Transformers but also enrich the broader landscape of Transformer-based architectures."}
{"paper_id": 429, "abstract": "In the realm of Reinforcement Learning (RL), where agents navigate through complex environments, the exploration-exploitation conundrum stands as a formidable challenge. This dilemma arises when agents must choose between exploring new territories or exploiting well-trodden paths, all while grappling with the constraints of finite resources. In the context of Multi-Agent RL (MARL), this quandary is further complicated by the need to manage the balance between exploration and exploitation not only across the temporal dimension but also among multiple agents. Each agent may find itself at varying stages of learning, necessitating a nuanced approach to balancing these dual objectives. \n\nTo tackle this intricate challenge, we introduce a novel framework centered around entropy regularization, designed specifically for adaptive exploration in MARL. At the heart of this framework lies a pivotal concept: the allocation of distinct target entropies tailored both to individual agents and to the evolving dynamics of time. The cornerstone of our approach is a meticulously crafted metric, one that gauges the potential benefits of increased exploration for each agent. This metric serves as a beacon, guiding us in determining the optimal level of exploration for every agent within the system. \n\nTo bring this vision to life, we harness the power of disentanglement—a method that separates the joint soft value function into two distinct components: one focused on the return and another on the entropy sum. This disentanglement not only mitigates the risks of instability that can arise from temperature parameter updates but also allows for the application of value factorization to both the return and entropy dimensions independently. This separation is crucial, as it reveals that the impact on reward can differ significantly from the influence on entropy when viewed through the lens of individual agents. \n\nFrom this foundation, we derive a metric that quantifies the desired level of exploration for each agent, grounded in the partial derivative of the joint value function of pure return with respect to action entropy. This derivative serves as a powerful indicator of how much additional exploration would enhance the overall performance. By imposing the constraint that the total target entropy across all agents remains constant, we ensure that any increase in exploration for one agent is met with a corresponding reduction elsewhere, thus maintaining a delicate equilibrium. \n\nOur empirical results underscore the efficacy of this innovative framework in addressing the multi-agent exploration-exploitation trade-off. Through rigorous testing, we demonstrate that our approach not only enhances the performance of agents but also fosters a harmonious balance among them, paving the way for more sophisticated and adaptable solutions in the ever-evolving field of MARL."}
{"paper_id": 430, "abstract": "[Abstract]\n\nAs reinforcement learning continues to advance, the capabilities of trained agents have expanded significantly, allowing them to navigate intricate environments and tackle complex challenges. However, a critical issue emerges when these agents encounter environments that differ slightly in their dynamics—a phenomenon that can lead to catastrophic failures. This fragility in the face of even minor changes underscores a significant challenge in the field. \n\nIn response to this vulnerability, researchers have turned to the concept of robust Markov Decision Processes (MDPs), which aim to protect against the uncertainties that arise from environmental changes. The traditional approach in robust MDPs has focused on finding the best policy within a given uncertainty set, a task that often requires a substantial amount of data. This reliance on extensive datasets poses a significant barrier to practical implementation, especially given the challenges of data collection in real-world scenarios. \n\nTo address this limitation, we introduce a novel policy optimization algorithm specifically designed for robust MDPs. This method operates without the need for a generative model, making it more adaptable to the complexities of real-world applications. Our algorithm not only provides a theoretical foundation for understanding robust MDPs but also demonstrates its effectiveness through empirical evaluations. \n\nMoreover, we provide the first regret bound for robust MDPs, achieving a sublinear regret of $O(\\sqrt{K})$ across both (s, a) and s-rectangular uncertainty sets, where $K$ represents the number of episodes. These contributions mark a significant advancement in the realm of robust reinforcement learning, offering a path forward for developing more resilient and practical RL solutions."}
{"paper_id": 431, "abstract": "In the realm of deep learning, where the stakes are high and the landscapes complex, the humble Stochastic Gradient Descent (SGD) reigns supreme, offering both speed and robust generalization—qualities that make it the go-to choice for practitioners worldwide. Yet, amidst this acclaim, a shadow looms: the notorious challenge of tuning the learning rate. This task, akin to navigating a treacherous labyrinth, demands meticulous effort and expert knowledge, a burden that often falls upon the shoulders of experienced hands.  In response to this dilemma, the adaptive gradient methods, namely Adam, RMSprop, and Adagrad, emerged, promising to alleviate the pain of tuning by dynamically adjusting the learning rate for each parameter. While these methods have demonstrated impressive gains in convergence speed, a troubling truth soon became apparent: they often falter in their ability to generalize effectively. This revelation sparked the quest for new algorithms that could harness the rapid convergence of adaptive gradients while retaining the remarkable generalization prowess of SGD.  Enter our innovative proposal, the Dimension-Reduced Adaptive Gradient Method, or DRAG—a method designed to strike a harmonious balance between the two extremes. By optimizing the loss function along multiple descent directions, DRAG introduces a nuanced approach that mitigates the overwhelming adaptivity inherent in traditional adaptive gradient methods. This reduction in adaptivity not only streamlines the optimization process but also enhances the method's overall effectiveness.  To further refine our approach, we leverage the power of trust-region techniques, allowing us to explore the most promising directions for parameter updates. The beauty of this strategy lies in its simplicity: the trust-region subproblem we solve is of a mere two dimensions, drastically reducing the computational overhead associated with higher-dimensional problems. Moreover, we have crafted a straightforward yet effective heuristic for determining the trust-region radius, ensuring that our method remains both efficient and reliable.  Our theoretical analysis reveals that DRAG is capable of converging to an ϵ-approximate first-order stationary point, achieving a stochastic gradient complexity of O(ϵ^-4) on non-convex stochastic problems. This complexity aligns perfectly with the established lower bound, as demonstrated by Arjevani et al. (2022). Furthermore, our empirical evaluations on a diverse array of benchmark tasks underscore DRAG's superiority, as it consistently outperforms existing methods in both speed and generalization.  In essence, DRAG emerges as a beacon of hope in the optimization landscape, offering a path forward that combines the best attributes of SGD and adaptive gradient methods."}
{"paper_id": 432, "abstract": "In the realm of anomaly detection, traditional methods often grapple with the challenge of limited labeled data, resorting to complex heuristics that fall short in capturing the intricate dynamics of real-world systems. In this paper, we introduce a novel framework centered around quantile-LSTM, a robust and versatile solution that leverages the power of quantiles to model the probabilistic distribution of time-series data. By employing this approach, we sidestep the cumbersome process of labeling data, instead focusing on the inherent properties of the data itself. \n\nAt the heart of our innovation lies the Parameterized Elliot Activation Function (PEF), a flexible and adaptive component that enhances the performance of our quantile-LSTM models. This activation function is designed to accommodate the variability present in the data, allowing for more accurate predictions. To further refine our models, we explore alternative strategies, including using the Inter-Quartile Range (IQR) and the median as thresholds to identify anomalies. Our empirical evaluations reveal that the IQR-LSTM variant emerges as the most effective choice, consistently outperforming both the quantile-LSTM and median-LSTM methods across a diverse array of datasets. \n\nMoreover, we delve into the nuanced behavior of the Elliot Activation Function (EA) and its parameterized counterpart, providing a comprehensive analysis of their impact on our models. We demonstrate through experiments that the PEF indeed exhibits superior performance in anomaly detection, supported by our extensive testing on both synthetic and real-world datasets. This work not only advances the field of anomaly detection but also opens new avenues for exploring the potential of quantile-based models in the ever-evolving landscape of machine learning."}
{"paper_id": 433, "abstract": "Graph augmentation is a cornerstone in graph contrastive learning (GCL), serving as a critical tool to distill instance-discriminative information for graph representation learning. Early attempts at augmentation often relied on random corruptions, which can inadvertently obscure the salient structural features of graphs, leading to a semantic gap that hinders the effectiveness of contrastive learning. To address this challenge, recent advancements have introduced the concept of rationale-aware augmentations. These methods aim to discover the most salient structural elements within graphs to create augmented views that are more conducive to learning. However, existing strategies typically focus on either node-based or edge-based transformations, neglecting the potential value of heterogeneous transformations that consider both nodes and edges. Moreover, these approaches tend to separate the rationale discovery process from the subsequent encoding step, which can hinder the smooth integration of the two processes. In this work, we propose a novel framework called Self-attentive Rationalization (SR), designed to harmoniously combine the discovery and encoding of rationales within a single module. By leveraging self-attention mechanisms, our approach is capable of generating diverse rationales that encompass both node and edge perspectives simultaneously, thereby enhancing the robustness of the learned representations. We further integrate this method into a comprehensive graph contrastive learning pipeline, which we term SR-GCL. Our experimental evaluations across a wide range of benchmark datasets demonstrate that SR-GCL significantly outperforms existing state-of-the-art techniques, consistently achieving superior results in graph representation learning. Furthermore, our method provides valuable interpretability, revealing the critical contributions of individual nodes and edges in driving the discriminative power of the model. This dual advantage—both in terms of performance and interpretability—makes SR-GCL a promising advancement in the field of graph representation learning."}
{"paper_id": 434, "abstract": "In the realm of decoding, where the intricate dance between external stimuli and the mind's internal states unfolds, we embark on a journey to harness the power of group modeling—a technique poised to illuminate the tapestry of brain activity across subjects. Our exploration focuses on magnetoencephalography (MEG) data, a fascinating window into the brain's electromagnetic whispers, as we delve into the decoding of visual stimuli. \n\nTraditionally, researchers have employed subject-level (SL) models, tailored meticulously for individual subjects, often resorting to simplistic linear decoders. Yet, this approach leaves much to be desired, as it fails to capitalize on the wealth of information that could be gleaned from a broader, more inclusive perspective. Enter our innovative solution: a group-level (GL) model designed to learn shared representations across subjects, augmented by subject-specific embeddings. This architectural marvel not only captures the nuances of between-subject variability but also significantly enhances the decoding accuracy when compared to its naive counterparts. \n\nIn our experimental odyssey, we confront the challenge of within-subject splitting evaluation, a common practice in the neuroscience community. Here, we demonstrate that our GL model outshines traditional methods, offering a beacon of hope for future endeavors. Additionally, we explore the potential of our approach in leave-one-subject-out (LOSO) scenarios, showcasing its robustness and adaptability. \n\nBeyond the technical triumphs, our work yields profound neuroscientific insights, shedding light on the mechanisms that underpin visual perception. We reveal that our model excels in predicting both the timing and location of visual processing events, offering a tantalizing glimpse into the brain's complex symphony. Furthermore, our investigation into the model's learned weights uncovers rich spatiotemporal patterns and spectral information, enriching our understanding of the brain's response to visual stimuli. \n\nIn this paper, we lay the groundwork for a new era in decoding, where the collective wisdom of diverse minds illuminates the path forward."}
{"paper_id": 435, "abstract": "[Abstract]\n\nActive learning, a strategy that judiciously selects the most informative samples from an unlabelled pool for annotation, has emerged as a powerful tool for optimizing the allocation of limited resources. Despite its effectiveness, a significant challenge remains: the model's performance on test data often fails to live up to expectations. To address this issue, we introduce Sharpness-Aware Active Learning (SAAL), a novel approach that integrates the principles of generalization with active learning through the lens of loss sharpness—a metric that quantifies how susceptible the model's predictions are to minor perturbations in the input space. \n\nAt the heart of SAAL lies the maximally perturbed loss, which serves as a beacon for guiding the selection process. This innovative acquisition function is designed to pinpoint those instances where the model's confidence is both low and highly sensitive to slight variations in the input. To make this concept operational, we employ pseudo labels generated by the current model to estimate the perturbed loss, a method we rigorously validate through theoretical analysis. We further delve into the intricacies of the upper bound of the acquisition score, revealing its intimate connection with the first eigenvalue of the loss Hessian. This discovery not only sheds light on the relationship between the acquisition score and the generalization performance of the model but also underscores the importance of considering loss sharpness in the context of active learning.\n\nThrough extensive experimentation across a diverse array of vision-based tasks, we demonstrate that SAAL consistently outperforms its counterparts. Our findings suggest that by focusing on instances that exhibit high loss sharpness, SAAL can significantly enhance the model's generalization capabilities, paving the way for more robust and reliable machine learning applications. In this manner, SAAL not only addresses the critical issue of generalization in active learning but also sets a new standard for the development of future acquisition strategies."}
{"paper_id": 436, "abstract": "[Abstract]\n\nIn the realm of distance metric learning, a common practice involves employing a convolutional neural network (CNN) followed by a global pooling layer, most often the global average pooling (GAP). This conventional approach is predicated on the idea that each pixel within the CNN feature map corresponds to a distinct semantic entity, allowing for the representation of diverse attributes through a convex combination of these elements. However, this perspective leaves much to be desired—namely, the nuanced understanding of which features truly hold significance and how they should be weighted in the grand scheme of things. \n\nIn this work, we embark on a journey to challenge this status quo, introducing a novel, learnable, and generalized form of GAP that empowers the model to select both the subset of pertinent features and the weights assigned to each. Our innovative approach hinges on the formulation of GAP as the resolution to an optimization problem, a strategy that allows for the exclusion of certain features by assigning them zero weight. This method not only broadens the scope of potential solutions but also introduces regularization to prevent the model from falling into the trap of relying on all available features, thus fostering a more robust and interpretable representation.\n\nTo further enhance the effectiveness of our proposal, we incorporate a zeroshot prediction loss as a regularization term, leveraging the power of attribute embeddings to guide the selection process. Through a series of synthetic experiments, we demonstrate that our method outperforms traditional GAP, along with other pooling alternatives, in terms of its ability to select the most relevant features and improve overall performance. Moreover, we showcase the versatility of our approach by applying it across six distinct DML losses and testing it on four diverse datasets. The results unequivocally证实了我们的方法在各种场景下的优越性，为距离度量学习领域注入了新的活力。"}
{"paper_id": 437, "abstract": "In the realm of deep learning, where complexity often reigns supreme, we embark on a quest to distill the essence of large neural networks into their leaner counterparts—Lottery Tickets (LTs). The Lottery Ticket Hypothesis (LTH) of Frankle & Carbin (2019) illuminates a path forward, revealing that within any sufficiently over-parameterized network lies a sparse subnetwork that, once trained in isolation, achieves comparable performance to its original, resource-hungry counterpart. This revelation has sparked a flurry of interest, yet many questions remain unanswered about the intricate nature of these lottery tickets. In this study, we delve into the heart of the matter, investigating the role of layer-wise importance in shaping the lottery tickets that emerge from a network.   Our exploration begins by refining the traditional weight magnitude criterion through the lens of various weight rescaling techniques. We find that employing layer-wise weight scaling methods not only enhances the robustness of our findings but also introduces a fascinating new perspective on the lottery ticket phenomenon. To substantiate our claims, we present extensive empirical evidence gathered from diverse network architectures, including ResNet, VGG, AlexNet, LeNet, and DenseNet, as well as a variety of training strategies.   Perhaps the most intriguing discovery of our investigation is the presence of highly stable, common connections among the lottery tickets extracted under identical training conditions. These shared connections persist even when the remaining weights in the network are re-initialized. This observation suggests that there may be a more efficient method for uncovering lottery tickets—a tantalizing prospect that warrants further exploration. Join us on this journey as we unravel the mysteries of lottery tickets and pave the way for future advancements in the field."}
{"paper_id": 438, "abstract": "In the realm of instance segmentation, where the intricate dance of pixels unfolds, deep learning has emerged as a formidable ally, capable of delivering remarkable results on natural images. Yet, in the specialized domains of surveillance, industrial quality control, and, notably, medical and biological imaging, the scarcity of training data presents a formidable challenge. To address this conundrum, we delve into the heart of reinforcement learning, crafting a novel approach that harnesses the power of high-level priors without the need for annotated images. In this endeavor, we transform the instance segmentation task into an agglomeration of image superpixels, steering the process with edge weights predicted by our actor. The resulting segmentation, born from the crucible of graph partitioning, is then scrutinized by a critic that approximates the rewards, drawing upon the richness of object-and image-level reasoning. What sets our method apart is its seamless integration of a broad spectrum of non-differentiable high-level priors, each contributing to the refinement of the segmentation process. Through rigorous experimentation on both synthetic and real-world datasets, including critical applications in biology, we demonstrate the robustness and adaptability of our framework. Thus, we pave the way for a new era in instance segmentation, where prior knowledge becomes a cornerstone of success."}
{"paper_id": 439, "abstract": "[Abstract]\n\nIn the realm of decentralized machine learning, where collaboration across a network of nodes becomes the cornerstone of progress, a formidable challenge emerges: the specter of Byzantine adversaries. These malevolent entities not only conspire to disrupt the harmony of communication but wield the power to manipulate the very data and models that fuel our algorithms. In this paper, we embark on a journey to unravel the mysteries of robust decentralized training within the confines of a communication graph, revealing the intricate dance of resilience against Byzantine adversaries.\n\nAt the heart of our exploration lies a novel network robustness criterion—a beacon that illuminates the path forward. This criterion is forged from the spectral gap of the topology and the count of adversaries, offering a far-reaching perspective that transcends the limitations of previous frameworks. We introduce CLIPPEDGOSSIP, a groundbreaking defense strategy designed to safeguard the integrity of our training process. Through rigorous analysis, we unveil precise rates of robust convergence, guiding us toward a neighborhood of a stationary point for stochastic objectives, all while adhering to standard assumptions.\n\nBut the story does not end there. In our empirical odyssey, we find that CLIPPEDGOSSIP outshines its predecessors, demonstrating superior performance and reliability in the face of adversity. Moreover, our work culminates in the revelation of the fastest convergence rates for standard, non-robust decentralized stochastic non-convex optimization, achieved through the innovative application of local worker momentum.\n\nJoin us as we navigate the complex landscape of decentralized learning, where resilience meets innovation, and where the future of robust training awaits discovery."}
{"paper_id": 440, "abstract": "[Abstract]\n\nIn the realm of network science, the quest for efficient and accurate shortest-path (SP) representations stands as a pivotal challenge, particularly when navigating the vast and intricate landscapes of modern graphs. Traditional approaches, while adept at capturing distance approximations, often falter in their ability to convey nuanced structural insights, leading to subpar performance in critical applications such as POI search, social relationship analysis, and biomedical structure prediction. In this paper, we introduce Betweenness Centrality-based Distance Resampling (BCDR), a groundbreaking method designed to revolutionize the way we estimate SP distances. \n\nAt the heart of BCDR lies a novel concept: Betweenness Centrality-based Random Walk. This innovative approach equips us with a powerful tool to explore a broader spectrum of SP distance correlations within the confines of limited walk steps. By leveraging the inherent properties of betweenness centrality, we enhance our ability to uncover and navigate the complex tapestry of relationships that define a graph's structure. Our theoretical analysis reveals that this method not only broadens the scope of our investigations but also significantly enhances the approximation quality of our SP representations.\n\nTo further refine our technique, we introduce a second cornerstone of BCDR: Distance Resampling. This component works in tandem with our random walk to ensure that the learned SP representations not only capture the essence of distance relationships but also remain true to the original graph's intricate web of connections. Through rigorous testing, we demonstrate that BCDR consistently outperforms existing state-of-the-art methods across a diverse array of real-world and synthetic graphs. \n\nIn conclusion, BCDR offers a fresh perspective on the art of SP representation, promising to unlock new possibilities in the field of network analysis. With its enhanced accuracy and streamlined efficiency, BCDR sets a new standard for tackling the ever-evolving challenges of graph-based data analysis."}
{"paper_id": 441, "abstract": "[Abstract]\n\nIn the ever-evolving landscape of neural network training, the quest for efficiency and accuracy has led us to explore the realm of second-order optimization—a territory often overshadowed by the simplicity of gradient descent. While second-order methods promise faster convergence, their practical application remains a challenge due to the prohibitive cost of computing curvature and its inverse, particularly when dealing with large-scale, deep neural networks. Furthermore, the use of second-order optimization has been linked to diminished generalization capabilities, further complicating their integration into real-world applications.\n\nIn this work, we unveil a groundbreaking approach that leverages the strengths of both first-and second-order optimization methods, tailored specifically for those intricate loss functions that are inherently difficult to optimize. We delve into the mathematical intricacies of iterative optimization, revealing that it can be elegantly decomposed into a two-stage scheme. This decomposition not only illuminates the underlying structure of optimization but also provides a pathway for the creation of Newton losses—our innovative solution that merges the power of second-order optimization with the robustness of first-order methods.\n\nTo validate our approach, we conducted an extensive series of experiments across two prominent benchmarks: the MNIST sorting task and the Warcraft shortest-path challenge. Our findings unequivocally demonstrate that our Newton losses significantly enhance performance when applied to loss functions that are notoriously challenging to optimize. In contrast, when used with more straightforward loss functions, our method maintains the same level of performance as traditional techniques.\n\nThrough this exploration, we hope to inspire a new wave of innovation in neural network training, paving the way for more efficient and effective solutions in the ever-expanding field of machine learning."}
{"paper_id": 442, "abstract": "In the realm of sequential prediction tasks, where sequences teem with continuous signals vital for model predictions, a pressing need arises for interpretable artificial intelligence solutions. The complexity of both the sequential data itself and the burgeoning intricacy of the models employed make it a formidable challenge for humans to grasp the inner workings of these predictions. Enter SeqSHAP—a novel method that harnesses the power of Shapley values to illuminate the pathways of sequential models, offering explanations at the subsequence level, a perspective uniquely suited to the nuances of sequential scenarios. This approach is bolstered by a distribution-based segmentation technique that adeptly captures the intricate distribution information within sequential features. By dividing the sequence into meaningful subsequences, SeqSHAP groups feature elements into cohesive units, allowing for more insightful Shapley value estimations. Our rigorous experimentation on two sizable real-world transaction datasets reveals the robustness and clarity of our segmentation method, alongside the compelling subsequence-level explanations it generates. In this way, SeqSHAP not only enhances transparency but also fosters a deeper understanding of the complex interplay between sequential data and predictive models."}
{"paper_id": 443, "abstract": "[Abstract]\n\nIn the realm of Chinese Word Segmentation (CWS), the pre-trained language model has emerged as a formidable force, propelling performance to unprecedented heights. However, a critical challenge persists—the performance gap between open-vocabulary (OOV) words and common vocabulary words remains a formidable barrier. To address this issue, we present the Boundary-Enhanced Decoder (BED), a novel approach designed to refine the output of the pre-trained language model, thereby enhancing its overall capabilities. \n\nAt the heart of BED lies the recognition that the difficulty of segmenting a word varies significantly from one to another. For instance, consider the ease with which punctuation marks or transition words can be identified and segmented, compared to the nuanced complexity involved in parsing more intricate terms. Our method mirrors the human process of word segmentation: initially, we identify and isolate the easier segments within a sentence, effectively breaking it down into larger, more manageable chunks. This preliminary segmentation allows for a more focused and accurate refinement of the remaining, more challenging portions. By implementing this strategy, we aim to elevate the effectiveness of word segmentation across the board, particularly for those elusive OOV words.\n\nThrough rigorous experimentation, we demonstrate that our Boundary-Enhanced Decoder not only improves the accuracy of word segmentation but also significantly narrows the performance disparity between OOV and common words. In essence, BED serves as a powerful tool to bridge the gap in CWS, making it a valuable addition to the field of natural language processing."}
{"paper_id": 444, "abstract": "[Abstract]\n\nIn the ever-evolving landscape of artificial intelligence, denoising diffusion probabilistic models (DDPMs) have emerged as formidable contenders, particularly in the realm of image generation. Their prowess has been celebrated through the lens of large-scale benchmarks, where they consistently outshine their rivals in terms of both sample quality and diversity. However, the question remains: Can this power be harnessed to address the pressing challenges of modern industrial applications, especially those centered around the generation of synthetic tabular data? In this exploration, we embark on a journey to demystify the effectiveness of DDPMs within the context of tabular data synthesis, uncovering both the strengths and limitations of this approach.\n\nAt the heart of our investigation lies a simple yet robust design for DDPMs tailored specifically for tabular data. This design is not only versatile enough to accommodate a wide array of tabular tasks but also adept at handling mixed feature types, including both numerical and categorical variables. Our empirical studies reveal that this design yields remarkable results across a diverse spectrum of synthetic tabular benchmarks, demonstrating its potential as a go-to solution for tabular data generation.\n\nBut the story doesn't end there. We delve deeper into the intricate mechanisms at play, highlighting the unique characteristics of DDPMs that contribute to their superior performance. Specifically, we illuminate how the gradual removal of noise during the diffusion process facilitates the generation of synthetic data that closely mirrors real-world distributions. Furthermore, we explore how the noise level inherent to DDPMs can serve as a critical indicator of the synthetic data's utility in enhancing downstream tasks, thereby offering a practical tool for assessing the value of generated data.\n\nIn conclusion, our work not only showcases the remarkable capabilities of DDPMs in the tabular domain but also provides a comprehensive understanding of their strengths and weaknesses. By doing so, we pave the way for further advancements in the realm of synthetic data generation, promising to unlock new possibilities in the ever-expanding universe of AI-driven solutions. The full implementation of our proposed method, along with additional details, can be found at https://github.com/DeepChem/tabddpm."}
{"paper_id": 445, "abstract": "In the realm of generative modeling, Generative Flow Networks (GFlowNs) stand tall as a beacon of innovation, employing a learned policy to craft objects within a target space \\(X\\). These networks are meticulously crafted to ensure that the likelihood of generating a specific object \\(x \\in X\\) is in harmony with a designated reward function \\(r(x)\\). Recent advancements have unveiled three distinct learning objectives—Flow Matching (FM), Detailed Balance (DB), and Trajectory Balance (TB)—each designed to refine the policy's prowess. Yet, a critical challenge persists: the FM and DB objectives often falter in their ability to allocate credit effectively over extended time horizons, leading to suboptimal performance. In contrast, the TB objective emerges as a more robust alternative, though it carries the burden of higher variance, akin to the infamous bias-variance trade-off in Temporal Difference learning. This paper introduces a novel learning objective, the Subtrajectory Balance (SubTB) objective, which strikes a delicate balance between the two extremes, offering a nuanced approach to credit assignment. Through rigorous experimentation across diverse synthetic and real-world domains, we demonstrate that SubTB not only accelerates the training process but also enhances the overall effectiveness of GFlowNs, particularly in scenarios fraught with sparse rewards or lengthy action sequences. Our findings underscore the pivotal role of SubTB in advancing the capabilities of GFlowNs and pave the way for future innovations in generative modeling."}
{"paper_id": 446, "abstract": "Conformal prediction is a powerful technique for generating uncertainty sets for machine learning models. However, when faced with distribution shifts, conformal predictors often struggle to maintain their validity, leading to coverage that falls short of the intended levels. In this paper, we introduce a straightforward recalibration approach designed to address these challenges. Our method leverages unlabeled data from the target distribution to adjust a conformal predictor, ensuring that its coverage aligns with the desired confidence level. We demonstrate the effectiveness of our approach both theoretically and empirically, showcasing its prowess on several real-world datasets. Notably, our recalibration method outperforms existing techniques, achieving nearly oracle-level coverage in some scenarios. This advancement opens new avenues for robust uncertainty quantification in the face of distributional changes."}
{"paper_id": 447, "abstract": "In the realm of federated graph learning (FGL), a groundbreaking approach emerges, aiming to harness the collective power of distributed subgraphs without the need for direct data sharing. However, a critical challenge arises: the pervasive issue of graph structure heterogeneity across these subgraphs. This phenomenon, often overlooked, poses significant obstacles to the performance of traditional federated learning models. In this paper, we delve into the intricacies of this problem, exploring how the structure of local subgraphs can deviate significantly from the overarching global graph.  We meticulously examine two distinct scenarios—homogeneous and heterogeneous subgraphs—and reveal that the prevalent methods designed to address the label non-independent and identically distributed (Non-IID) problem fall short in handling the complexities of structure Non-IID.  To tackle this formidable challenge, we introduce AdaFGL, a novel framework that empowers each client to dynamically select the most suitable local subgraph model tailored to their specific context. Our approach hinges on a robust analysis of the local subgraph's structure, utilizing a straightforward yet effective label propagation technique. By doing so, we enable clients to adapt their strategies, choosing between homogeneity and heterogeneity propagation modules. Furthermore, we propose a federated global knowledge extractor, a powerful tool that enhances the representation capabilities of the global model.  Through extensive experimentation, we validate the efficacy of our proposed framework. On a variety of benchmark datasets, including Cora, Citeseer, Pubmed, and Squirrel, AdaFGL outperforms existing state-of-the-art methods. Notably, our approach achieves performance gains of 4.67% and 2.65% in homogeneous and heterogeneous subgraph settings, respectively. These results underscore the significant impact of structure Non-IID on federated graph learning and the transformative potential of AdaFGL in overcoming this challenge."}
{"paper_id": 448, "abstract": "[Abstract]\n\nIn the realm of neural networks, where intricate patterns weave through the fabric of data, we introduce an innovative concept: the symbolic conceptual view—a mathematical abstraction that bridges the gap between neural networks and human comprehension. In this conceptual landscape, each neuron within a network is transformed into a symbol, and the network itself becomes a lattice of these symbols, offering a clear path to understanding.\n\nOur journey begins with the construction of these views, where we illuminate the intricate connections between input features and output predictions. This illumination reveals the hidden logic within the network, making it accessible to both humans and machines alike. To harness the power of these views, we employ a variety of algorithms tailored to explore the depths of the lattice. These include hierarchical clustering, which organizes the concepts into a structured hierarchy; k-means clustering, which groups similar concepts together; and a sophisticated algorithm that identifies significant subgroups based on statistical measures. Each of these tools serves to uncover the true nature of the network's decision-making process.\n\nBut the exploration does not stop there. We delve deeper into the heart of the symbolic conceptual view by leveraging formal concept lattices. These structures provide a rich framework for discovering propositional statements that capture the essence of the network's behavior. By integrating background knowledge, we refine these statements, ensuring they resonate with our understanding of the world.\n\nTo validate our approach, we present experimental results across a range of datasets, demonstrating the remarkable ability of our symbolic conceptual views to reveal meaningful insights. In doing so, we pave the way for a new era of explainable machine learning, where transparency and human understanding coexist harmoniously with the power of artificial intelligence."}
{"paper_id": 449, "abstract": "In the realm of machine learning, bilevel optimization stands as a formidable challenge, captivating the imaginations of researchers and practitioners alike. This paper embarks on a comprehensive exploration of the generalization behavior of first-order methods tailored for tackling the complexities of bilevel optimization problems. At the heart of our investigation lies a profound connection between the generalization gap and the concept of algorithmic stability—specifically, argument stability and uniform stability—unveiling the intricate interplay between these two pivotal notions. We delve into the high probability form of the generalization gap, revealing how it can be significantly enhanced from O(√n) to O(log n), surpassing previous bounds established by Bao et al. (2021). \n\nIn a groundbreaking move, we also present the first-ever stability bounds for gradient-based methods employing both single-timescale and two-timescale update strategies. Our analysis unfolds across a spectrum of standard settings, including the intriguing territory of nonconvex-strongly-convex scenarios, which frequently arise in practical applications. Moreover, we tackle the thorny issue of continuous parameter changes for both the outer and inner levels, offering a fresh perspective that avoids the pitfalls of reinitialization steps. Our theoretical findings are bolstered by empirical evidence, drawn from the dynamic landscapes of meta-learning and hyper-parameter optimization. In this way, we not only illuminate the path forward but also offer concrete insights into the future of bilevel optimization in machine learning."}
{"paper_id": 450, "abstract": "Recent years have witnessed the remarkable success of transformer architectures across an array of domains, from natural language processing to computer vision, speech processing, and beyond. Yet, a significant challenge looms: the quadratic complexity of attention mechanisms in both time and memory. This issue has grown particularly acute as the demand for handling longer sequences has surged, prompting a surge in innovative attention designs aimed at enhancing efficiency without compromising performance. The Long Range Arena (LRA) has emerged as a primary platform for evaluating these advancements, serving as a standardized benchmark to gauge progress. However, our recent exploration uncovers a concerning truth—when properly tuned, the disparities in performance among various transformer variants within the LRA become negligible, casting doubt on the validity of this benchmark. Furthermore, the LRA predominantly focuses on self-attention scenarios, overlooking the critical roles of cross attention and causal attention in real-world applications. To address these limitations, we introduce a new comprehensive attention benchmark (CAB) tailored for long sequence modeling. At the heart of CAB lies a detailed attention taxonomy, delineating distinct attention patterns based on their functional roles in conditionality and causality modeling. We then curate a diverse set of real-world tasks spanning multiple domains, including computer vision, natural language processing, speech processing, and time series forecasting. Through rigorous experimentation, we not only validate the effectiveness of CAB but also reveal valuable insights into the design of efficient attention mechanisms. Our findings underscore the importance of addressing the inherent inefficiencies in current attention methods and point toward promising avenues for future research in efficient attention design. CAB, along with all associated materials, is openly accessible for further investigation.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue and collaboration within the community.  Anonymity is maintained to encourage open dialogue"}
{"paper_id": 451, "abstract": "In the realm of reinforcement learning, where the complexities of action spaces can stymie progress, we introduce Neural Discrete Reinforcement Learning (NDRL), a pioneering framework designed to distill the intricacies of high-dimensional continuous or hybrid action spaces into manageable discrete representations. At the heart of NDRL lies a novel Action Discretization Variational Auto-Encoder (AD-VAE), a method that not only learns to encode the original action space into a compact latent space but also decodes it back into the realm of the discrete. This transformation is achieved through a carefully crafted state-conditioned action encoder and decoder, bolstered by the power of Graph Neural Networks and the precision of soft-argmax operations. The result is a rich, detailed latent space that retains the essential characteristics of the original action landscape. To navigate this newfound discrete space effectively, we employ a technique known as action remapping, which allows for seamless transitions between the latent and original action spaces. Additionally, we introduce ensemble Q-learning, a robust mechanism that ensures the integrity of the learning process. Together, these innovations enable NDRL to tackle a wide array of challenging action spaces with remarkable ease. Our empirical evaluations across diverse benchmarks, including MuJoCo, Gym Hybrid, HardMove, and GoBigger, underscore the effectiveness and adaptability of our approach, demonstrating its potential to revolutionize the field of reinforcement learning."}
{"paper_id": 452, "abstract": "[Abstract]\n\nDeep neural networks have revolutionized the landscape of artificial intelligence, achieving remarkable performance across a multitude of applications. However, their success often comes at a price—the infamous \"black box\" problem, which leaves us grappling with the question of what precisely drives these models' decisions. This opacity poses significant challenges when deploying AI in critical domains like healthcare and autonomous vehicles, where the stakes are unacceptably high. In light of these concerns, we introduce Multilevel Explainable Artificial Intelligence (Multilevel XAI), a groundbreaking framework designed to demystify the inner workings of deep neural networks. At its core, Multilevel XAI employs per-class attributes, readily accessible through online platforms or automated tools, to illuminate the decision-making process within DNNs. By generating linguistic salient attributes alongside corresponding attribute-wise saliency maps, our approach achieves a level of clarity that closely mirrors human explanation methods. This innovation not only addresses the shortcomings of existing XAI techniques but also offers a more intuitive understanding of DNN behavior. Through rigorous experimentation on both coarse-grained and fine-grained datasets, we demonstrate the robustness and efficacy of our proposed framework, paving the way for more transparent and trustworthy AI systems in the future."}
{"paper_id": 453, "abstract": "[Abstract]\n\nIn recent years, Physics-Informed Neural Networks (PINNs) have emerged as a promising tool for tackling the formidable challenge of solving Partial Differential Equations (PDEs). Yet, despite their remarkable success in numerous applications, PINNs often falter in the face of high-frequency target functions, succumbing to the insidious phenomenon known as spectral bias. This issue poses a significant hurdle for the practical deployment of these innovative methodologies. \n\nTo address this critical concern, we delve into the heart of spectral bias, uncovering its origins within the dynamics of PINNs. Our analysis reveals that the optimization process, particularly under vanilla Gradient Descent (GD), tends to gravitate towards low-frequency modes, effectively diminishing the influence of high-frequency components. We rigorously demonstrate that, under GD, the rate at which PINNs learn high-frequency features is inversely proportional to the square of the frequency index, rendering the convergence to accurate solutions prohibitively slow.\n\nIn response to this limitation, we introduce the concept of momentum within the optimization framework of PINNs. Our findings reveal that incorporating momentum significantly accelerates the learning process for high-frequency features, thereby mitigating the adverse effects of spectral bias. Moreover, our investigation into the Adam optimizer—long noted for its superior performance in training deep neural networks—uncovers additional benefits: it not only enhances the learning rates for high-frequency modes but also exhibits robustness across various initializations, further bolstering its appeal.\n\nTo substantiate our theoretical insights, we present a comprehensive suite of numerical experiments, leveraging sufficiently wide networks to provide concrete evidence of our claims. These results not only validate our theoretical findings but also offer a pathway forward for enhancing the effectiveness of PINNs in tackling complex, high-frequency PDEs. As we continue to explore the landscape of PINNs, the understanding gleaned from this work serves as a beacon, illuminating the path toward more accurate and efficient solutions in the realm of physics-inspired machine learning."}
{"paper_id": 454, "abstract": "In this paper, we introduce radial neural networks, a novel architecture that employs radial rescaling activations—a departure from the conventional pointwise nonlinearities. These activations, which are based on rescaling each vector by a scalar dependent solely on its norm, offer a fresh perspective on neural network design. Our investigation reveals several compelling properties of radial neural networks. First, we establish their universal approximation capabilities, demonstrating that they can effectively approximate asymptotically affine functions with bounded width. This is achieved through a unique approach that diverges from traditional methods, offering insights into the potential of radial networks for various applications. Second, we delve into the inherent compressibility of radial neural networks, uncovering a lossless compression algorithm that systematically factors out orthogonal symmetries through iterated QR decompositions. This process yields a compressed network with fewer neurons in each hidden layer, yet retains the original model's performance. Importantly, we prove that the loss incurred by the compressed model after a single step of gradient descent mirrors the loss of the original model under projected gradient descent. This relationship underscores the robustness of our compression technique. Finally, we present empirical evidence from real-world datasets that validates our theoretical findings and showcases the superior performance of radial networks over pointwise networks in a noisy image recovery task. Together, these contributions highlight the transformative potential of radial neural networks in advancing the field of deep learning."}
{"paper_id": 455, "abstract": "In the realm of Federated Learning (FL), a captivating parallel emerges between the centralized mini-batch Stochastic Gradient Descent (SGD) and the intricacies of FL itself. This analogy invites us to explore the fundamental training dynamics at play, offering new insights into the nature of FL. In this work, we embark on a journey to uncover these dynamics, focusing on two pivotal aspects: client coherence and global weight shrinking. \n\nClient coherence, much like the notion of sample coherence in mini-batch SGD, sheds light on the impact of the relationships between clients on the performance of the global model. We delve into the nuances of local gradient coherence and heterogeneity coherence, revealing that optimizing for coherence during specific phases of training can significantly enhance the final outcome. \n\nMoreover, we investigate the role of global weight shrinking, drawing parallels with the weight decay techniques employed in mini-batch SGD. We find that adjusting the weights of the global model through controlled shrinking not only stabilizes the training process but also contributes to improved generalization. The magnitude of the global gradient serves as a guide for determining the optimal shrinking factor, with larger norms necessitating stronger regularization. \n\nOur exploration culminates in the development of FEDAWO, a straightforward yet powerful method for optimizing server-side aggregation weights. By leveraging the insights gained from understanding client coherence and global weight shrinking, FEDAWO enhances both the attentiveness of aggregation weights and the effectiveness of global weight shrinking. Consequently, it elevates the overall performance of the global model, paving the way for more robust federated learning solutions. Through this work, we hope to illuminate the path forward for those seeking to harness the full potential of FL."}
{"paper_id": 456, "abstract": "In the realm of natural language processing, where the interplay between text and metadata often goes unnoticed, we unveil a novel approach that harnesses the power of transformers to revolutionize message classification and regression tasks. Our innovation lies in constructing a modular architecture composed of distinct blocks, each tailored to handle various types of inputs, including the intricate complexities of metadata. This design allows our model to seamlessly integrate diverse sources of information, such as timestamps, authors, and even attached images, into its decision-making process. \n\nAt the heart of our architecture is a transformer block dedicated to capturing the nuanced relationships within text, while specialized blocks adeptly process the metadata, transforming it into valuable insights. We meticulously train our model on the text using transfer learning techniques, allowing it to leverage pre-existing knowledge from vast corpora. This synergy of training across different blocks enables our model to uncover hidden dependencies and patterns that would otherwise remain obscured.\n\nTo validate the efficacy of our approach, we conducted rigorous experiments on a variety of publicly available datasets, pitting our method against established benchmarks like BERT, Random Forest classifiers, and Multi-Layer Perceptrons. The results speak volumes: our method not only matches but often surpasses these benchmarks, demonstrating its superior ability to distill meaningful information from the metadata. In doing so, we pave the way for a new era in message classification, where the full potential of metadata is finally realized."}
{"paper_id": 457, "abstract": "In the realm of video-language pre-training, the quest for efficiency is paramount, yet it often comes at the cost of performance, leaving many researchers behind in the race for innovation. In this paper, we unveil a groundbreaking approach that not only streamlines the process but also enhances performance across multiple downstream tasks—without compromising accuracy. At the heart of our method lies a novel combination of offline region features and fine-grained cross-modality alignment regularization. By harnessing the power of pre-extracted region features, we significantly reduce the complexity of attention mechanisms, allowing our model to achieve remarkable results even when trained on just 80% of the standard dataset size. Additionally, we introduce a bidirectional region-word alignment regularization strategy, which meticulously refines the relationships between regions and words, ensuring that our model remains robust and adaptable. Through rigorous testing, we demonstrate that this innovative approach can even enhance the performance of end-to-end video-language pre-training models, surpassing their original capabilities. With this breakthrough, we hope to democratize the field of video-language pre-training, making it accessible to a broader community of researchers."}
{"paper_id": 458, "abstract": "In the realm of federated learning, the quest for privacy-preserving methods continues to captivate researchers. While many approaches have emerged to tackle machine learning challenges with single-level structures, there remains a significant gap in addressing the more complex landscape of bilevel programming—specifically, hyper-representation learning and hyperparameter tuning. In this paper, we introduce two innovative solutions: BAMBI and BAMBI-DP, designed to navigate the intricate world of vertical federated bilevel optimization (VFBO) problems. Our key contributions lie in the development of a stochastic Bilevel Optimization Method equipped with a clever JacoBian estImator, allowing for the efficient and privacy-preserving computation of hypergradients. Moreover, we enhance this method with differential privacy, ensuring robust protection against label privacy breaches. Through rigorous theoretical analysis, we demonstrate that BAMBI maintains an optimal convergence rate, while BAMBI-DP achieves an impressive $(\\varepsilon, 0)$-differential privacy guarantee relative to the labels. Our empirical evaluations further validate the efficacy of these methods, paving the way for new possibilities in the intersection of privacy and machine learning."}
{"paper_id": 459, "abstract": "[Abstract]\nIn the realm of decentralized multi-agent online learning, a fascinating new frontier emerges: the study of consecutive-play games, where players take turns to select actions and incur associated rewards. This setting invites a fresh exploration into the intricate dance of learning and adaptation among agents. Our primary objective? To unravel the complexities of learning in this environment, providing a robust framework for understanding the interplay between individual and collective performance.\n\nWe embark on this journey by introducing the concept of joint pseudo-regret—a measure that captures the cumulative reward where all players observe the same bandit reward. In doing so, we reveal the hidden connections within this intricate system. The heart of our contribution lies in devising an innovative online learning algorithm specifically tailored for general consecutive-play games. This algorithm hinges on the application of no-regret algorithms, originally designed for the single-player multi-armed bandit challenge, allowing players to adapt and optimize their strategies over time.\n\nThe true power of our approach is revealed through rigorous theoretical analysis. We demonstrate that our algorithm achieves a joint pseudo-regret bound that is on par with the state-of-the-art in the single-agent multi-armed bandit domain. Furthermore, we delve into the stochastic and adversarial settings, proving that our algorithm is indeed no-regret in both scenarios. This groundbreaking work not only advances our understanding of decentralized learning but also opens new avenues for practical applications in complex, dynamic environments."}
{"paper_id": 460, "abstract": "In this paper, we introduce OptCtrlOP, a groundbreaking operator learning framework designed to solve optimal control problems (OCPs). By embracing an innovative instance-solution operator perspective, OptCtrlOP adeptly addresses the inherent challenges associated with conventional numerical solvers. This approach enables us to learn direct mappings from OCP instances to their corresponding solutions, eliminating the need for explicit system dynamics or iterative optimization processes during inference. \n\nOur theoretical underpinnings are robust, grounded in the application of Pontryagin's Maximum Principle, which allows us to transform the original problem into a more tractable boundary value problem. This transformation is pivotal in establishing the feasibility of our operator learning approach. Moreover, we provide rigorous bounds on the approximation error, further solidifying the reliability of our method.\n\nEmpirical evaluations across a diverse array of synthetic and real-world systems underscore the versatility of OptCtrlOP. It demonstrates remarkable performance, achieving up to 100x speedup compared to traditional multi-layer perceptron baselines and an astounding 10Kx speedup relative to classical direct method solvers in synthetic environments. Perhaps most impressively, OptCtrlOP exhibits strong generalization capabilities, consistently delivering accurate solutions across both in-distribution and out-of-distribution OCP instances. Thus, OptCtrlOP represents a significant leap forward in the realm of optimal control problem-solving."}
{"paper_id": 461, "abstract": "[Abstract]\n\nRecent studies have revealed the critical learning (CL) periods phenomenon, wherein the early stages of training significantly influence the performance of deep neural networks (DNNs). This discovery has profound implications for federated learning (FL), where clients' data often exhibit substantial statistical and system-level heterogeneity. In this work, we introduce FedCL—a CL periods-aware FL framework designed to enhance model generalization and robustness. By capitalizing on the CL periods concept, FedCL dynamically adjusts the number of participating clients in each FL training round. Specifically, it increases the client participation rate during the initial training phases and decreases it as the training progresses. This approach not only mitigates the detrimental effects of statistical heterogeneity but also addresses the challenges posed by system heterogeneity. Our extensive experiments across diverse datasets and model architectures demonstrate that FedCL consistently outperforms existing methods, achieving superior model performance and robustness. Additionally, we provide theoretical guarantees for FedCL, further validating its effectiveness in optimizing FL training. By integrating CL periods awareness into FL, FedCL represents a significant stride towards more efficient and effective federated learning solutions."}
{"paper_id": 462, "abstract": "In this paper, we introduce OBPOSE, a groundbreaking object-centric generative model designed to revolutionize the landscape of 3D scene inference. This model is uniquely equipped to handle RGB-D images, transforming them into a rich tapestry of object-centric representations that illuminate the complexities of the underlying scene.  At the heart of OBPOSE lies a novel inductive bias: the concept of pose. By leveraging this bias, OBPOSE adeptly decomposes scenes into their constituent parts, unraveling the intricate relationships between objects.  What sets OBPOSE apart is its innovative approach to pose estimation, where the tightest bounding box that encloses an object becomes the key to uncovering its location and orientation. This method not only streamlines the learning process but also enhances the accuracy of our scene understanding.  To facilitate this process, OBPOSE employs a voxelized approximation technique that distills an object's shape from a neural radiance field, allowing for efficient integration into the training framework. This approach ensures that the model can swiftly adapt and refine its understanding of the scene.  In our extensive evaluations, OBPOSE emerges as a clear leader, surpassing the current benchmarks in 3D scene inference. Our experiments, conducted on datasets such as CLEVR, MultiShapeNet, and the YCB dataset, consistently demonstrate the robustness and versatility of OBPOSE. Through this work, we pave the way for more sophisticated and nuanced understanding of complex 3D environments."}
{"paper_id": 463, "abstract": "In the realm of supervised learning, the quest for high-quality labels often leads us down the path of crowdsourcing, where the collective wisdom of many shines bright. Yet, amidst this brilliance, lies a subtle yet significant challenge—some tasks prove too ambiguous for even the most diligent crowdworker, potentially undermining the entire learning process. In this paper, we introduce a novel concept: the Weighted Area Under the Margin (WAUM). This innovative metric allows us to pinpoint the most problematic tasks within a crowdsourced dataset, enabling us to prune away those that could otherwise jeopardize our models' performance. By embracing the power of crowdsourcing while remaining vigilant against the pitfalls of ambiguity, we pave the way for more robust and reliable machine learning systems."}
{"paper_id": 464, "abstract": "[Abstract]\n\nIn the quest to harness the power of self-supervised learning from video data, we draw upon the rich tapestry of insights woven by cognitive science and neuroscience. This journey begins with the intricate dance of saccadic eye movements and fixations, fundamental components of human visual perception. These phenomena offer a unique window into how we navigate our environment, serving as potent indicators of semantic changes within the visual landscape. By embracing this bio-inspired perspective, we introduce a novel video self-supervised learning (SSL) framework that meticulously captures these semantic shifts.\n\nAt the heart of our approach lies a sophisticated contrastive learning mechanism. Here, we employ saccades as a global synaptic modulator, enhancing the distinctions between semantically diverse frames. Yet, we do not stop at mere differentiation; we delve deeper into the realm of semantic consistency. By minimizing prediction errors between the current and future states within the same fixation period, we ensure that our learned representations remain harmoniously aligned with the evolving semantics of the scene. This innovative strategy mirrors the way humans process visual information, where consistency across fixations plays a pivotal role in understanding the world around us.\n\nMoreover, we recognize that the human brain's ability to reorganize learned representations is crucial for adaptation and learning. To emulate this capability, we integrate prototypical contrastive learning into our framework. This allows us to systematically redistribute and refine the learned representations, thereby strengthening the associations among perceptually similar entities. In essence, we empower our models to reorganize knowledge in a manner that closely resembles human cognition, fostering a more robust understanding of the visual world.\n\nOur experiments validate the efficacy of our bio-inspired video SSL method. On the challenging UCF101 dataset, we achieve significant improvements in top-1 video retrieval accuracy. Additionally, we demonstrate its versatility across downstream tasks, including action recognition. By bridging the gap between biological insights and artificial intelligence, we pave the way for more advanced and intuitive self-supervised learning algorithms capable of rivaling human perceptual prowess."}
{"paper_id": 465, "abstract": "In the realm of Multi-Agent Reinforcement Learning (MARL), the pursuit of cooperation among agents remains a formidable challenge, especially when dealing with sparse reward signals. In this paper, we introduce a groundbreaking approach called Skilled Population Curriculum (SPC), designed to tackle the complexities of large-scale MARL through the lens of automatic curriculum learning. At its heart, SPC ingeniously combines the concepts of ACL and hierarchical MARL, fostering the acquisition of versatile skills that seamlessly transfer across a diverse range of tasks.  Our approach centers on a teacher-student framework, where the teacher navigates the complexities of non-stationarity by employing a contextual bandit strategy. To ensure adaptability, we harness the power of transformers, enabling the teacher to generate curricula that accommodate an arbitrary number of agents within a single task. Meanwhile, the student leverages a skill-based framework to master various low-level policies, all while communicating through a self-attention mechanism.  Empirical evidence from rigorous testing on both the Multi-Particle Environment (MPE) and the Google Research Football (GRF) benchmarks demonstrates the remarkable efficacy of our method. Notably, our approach outperforms previous state-of-the-art techniques, showcasing its potential to revolutionize the landscape of MARL. As we continue to explore this innovative path, the future of cooperative learning in complex environments looks brighter than ever before."}
{"paper_id": 466, "abstract": "In the vast expanse of high-dimensional spaces, the curse of dimensionality looms large, threatening the efficiency of learning algorithms. Yet, within the realm of deep convolutional neural networks (CNNs), a surprising resilience emerges—these networks often manage to learn complex functions with remarkable speed. In this paper, we delve into the heart of this mystery, uncovering the conditions under which deep CNNs can triumph over the curse of dimensionality.  We embark on a journey through the landscape of kernel ridge regression, employing rigorous mathematical techniques to explore the generalization capabilities of deep hierarchical kernels. Our findings reveal that deep CNNs exhibit a remarkable adaptability, capable of learning functions with an effective dimensionality significantly lower than their apparent complexity. Moreover, we extend our analysis to the realm of ridgeless regression, leveraging insights from the replica method to bolster our understanding.  Through our investigations, we demonstrate that the effectiveness of deep CNNs hinges on the spatial locality of the target function. When the target function is localized or can be decomposed into localized components, deep CNNs shine, achieving generalization rates that are nearly optimal in the context of kernel ridge regression. However, when confronted with hierarchical functions generated by deep CNNs themselves, the same networks falter, unable to harness their full potential.  To validate our theoretical findings, we conduct extensive numerical experiments, confirming the robustness of our conclusions even when the non-overlapping patches assumption is relaxed. In this way, we illuminate the path forward for understanding the profound capabilities of deep CNNs in the face of high-dimensional challenges."}
{"paper_id": 467, "abstract": "In the realm of computer vision, traditional instance segmentation techniques have long been confined to a narrow path, assuming that the objects within images could be neatly categorized into a predetermined set of classes—a world we now term \"closed.\" Yet, reality paints a vastly different picture. In the ever-evolving landscape of open-world instance segmentation, where the boundaries blur and new object classes emerge without warning, the task of identifying and delineating each entity becomes an intricate challenge.   To navigate this complex terrain, we introduce a pioneering approach: Transformer-based Open-world Instance Segmentation (TOIS). This method leverages the power of transformers, a revolutionary architecture that has reshaped the field of deep learning, to tackle the formidable challenges of open-world segmentation head-on.   A central feature of our method is a novel regularization module designed to counteract the significant hurdle posed by incomplete annotations—a common occurrence in open-world scenarios. By meticulously crafting a cross-task consistency loss, we ensure that the predicted foreground map harmonizes with the collective output of all instance masks. This innovative strategy not only enhances the robustness of our model but also empowers it to recover even those instances that might have been overlooked during the annotation process.   Moreover, we extend the capabilities of TOIS into the semi-supervised domain, where labeling every image becomes prohibitively expensive. By integrating unlabeled images into our training regimen, we significantly reduce the reliance on fully annotated examples, making our approach not only more efficient but also more practical for real-world applications.   The results speak volumes. Through rigorous experimentation, we have demonstrated that TOIS outperforms existing state-of-the-art methods, achieving superior accuracy in both fully supervised and semi-supervised settings. This breakthrough paves the way for more reliable and versatile open-world instance segmentation systems, ready to tackle the complexities of tomorrow's visual challenges."}
{"paper_id": 468, "abstract": "In the realm of reinforcement learning, where the quest for optimal policies unfolds, we encounter a formidable challenge: the presence of uncertainty in the transition kernel—a phenomenon that can render our carefully crafted solutions obsolete. This uncertainty, lurking like a shadow, threatens the very foundation of our algorithms, leading us down a path fraught with potential failures. In response to this pressing issue, we introduce the concept of robust constrained reinforcement learning, a framework designed to navigate the treacherous waters of transition kernel ambiguity. Here, we define a robust value function as the infimum of the expected cumulative rewards across all possible environments within a given uncertainty set. Our exploration delves into the intriguing properties of this function, revealing a landscape of dual problems and saddle-point formulations. With a focus on practicality, we present a robust primal-dual algorithm tailored for this setting, complete with theoretical guarantees of convergence and robust feasibility. Furthermore, we extend our findings to the realm of model-free learning, crafting an online algorithm that leverages actor-critic methods. Through rigorous analysis, we demonstrate the effectiveness of our approach in maintaining robustness even as the environment shifts. Finally, we invite you to join us as we push the boundaries of robust constrained reinforcement learning, illuminating new pathways for future exploration."}
{"paper_id": 469, "abstract": "In the realm of reinforcement learning (RL), where agents navigate through complex environments to maximize rewards, a significant challenge emerges when dealing with unstable systems. This instability often manifests as a growing trajectory of costs that can render standard policy optimization techniques ineffective, particularly those reliant on gradient-based methods. In this paper, we delve into the heart of this issue, focusing on the convergence of policy optimization for RL problems that exhibit instability. Our theoretical analysis reveals a troubling truth: the convergence rate of these methods can be excruciatingly slow, a direct consequence of the large spectral radius inherent in the Hessian matrix. To combat this inefficiency, we introduce a novel approach—a logarithmic mapping of the loss function. This transformation not only accelerates the convergence rate but also enhances the robustness of the optimization process. We support our claims with rigorous theoretical foundations and compelling experimental evidence, demonstrating the effectiveness of our method across a range of scenarios, from linear quadratic regulator (LQR) problems to more intricate nonlinear cases employing neural network policies. Through this innovative lens, we hope to illuminate new paths forward in the challenging landscape of RL for unstable systems."}
{"paper_id": 470, "abstract": "[Abstract]\n\nIn the realm of computer vision, where complex patterns weave through the fabric of pixels, we unveil a groundbreaking approach: the Sparse Low-Dimensional Decision Model, or SLDD-Model. This innovative method marries the power of deep learning with the clarity of interpretable models, crafting a framework that not only excels in accuracy but also opens the door to understanding the intricate decisions made by machines. \n\nAt the heart of our approach lies a meticulously crafted feature selection process, followed by the application of a generalized linear model with LASSO regularization—a technique that elegantly distills the essence of our data into a sparse, low-dimensional representation. This reduction not only streamlines the decision-making process but also enhances the global and local interpretability of our model, making it a beacon for those seeking transparency in AI.\n\nWhat sets our SLDD-Model apart is its ability to identify a single class with a mere handful of features—just five, to be precise. This simplicity is not a limitation but a strength, allowing us to delve deeply into each feature and uncover the meaningful attributes that drive our model's decisions. Through this lens, we reveal the human concepts that underpin our model's choices, bridging the gap between machine intelligence and human understanding.\n\nWe further enhance our methodology by introducing a novel feature diversity loss, designed specifically to foster the richness and variety of features within our model. This innovation is particularly vital when dealing with classes that hinge on a limited set of features, ensuring that the information landscape remains expansive and informative.\n\nOur experiments, conducted across four prominent benchmark datasets in the domain of fine-grained image classification as well as the vast expanse of ImageNet-1K, validate the robustness and competitiveness of our proposed method. Moreover, we demonstrate the remarkable connection between many of the features learned by our model and the attributes that humans rely upon in their own decision-making processes.\n\nIn this era of increasing reliance on artificial intelligence, our SLDD-Model stands as a testament to the power of interpretability, offering a path forward for those who seek to trust and understand the decisions made by machines."}
{"paper_id": 471, "abstract": "In the realm of aerial robotics, where the dance between unmanned aerial vehicles (UAVs) and the ever-changing wind fields unfolds, the quest for precise trajectory tracking becomes a formidable challenge. Traditional approaches often falter when faced with the uncertainties of real-world conditions, leaving us to grapple with the limitations of data-driven methodologies. In this paper, we introduce the OoD-Control framework—a groundbreaking solution designed to empower UAVs to maintain stability and achieve accurate control, even amidst the tumultuous shifts in environment domains. \n\nOur approach hinges on a novel formulation: we represent the unknown dynamics of the environment as a series of domain-dependent functions, each encapsulating the complexities of the specific environment at hand. To tackle the inherent challenges of domain generalization, we employ a sophisticated mixture density network, meticulously trained using data collected across a diverse array of environments. This allows us to not only approximate the unknown dynamics but also to derive rigorous bounds on the prediction errors, ensuring robustness and reliability.\n\nWe substantiate our theoretical findings through extensive simulations, showcasing the remarkable performance of OoD-Control in comparison to existing state-of-the-art methods. Our results demonstrate that, even when faced with significant deviations in environment distribution, OoD-Control maintains its efficacy, providing a beacon of hope for the future of UAV control in the face of uncertainty."}
{"paper_id": 472, "abstract": "[Abstract]\n\nIn the realm of artificial intelligence, the quest for generating lifelike talking head videos stands as a testament to the ever-evolving capabilities of deep learning. Yet, despite significant advancements, the challenge persists: how do we bridge the gap between a static source image and the dynamic intricacies of a driving video? In this paper, we unveil a groundbreaking approach—our Implicit Scale Conditioned Memory Compensation Network (MCNet)—designed to confront the ambiguities inherent in the talking head generation process. At its core, MCNet harnesses the power of an innovative meta memory bank, meticulously crafted to encapsulate the universal essence of human faces. This memory bank serves as a repository of the most common and salient facial features across all training images, enabling a seamless transfer of critical information. Furthermore, we introduce the concept of an implicit scale conditioned memory module, which adeptly adjusts the scale of the meta memory bank based on the detected discrete facial keypoints, ensuring that even the most nuanced facial details are preserved. The culmination of these innovations lies in our memory compensation module, where we dynamically refine the warped feature map through cross-attention mechanisms, drawing upon the rich tapestry of the scale-aware memory bank to enhance the final product. Extensive empirical evaluations conducted on two leading benchmarks—the VoxCeleb and CelebV datasets—demonstrate the remarkable efficacy of our proposed method. Notably, MCNet not only outperforms previous state-of-the-art techniques but also exhibits robust generalizability across various talking head models. In this way, we have taken a significant stride toward crafting talking head videos that are not only visually stunning but also truly reflective of the human experience."}
{"paper_id": 473, "abstract": "In the realm of reinforcement learning, tackling innumerable action spaces—be they vast discrete sets or continuous landscapes—poses a formidable challenge. Traditional approaches often fall short, particularly when confronted with the intricate complexities that arise from the mismatch between action representations and their impact on the task at hand. To address this conundrum, we introduce FLAIR, a groundbreaking framework that reimagines the very essence of action retrieval through the lens of listwise reinforcement learning. At its core, FLAIR allows for the dynamic construction of a list of k candidate actions, all without the need to exhaustively explore every conceivable combination. This innovative approach not only streamlines the retrieval process but also empowers the selection phase to harness the power of a rich array of potential actions. We further enhance FLAIR by adapting the cascaded deep deterministic policy gradient (DDPG) method, ensuring that both the retrieval and selection phases are seamlessly integrated. Through rigorous experimentation across a diverse array of environments—from recommender systems and a novel mine-world setting to continuous control challenges—we demonstrate that FLAIR significantly outperforms existing methods, showcasing its prowess in navigating the treacherous waters of innumerable action spaces. With FLAIR, we pave the way for more robust and efficient reinforcement learning solutions in the face of complexity."}
{"paper_id": 474, "abstract": "In this study, we delve into the intricate relationship between semantic word information and the robustness of deep models, employing the canonical correlation analysis (CCA) method to illuminate this connection. Our findings reveal that the robustness of a model is directly correlated with the strength of the relationships between the visual representation and the associated semantic word vector. Additionally, we explore the structural relevance between visual representations and word vectors, uncovering a compelling link between the robustness of image features and the semantic associations they convey. To harness the power of this insight, we introduce a novel framework called Semantic Constraint Adversarial Robust Learning (SCARL). This framework enhances both the distributional and structural information derived from semantic word vectors through mutual information optimization and geometric constraints, ultimately boosting the robustness of deep models. Through rigorous experimentation on established benchmarks, we demonstrate that SCARL outperforms existing state-of-the-art methods, underscoring the pivotal role of semantic information in enhancing robustness."}
{"paper_id": 475, "abstract": "In recent years, foundation models have emerged as powerful tools for tackling a wide array of tasks, yet they often come at the cost of immense computational resources. To address this challenge, knowledge distillation has been explored as a means to harness the capabilities of these large models without the need for substantial hardware. In this work, we delve into the nuances of knowledge distillation from the CLIP model (Radford et al., 2021), a foundation model that has garnered significant attention. Our experiments reveal that straightforward knowledge distillation from CLIP fails to yield optimal results, prompting us to investigate the underlying reasons. Through a series of ablation studies, we uncover the critical role of the training paradigm employed by CLIP in enhancing the effectiveness of knowledge distillation. Furthermore, we demonstrate that CLIP exhibits robustness against the capacity gap issue—a phenomenon where performance gains diminish as the disparity between teacher and student models widens. This resilience is particularly pronounced when the amount of training data is limited, suggesting that the training regimen of CLIP plays a pivotal role in overcoming this hurdle. Drawing from these insights, we propose a novel approach to distill knowledge from CLIP, resulting in a lightweight MobileNetV3 model that outperforms previous methods significantly. Our findings not only shed light on the potential of CLIP for knowledge distillation but also offer a pathway for more efficient deployment of foundation models in practical applications."}
{"paper_id": 476, "abstract": "Abstract\n\nThe realm of deep learning has witnessed remarkable advancements, yet it often grapples with challenges such as an insatiable appetite for vast quantities of labeled data, a tendency toward brittleness when faced with novel tasks, and a reluctance to embrace human expertise. In response, researchers have sought to bridge the gap between the power of deep learning and the precision of symbolic systems, resulting in a wave of hybrid approaches. However, many of these efforts fall short, either failing to fully integrate the strengths of both paradigms or struggling to scale effectively.\n\nIn this paper, we introduce a pioneering hybrid framework designed to harness the synergies of symbolic and subsymbolic reasoning. This innovative approach leverages a rich tapestry of prior knowledge, grounded in logical rules that are meticulously crafted from the nuances of natural language descriptions. Our method seamlessly fuses this knowledge with image data through a sophisticated embedding technique, enabling the seamless integration of two distinct modalities—logic and images—into a unified multimodal fusion framework. This fusion empowers the model to reason more effectively about the intricate details of indoor scenes, thereby enhancing its performance in scene classification tasks.\n\nTo validate our approach, we conduct extensive experiments on the challenging SUN dataset, which encompasses a wide array of indoor scenarios. The results speak volumes: our method demonstrates significant improvements in classification accuracy, outperforming traditional image classification techniques and rivaling the capabilities of the most advanced deep learning models in the field. By bridging the worlds of symbolic and subsymbolic reasoning, we pave the way for a new era of intelligent systems that can learn and adapt with unprecedented efficiency and effectiveness."}
{"paper_id": 477, "abstract": "[Abstract]\n\nIn the ever-evolving landscape of video recognition, the pursuit of robust models capable of generalizing beyond their training domains remains a formidable challenge. In this paper, we introduce a novel data augmentation technique, dubbed Ghost Motion (GM), designed to enhance the generalization capabilities of existing video recognition models. At its core, GM ingeniously shifts the channels along the temporal dimension, thereby infusing motion information into adjacent frames. This innovative approach not only enriches the temporal representations but also fosters the propagation of motion cues throughout the video sequence, bolstering the model's ability to discern salient frames.\n\nThrough rigorous experimentation on diverse datasets, including Kinetics, Something-Something V1 & V2, and Epic-Kitchens-100, we demonstrate that GM consistently elevates the performance of various 2D video recognition methods. Furthermore, we uncover a complementary strategy—smoothing the logits via a temperature scaling parameter—that serves to mitigate overconfident predictions and further refine the calibration of the models. Our findings reveal that GM, when combined with established image-level data augmentation techniques, yields remarkable gains in performance, underscoring its versatility and effectiveness.\n\nIn summary, Ghost Motion stands as a promising advancement in the realm of video recognition, offering a straightforward yet powerful solution to the perennial problem of overfitting while enhancing the overall generalization prowess of modern video recognition frameworks."}
{"paper_id": 478, "abstract": "In the realm of Graph Neural Networks (GNNs), the art of sampling subgraphs stands as a crucial cornerstone in the training process. However, the intricate interplay between node-based and layer-based sampling methods often leads to suboptimal outcomes. In this paper, we introduce a novel approach: Layer and Neighbor Aware Batch Optimization Routine (LABOR), a sophisticated sampling algorithm designed to harness the strengths of both worlds.  LABOR ingeniously employs Poisson Sampling to forge a correlation between the sampling processes of seed nodes, ensuring that the sampled vertices from different seeds share ample overlap. This strategic maneuver not only mitigates the Neighborhood Explosion Phenomenon (NEP) but also results in remarkable reductions in computation, memory usage, and communication costs—up to a staggering 7×. Perhaps most compellingly, LABOR retains the simplicity of neighbor sampling's hyperparameters, making it a seamless replacement for existing methods.  Our extensive experimental evaluations underscore the efficacy of LABOR, demonstrating its superiority over both neighbor sampling and layer sampling techniques. Notably, LABOR achieves a significant performance boost, with a speedup of up to 2.6×. Moreover, it allows for an impressive increase in batch size, reaching up to 112× larger than neighbor sampling, all while maintaining the same level of vertex sampling. In the grand tapestry of GNN training, LABOR emerges as a beacon of innovation, paving the way for more efficient and effective learning on graph-structured data."}
{"paper_id": 479, "abstract": "[Pioneering Exploration into the Role of Positional Information Patterns from Padding in Neural Networks] In the realm of neural networks, padding stands as a cornerstone, yet its intricate dance with positional information remains largely uncharted. This study embarks on a journey to unveil the secrets of positional information patterns from padding (PPPs)—the subtle yet profound ways in which padding shapes the very fabric of these models. Our exploration reveals that while traditional methods fall short in consistency, we have uncovered a robust and reliable technique for discerning the presence and magnitude of PPPs. Through meticulous analysis, we delve into the origins and evolution of PPPs within a variety of neural network architectures. We also highlight the significant disparity between PPPs in pre-trained networks compared to their nascent stages, shedding light on the critical role of unbiased training in mitigating the detrimental effects of PPPs across a wide array of vision tasks. Join us as we illuminate this previously overlooked aspect of neural network design."}
{"paper_id": 480, "abstract": "In the realm of complex robotic tasks, the quest for effective long-horizon policy learning often feels like navigating through a labyrinth without a map. Traditional reinforcement learning methods, while valiant, struggle to generalize beyond the confines of training environments, leaving researchers in search of more robust solutions. In this paper, we introduce TRajectory TRanslation (TR²), a groundbreaking approach that bridges the gap between high-level abstract trajectories and low-level executable ones. This innovative method leverages the power of sequence-to-sequence translation, a technique previously honed in the art of natural language processing, to convert abstract trajectories generated by high-level agents into executable strategies for novel tasks. \n\nWhat sets TR² apart is its remarkable ability to function seamlessly even when the high-level and low-level state representations do not match perfectly. By utilizing a powerful seq-to-seq model, we can effectively translate abstract trajectories into executable actions, ensuring that our robots can tackle unforeseen challenges with grace. To validate the efficacy of our approach, we conducted rigorous evaluations across a variety of tasks and environments, including both navigation-based and manipulation-oriented scenarios. Our findings reveal that TR² not only outperforms baseline methods but also demonstrates impressive resilience against adversarial interventions. Furthermore, we successfully deployed TR² in real-world applications, showcasing its potential to revolutionize the way robots navigate and interact with their surroundings. With TR², we take a significant step forward in equipping robots with the adaptability and intelligence needed to thrive in the ever-evolving landscape of robotic technology."}
{"paper_id": 481, "abstract": "[Abstract]\n\nIn the rapidly evolving landscape of machine learning, the ability to interpret model predictions remains a critical challenge. As complex models such as deep neural networks become increasingly prevalent, understanding their inner workings becomes paramount for ensuring trust and reliability. In response to this pressing need, we introduce k-width and Bifold Embedded Data Ordered Neural Network (kaBEDONN), a novel post-hoc Explainable Artificial Intelligence (XAI) method designed to elucidate model predictions through the presentation of relevant data. This approach is grounded in the principle that data itself can serve as the most compelling form of explanation, offering users a tangible grasp of the factors influencing model decisions.\n\nAt the heart of kaBEDONN lies a dual-layered architecture: a main node and a sub-node. The main node captures the essence of the input data, while the sub-node refines this representation by incorporating additional information about the input. To facilitate efficient querying and retrieval, we employ a technique known as \"fingerprinting,\" which assigns unique identifiers to data samples. These fingerprints enable rapid identification and retrieval of relevant data within the kaBEDONN framework.\n\nOur experiments on diverse image datasets—MNIST, CIFAR-10, and ImageNet—demonstrate the remarkable capabilities of kaBEDONN. Notably, the method exhibits strong generalization across various tasks, including handwritten digit recognition, image classification, and fine-grained object categorization. Perhaps most impressively, kaBEDONN achieves a 94.6% accuracy rate on the ImageNet dataset, outperforming many state-of-the-art approaches. These results underscore the potential of kaBEDONN as a versatile tool for enhancing the transparency and interpretability of machine learning models.\n\nIn summary, kaBEDONN represents a significant advancement in the realm of XAI, providing both a robust framework for explaining model predictions and a practical means for developers to refine and enhance their systems. By leveraging the power of data-driven explanations, we pave the way for greater trust and confidence in the deployment of sophisticated machine learning algorithms."}
{"paper_id": 482, "abstract": "In the realm of reinforcement learning, value-based methods have long been celebrated for their elegance and practicality, yet they often stumble in the face of complexity. Imagine the Bellman operator, a cornerstone of our field, as a beacon of truth—a fixed point representing the optimal value function. However, when we venture into the vast territories of continuous spaces, where the value function must be approximated, the path grows treacherous. Traditional approaches like approximate value iteration and temporal-difference learning confront us with two formidable challenges: the relentless need for fresh samples and the arduous process of projecting estimates back into our functional space. These obstacles not only drain precious computational resources but also cast doubt on the accuracy of our final value function.  In this work, we unveil a revolutionary concept: the Projected Bellman Operator (PBO). Picture it as a powerful tool that learns the intricacies of the Bellman operator through a series of samples, transforming the operator into a function that operates directly within our chosen functional space. Once this operator is mastered, it can be wielded repeatedly without the necessity of additional samples, opening up a world of efficiency and precision.  To harness the full potential of PBO, we propose a novel algorithm designed specifically for value estimation. We rigorously investigate the conditions under which PBO converges to the optimal solution and explore the nuanced relationship between the accuracy of the operator and the precision of the resulting value function. Our theoretical findings are supported by compelling experimental results, demonstrating the remarkable performance of our approach across a variety of continuous-state Markov Decision Processes. In doing so, we not only advance the field of reinforcement learning but also pave the way for more efficient and reliable solutions to the complex problems that lie ahead."}
{"paper_id": 483, "abstract": "In the realm of text-guided image generation, where words weave tapestries of pixels, we unveil a surprising vulnerability—a sensitivity to character encodings that transcends mere appearance. Text encoders, the architects of meaning, are susceptible to subtle shifts in character encoding, rendering them oblivious to the nuanced distinctions between homoglyphs—characters that look alike but hold distinct meanings. This oversight allows attackers to inject cultural biases into the fabric of image generation, skewing the representation of ethnicity, religion, and nationality. For instance, replacing a single letter with a homoglyph can transform a generic cityscape into a culturally specific scene, altering the very essence of the generated image. Moreover, we reveal that homoglyph manipulations can be leveraged to obscure entire objects within the generated images, casting shadows over the intended content. In this paper, we delve into the implications of these findings, proposing a novel homoglyph unlearning procedure to safeguard text encoders against such exploitations. Through rigorous experimentation, we expose the unsettling reality that even minor variations in text can wield significant power over the outcome of image generation. Our work serves as a critical wake-up call for the development and deployment of text-guided image generation models, urging us to confront the biases that lurk within their design."}
{"paper_id": 484, "abstract": "In the realm of machine learning, the quest for understanding the intricate web of causality has led to the development of structural equation models (SEMs), which elegantly capture the essence of cause and effect. However, the challenge remains: can we train these models without the crutch of labeled data? In this paper, we embark on a bold journey, introducing the first framework capable of learning SEMs from raw, unlabeled data. Our approach hinges on the principles of additive noise models (ANMs), where each variable is seen as the sum of a deterministic function of its causes and an independent noise term. We reveal that the evidence lower bound (ELBO) associated with this model aligns precisely with the likelihood, offering a powerful tool for learning the latent variable representation of the data. Armed with automatic differentiation, we craft a linear approximation of the non-linearities inherent in ANMs, allowing us to seamlessly integrate with popular deep learning frameworks. To tackle the complexities of high-dimensional time-series data, we introduce a specialized version of the ANM that leverages the temporal arrow of time to infer causal relationships. Through rigorous experimentation on both synthetic and real-world datasets, we demonstrate the remarkable ability of our framework to uncover the underlying causal structure, even in the absence of labels. Thus, we pave the way for a new era of unsupervised causal discovery in the vast landscape of machine learning."}
{"paper_id": 485, "abstract": "[Abstract]\n\nIn the realm of multi-object tracking, where the dance of identities unfolds across frames, the challenge lies not only in discerning the presence of objects but also in recognizing the major players amidst the backdrop and their secondary counterparts. To tackle this intricate problem, we introduce a novel framework that harnesses the power of hierarchical part-whole representations, coupled with the transformative capabilities of attention mechanisms. This innovative approach allows us to distill the essential visual signatures of objects into a concise yet comprehensive form, thereby enhancing the discriminative prowess required for accurate tracking.\n\nAt the heart of our method lies a three-tiered hierarchy: the \"Part-Body-Union\" structure. This hierarchy meticulously captures the visual essence of an object through its distinct body parts, the cohesive full-body appearance, and the broader union region that encompasses both the object and any overlapping elements. By leveraging these levels, we can pinpoint the most salient visual attributes of an object while filtering out extraneous information, such as the background or secondary objects that may share space within the same bounding box.\n\nTo seamlessly integrate this hierarchical representation with the attention-based feature fusion, we employ transformers. These powerful models excel at identifying critical visual components, further bolstering our ability to generate highly discriminative visual signatures. Through rigorous experimentation on a diverse array of multi-object tracking datasets, we have demonstrated that our proposed method not only matches but often surpasses the performance of current state-of-the-art transformer-based techniques. Additionally, our approach offers significant advantages in terms of computational efficiency and time savings during both training and inference phases. In this way, we present a robust solution that promises to elevate the field of multi-object tracking to new heights."}
{"paper_id": 486, "abstract": "[Abstract]\n\nThe advent of large-scale Pre-Trained Language Models (PTLMs) has revolutionized the landscape of natural language processing, offering a wealth of knowledge derived from vast, meticulously curated datasets. Yet, amidst this triumph, a critical concern looms: the presence of societal biases and harmful representations that may perpetuate unfair treatment of certain groups. This study embarks on a comprehensive exploration into the representational harms harbored within PTLMs, focusing specifically on 13 marginalized demographics. \n\nAt the heart of our investigation lies the development of a novel metric designed to illuminate the extent of these harms. This innovative tool is crafted to align seamlessly with any dataset that delineates between harmful and benign examples, thereby enhancing our understanding of the nuanced ways in which these models may inadvertently propagate inequality. Furthermore, we delve into the shortcomings of existing metrics, striving to rectify them through our proposed approach. \n\nWe then turn our attention to an extensive empirical analysis, scrutinizing 24 renowned PTLMs across various dimensions. Our findings reveal significant disparities in the representational harms among these models, prompting us to explore the potential correlations with other established metrics and the intricate relationship between these harms and the underlying architectural designs of the models themselves. In doing so, we aim to shed light on the path forward, advocating for the development of fairer and more equitable PTLMs that do not perpetuate the injustices of the past."}
{"paper_id": 487, "abstract": "In the realm of self-supervised contrastive learning, we embark on an exploration into the robustness of learned representations when faced with the uncertainties and challenges of downstream data corruptions. Our journey reveals a fascinating dichotomy: while contrastive learning shines with remarkable robustness against various types of data corruptions, it also exhibits an unsettling sensitivity to certain forms of perturbations during the pre-training phase. This paradox raises critical questions about the very nature of contrastive learning itself. \n\nTo shed light on these complexities, we delve into the intricacies of representation learning dynamics, uncovering intriguing patterns in the evolution of feature spaces. We find that contrastive learning fosters greater overall and increasingly consistent class uniformity compared to its supervised counterpart. This phenomenon not only enhances the robustness of learned representations but also offers valuable insights into the fundamental mechanisms driving contrastive learning. \n\nMoreover, our analysis reveals that the robustness gap between contrastive and supervised learning can be significantly narrowed by incorporating a uniformity regularization term. This approach promotes greater intra-class variance, thereby improving the model's resilience to downstream corruptions. In doing so, we illuminate a path forward for enhancing the robustness of self-supervised models, paving the way for more reliable and adaptable AI systems."}
{"paper_id": 488, "abstract": "In the realm of real-time applications, the quest for efficient and robust solutions often leads us to embrace the power of multi-stage classification systems. These systems, meticulously designed to harness the strengths of different classifiers, serve as the backbone of various practical endeavors. However, a pressing challenge arises when the primary goal shifts from mere accuracy to achieving the lowest possible latency. In this paper, we unveil a groundbreaking approach called Feedback Training—a dynamic duo comprising an ultra-lightweight classifier and a more substantial counterpart. This innovative framework not only accelerates the decision-making process but also ensures that each step is optimized for the specific characteristics of the data at hand.  Our methodology hinges on a unique training sequence, where the secondary classifier, the Main-classifier, takes center stage during training. It learns to distinguish between samples that are deemed positive and those that are negative, providing valuable insights that guide the training of the initial classifier, the Pre-classifier. Through this collaborative effort, we enhance the performance of both classifiers, ultimately leading to a more efficient and effective two-stage classification system. We invite you to explore our findings and their implications for the future of real-time applications."}
{"paper_id": 489, "abstract": "In the realm of reinforcement learning, the challenge of crafting a curriculum of tasks that propels students toward mastering the most challenging goals remains a formidable obstacle. In our quest to address this issue, we introduce ZONE, a novel framework inspired by the concept of the Zone of Proximal Development, a cornerstone of developmental psychology. ZONE empowers teachers to generate tasks tailored to the current capabilities of students, ensuring they are neither overwhelmed nor bored, and fosters rapid advancement in their skill set.  At the heart of ZONE lies a dual approach. First, we employ a rejection sampling method, denoted as REJECT, to meticulously select tasks that align with the student's current abilities. This ensures that the student is neither challenged beyond their means nor left to languish in tasks that offer no growth. Second, we harness the power of a gradient norm maximizer, GRAD, to pinpoint tasks that drive significant progress in the student's learning trajectory. By doing so, we ensure that each task selected accelerates the student's journey toward mastery.  To put our theory into practice, we integrate ZONE into two prominent curriculum generation algorithms: PAIRED and Goal GAN. Through rigorous evaluation, we demonstrate that our method significantly enhances the performance of both the student and the teacher. Furthermore, we delve into the intricate dynamics of the teacher-student relationship, revealing insights into the optimal strategies for both entities to thrive. In this way, ZONE not only illuminates the path forward in curriculum generation but also enriches our understanding of the delicate balance between teacher and student in the pursuit of knowledge."}
{"paper_id": 490, "abstract": "[Abstract]\n\nIn recent years, the realm of natural language processing has been revolutionized by the emergence of large-scale pre-trained language models such as BERT, which have garnered significant attention due to their remarkable performance across a wide array of tasks. However, this progress comes at a substantial cost—both in terms of computational resources and the environmental impact associated with training these massive models. Consequently, there is an urgent need to develop smaller, more efficient models that can deliver comparable performance while being more sustainable and accessible.\n\nTo address this challenge, knowledge distillation emerges as a promising solution. By leveraging the insights gained from multiple large pre-trained models, knowledge distillation enables the creation of smaller, more efficient models that can match or even surpass the performance of their larger counterparts. Recent advancements in knowledge distillation techniques have demonstrated impressive results when applied to language modeling tasks. For instance, the work by Wu et al. (2021) introduced a method that utilizes multiple teachers to enhance the performance of knowledge distillation in language models, yielding state-of-the-art results on the GLUE benchmark. Similarly, Wang et al. (2020) proposed a novel approach called Self-Knowledge Distillation, which leverages the attention maps from a pre-trained teacher to guide the training of a student model, further pushing the boundaries of what is possible in language modeling.\n\nDespite these encouraging developments, a fundamental question remains: Is it truly beneficial to employ multiple teachers in knowledge distillation? In our investigation, we delve into the intricacies of this approach and uncover some surprising insights. Specifically, we find that the ensemble predictions generated by multiple teachers often exhibit less diversity compared to those made by individual teachers. Moreover, we observe that there exists a significant capacity gap between the large ensemble of teachers and the relatively small student model, which can impede the effectiveness of knowledge distillation. These findings suggest that the conventional wisdom regarding the advantages of using multiple teachers may not always hold true.\n\nMotivated by these observations, we introduce AutoSKDBERT, a groundbreaking approach designed to overcome the challenges posed by traditional knowledge distillation methods. Unlike previous approaches that rely on fixed teacher ensembles, AutoSKDBERT employs a stochastic sampling mechanism that allows for the dynamic selection of teachers from a predefined team. This innovative strategy is optimized through a two-phase optimization framework, complemented by a teacher selection strategy that ensures only the most effective teachers contribute to the distillation process. Furthermore, we introduce the concept of Stochastic Single-Weight Optimization (SSWO), which helps to mitigate the consistency"}
{"paper_id": 491, "abstract": "In the realm of domain generalization, where the goal is to equip models with the ability to excel across a spectrum of unseen domains, a fundamental challenge looms large: the entanglement of domain-invariant and domain-specific features. This conundrum not only obstructs the model's path to excellence but also complicates the process of disentangling these intertwined elements. In response, we introduce a novel approach that leverages the power of a dual-branching network—a structure where one branch focuses on class prediction and the other on domain prediction. Here, the features emanating from these branches serve as a proxy for domain-invariant and domain-specific representations, respectively. To further refine our methodology, we incorporate a regularization term that enforces the independence of these two sets of features, ensuring that the model does not inadvertently rely on domain-specific information for its predictions. \n\nTo bolster the effectiveness of our approach, we delve into the architectural nuances of the dual-branching network. Our findings reveal that the commonly employed early-branching structure, which utilizes only a few shared convolutional layers before diverging, outperforms alternative designs. Furthermore, we enhance our method by introducing a novel random style sampling (RDS) scheme. This innovative technique perturbs the mean and variance of feature maps through the application of random noise, effectively broadening the range of domain-specific representations that the model can leverage. By integrating this augmentation strategy seamlessly into our framework, we demonstrate significant improvements in performance.\n\nOur extensive empirical evaluations underscore the robustness and versatility of our proposed method. When tested against a wide array of benchmarks, including the recent domain generalization protocol introduced by Gulrajani & Lopez-Paz (2021), our approach consistently achieves superior results. Moreover, we conduct ablation studies to meticulously analyze the impact of each component in our framework, providing a deeper understanding of its strengths and limitations. In essence, this work not only advances the state-of-the-art in domain generalization but also illuminates new paths for future exploration in this dynamic field."}
{"paper_id": 492, "abstract": "In the rapidly evolving landscape of self-supervised learning (SSL), the quest for sustainability and efficiency is paramount. While recent advancements have pushed the boundaries of what is possible, they often come at the expense of significant computational resources. In this paper, we introduce a groundbreaking approach called Target-Enhanced Conditional (TEC) mask-reconstruction, designed to harness the latent power of existing pre-trained SSL models and propel them toward even greater heights.  At the heart of TEC lies a simple yet profound idea: we augment the reconstruction targets derived from a pre-trained base model by introducing two innovative enhancements—patch-dimension normalization and patch attention maps. These enhancements not only enrich the targets but also ensure their compatibility across diverse base models, each with its unique characteristics. To facilitate this seamless integration, we incorporate conditional adapters within the new model, allowing it to dynamically adapt its parameters to align with the specific properties of the base model.   The beauty of our approach is its simplicity and flexibility. Once trained, these adapters can be discarded, leaving behind a streamlined model ready for fine-tuning. This dual advantage of enhancing both sustainability and performance sets TEC apart from its predecessors. Our experimental results on ImageNet are nothing short of impressive, demonstrating substantial improvements over state-of-the-art (SOTA) methods across multiple benchmarks, including ImageNet classification, COCO object detection, and ADE20K semantic segmentation. Not only does TEC excel in terms of accuracy, but it also accelerates the learning process, making it a game-changer in the realm of SSL.  In summary, we invite you to explore the future of self-supervised learning with TEC, where innovation meets sustainability, and progress is no longer measured in epochs alone."}
{"paper_id": 493, "abstract": "[Abstract]\n\nIn the realm of robotics, where sound serves as a crucial sensor for understanding the environment, a new frontier emerges: the concept of a Receiver-to-Receiver Sound Neural Room Impulse Response Field, or SoundNeRirF. This innovative framework stands as a beacon, offering a rapid and efficient means to predict the sounds that will resonate at any given location within a room—a capability that is both scalable and adaptable. Unlike traditional approaches, SoundNeRirF does not demand precise knowledge of sound sources or intricate details about room acoustics. Instead, it relies on the sparse yet abundant data collected by a mobile agent as it traverses the space. \n\nAt the heart of SoundNeRirF lies a continuous six-dimensional function, designed to take the spatial coordinates of two receivers—both a reference and a target—as its input. In return, this function yields two room impulse responses, which together project the sound from the reference point to the target. By meticulously designing the training objective, we ensure that SoundNeRirF learns the intricacies of sound propagation, including direct paths, specular reflections, and the subtle echoes of late reverberations. The resulting model is not only differentiable but also continuous across space, enabling seamless optimization through gradient descent.\n\nOur experiments reveal that SoundNeRirF demonstrates remarkable robustness, effectively handling unseen situations such as novel receiver positions and even variations in room geometry. This versatility is underscored by its ability to predict sound with impressive accuracy. Moreover, SoundNeRirF shines in its capacity to generalize, successfully forecasting sounds that have never been experienced before. To facilitate future research, we have made available all datasets generated in both synthetic and real-world environments, along with our code, encouraging exploration in this exciting domain."}
{"paper_id": 494, "abstract": "In this paper, we delve into the intricate realm of sound counting—a pressing challenge that has garnered limited attention compared to its well-explored visual counterparts. Traditional approaches have often relied on sound event detection techniques, which, while effective in simpler settings, falter when confronted with the complexities of high polyphony, spectral overlap, and the dynamic interplay of loudness. To address these multifaceted issues, we introduce a groundbreaking method: the dyadic decomposition neural network (DyDecNet). This innovative approach meticulously dissects raw sound waveforms through a multi-stage, coarse-to-fine analysis, ensuring that the resulting representation is robust and resilient to the aforementioned challenges. At the heart of our solution lies an energy gain normalization module, designed to mitigate the effects of loudness variance and spectral overlap. Furthermore, we unveil a suite of novel evaluation metrics tailored to gauge the intricacies of sound counting tasks. These metrics provide a comprehensive framework for understanding and quantifying the nuances of our problem. Our experimental validation spans a diverse array of datasets, including both real-world and synthetic examples, showcasing the remarkable versatility and efficacy of our proposed methodology. Through rigorous testing, we demonstrate not only the superiority of our approach but also its potential to revolutionize the landscape of sound counting research."}
{"paper_id": 495, "abstract": "In the realm of visual navigation, the creation of a comprehensive map of an uncharted environment stands as a formidable challenge. To tackle this issue, we introduce a groundbreaking method called Active Topological Mapping (ATM), designed specifically for the exploration and subsequent navigation within unknown spaces. \n\nOur approach hinges on the concept of topological maps, which offer a unique blend of simplicity and efficiency. In the first stage, we employ a two-layer Long Short-Term Memory (LSTM) network, trained meticulously with deep supervision, to craft an optimal plan. This plan is crafted with the aim of maximizing the agent's exploration potential, drawing upon a rich tapestry of observed image features. Concurrently, we leverage a straightforward multi-layer perceptron to determine the most effective action, one that propels the agent toward a destination whose feature representation aligns closely with the planned goal. \n\nIn the second stage, we harness the power of Visual Place Recognition (VPR) to establish additional connections within our topological map, thereby enriching its structure and utility. Furthermore, we refine our approach by training an action assigner to match these newly forged connections with the precise actions required to traverse them. \n\nThrough rigorous testing on the highly realistic Gibson and Matterport3D datasets, we demonstrate that our Active Topological Mapping (ATM) outperforms existing methods in both visual exploration and navigation tasks. This innovation paves the way for more efficient and effective visual navigation strategies in the future."}
{"paper_id": 496, "abstract": "In the realm of artificial intelligence, a quest unfolds to unravel the enigma of systematic generalization—the capacity to interpret an infinite array of novel combinations from finite, known elements. This challenge stands as a formidable barrier for traditional neural networks, yet it paves the way for the emergence of a groundbreaking approach: the Neural-Symbolic Recursive Machine (NSR). At the heart of NSR lies the concept of a Grounded Symbol System (GSS), an innovative construct that arises organically from the training data itself, unburdened by the constraints of domain-specific knowledge. \n\nOur exploration begins with the development of a perception module, where neural networks adeptly ground symbols upon raw inputs. This is followed by the intricate process of syntactic parsing, wherein a transition-based neural dependency parser navigates through the complexities of the GSS, crafting a syntax tree that encapsulates the essence of the symbols. Finally, functional programs are employed to bring semantic meaning to life, transforming the abstract into the concrete. The elegance of this framework lies in its theoretical underpinnings—NSR is shown to be sufficiently expressive to model a wide array of sequence-to-sequence tasks. Yet, the true power of NSR resides in the inductive biases of equivariance and recursiveness embedded within each module. These biases enable NSR to deconstruct lengthy inputs into manageable components, process them with precision, and ultimately reassemble them into coherent wholes. This process fosters the learning of meaningful symbols and their compositional rules, laying the foundation for NSR's exceptional prowess in systematic generalization.\n\nHowever, the journey to harness this power is fraught with challenges. The absence of annotations for the internal GSS and the inherent non-differentiability of NSR complicate the optimization process. To address these hurdles, we introduce a probabilistic learning framework and a novel deduction-abduction algorithm designed to harmonize the joint learning of the various modules. In the learning phase, NSR embarks on a greedy deduction, proposing an initial GSS that may fall short of the mark. It then engages in a meticulous search-based abduction, traversing the neighborhood of this initial GSS to uncover potential solutions. Through this iterative process, the GSS is refined until it yields the desired outcome. This revised GSS, serving as a guide, provides valuable pseudo-supervision to train each module, thereby enhancing the learning of individual components within NSR.\n\nTo put our theory into practice, we rigorously evaluate NSR across three diverse benchmarks: SCAN, PCFG, and HINT"}
{"paper_id": 497, "abstract": "Recent advancements in GAN inversion have seen remarkable progress, particularly through the innovative use of encoders that harness the power of powerful unconditional generators. This approach has led to significant strides in robust and disentangled image attribute editing. However, a critical issue persists: these encoders, while efficient, often struggle to capture high-frequency details, resulting in distortions that detract from the overall quality of the inverted images. To address this challenge, we introduce WaGI, a novel GAN inversion framework that leverages wavelets to enhance the fidelity of the inversion process. By meticulously analyzing the wavelet coefficients of both the original and inverted images, we reveal that existing inversion methods are heavily biased toward low-frequency components, leaving high-frequency details largely unattended. In response, we develop a wavelet-based loss function designed to mitigate distortions across all frequency bands, coupled with a wavelet fusion mechanism that seamlessly integrates high-frequency features into the reconstruction process. Our empirical evaluations demonstrate that WaGI surpasses previous state-of-the-art inversion models, achieving lower distortions on both low-and high-frequency sub-bands. Moreover, WaGI outperforms its counterparts in various editing tasks, showcasing its versatility and effectiveness in real-world applications. Through rigorous testing and validation, we confirm that each component of our proposed framework contributes meaningfully to the overall performance, making WaGI a promising advancement in the field of GAN inversion."}
{"paper_id": 498, "abstract": "In the ever-evolving landscape of video content, the demand for robust video highlights detection (VHD) has never been greater. However, a significant roadblock looms on the horizon: the challenge of adapting to new video domains without the luxury of retraining from scratch. This paper embarks on a journey to address this pressing issue, introducing a groundbreaking method for incremental VHD that not only learns new highlight concepts but also retains the invaluable knowledge gleaned from previous domains. At the heart of our approach lies a meticulously crafted dataset, LiveFood, a treasure trove of high-resolution gourmet videos meticulously labeled across four distinct domains: cooking, eating, ingredients, and presentation. This dataset, with its rich tapestry of 5,100+ carefully curated videos spanning nearly 197 hours, stands as a testament to our commitment to advancing the field of video highlights detection.   To tackle this formidable task, we unveil our Global Prototype Encoding (GPE) model, a novel architecture designed to incrementally classify video frames into highlight and vanilla categories, even as new domains emerge. Our model operates through a series of transformative steps: extracting frame-level features with a convolutional neural network, aggregating these features with a Transformer encoder to capture temporal dynamics, and finally employing two sets of learnable prototypes—one for highlights and another for non-highlights—to classify each frame. What sets GPE apart is its innovative use of a distance-based classification loss under the L 2 metric, which ensures that the new prototypes for highlights remain close to those learned previously, thereby facilitating seamless integration of new domains.  In our extensive evaluations, GPE emerges as a beacon of performance, surpassing other incremental learning methods in terms of both accuracy and computational efficiency. Notably, GPE improves the mean Average Precision (mAP) by an impressive 1.57% on average across various settings. These results underscore the potential of our approach to revolutionize the way we detect and understand video highlights in the dynamic realm of online content. As we move forward, we invite fellow researchers to explore the vast possibilities that LiveFood and GPE offer, paving the way for groundbreaking advancements in the future of video highlights detection."}
{"paper_id": 499, "abstract": "In the realm of deep learning, where complex models reign supreme, a curious phenomenon emerges: the explosive gradients that mar the early stages of training. Despite the widespread adoption of batch normalization—a technique designed to enhance model stability—this issue persists, casting a shadow over the optimization process. In this paper, we embark on a journey to unravel the mysteries behind this unexpected gradient explosion.  We reveal that the very activation functions themselves, when paired with batch normalization, serve as catalysts for this destabilizing effect. Our mathematical exploration delves into the intricate relationship between these components, uncovering the mechanisms that fuel the gradient's rapid ascent. Furthermore, we illuminate the path to stabilization, proposing solutions that can mitigate this disruption. Through our findings, we aim to empower researchers and practitioners alike to navigate the treacherous waters of deep learning with greater confidence and precision.   Empirical evidence from a variety of experiments, conducted across different datasets and model architectures, supports our theoretical framework. This comprehensive analysis not only sheds light on a long-standing challenge in deep learning but also paves the way for future advancements in the field.   Join us as we delve into the heart of this fascinating problem, where mathematics meets the cutting edge of artificial intelligence.  \\section{Introduction} The advent of deep learning has revolutionized the landscape of machine learning, empowering the creation of powerful models capable of solving a myriad of complex tasks. Central to this progress is the activation function, a key element in deep learning that transforms input signals, enabling the network to learn intricate representations. Yet, as we push the boundaries of model depth and complexity, we encounter a significant hurdle: the infamous exploding/vanishing gradient problem. This issue, once thought to be a relic of the past, rears its head in modern architectures, particularly those that employ batch normalization—a technique that normalizes each mini-batch of inputs to achieve more stable training dynamics. While the impact of this problem is relatively mild in practice, its presence during the early stages of training can disrupt the delicate balance between layers, ultimately leading to suboptimal performance. To date, much of the research has focused on the stability of forward activations, but little attention has been paid to the stability of backward gradients. In this work, we tackle the core question: why do exploding gradients arise in batch-normalized deep neural networks? Our investigation reveals that the activation function itself plays a pivotal role in this phenomenon. We derive an analytical expression that elucidates the conditions under which exploding gradients emerge, offering a new perspective on this long-standing issue."}
{"paper_id": 500, "abstract": "In the realm of organic chemistry, the quest for efficient synthesis routes remains a formidable challenge, akin to navigating through the intricate web of a vast network. To address this, we introduce Metro—a groundbreaking approach that harnesses the power of a memory-enhanced transformer, designed specifically for retrosynthetic planning. This innovative method not only captures the essential context among molecules within a reaction route but also ensures that the search for potential starting materials stays grounded within a scientifically validated reaction space.   To further elevate our exploration, we have meticulously crafted a new benchmark, drawing upon the rich tapestry of reactions found in the USPTO dataset. This benchmark encompasses no fewer than 124,869 reaction trees, offering a more comprehensive and rigorous evaluation framework than previous efforts. Our experiments reveal that Metro outperforms its counterparts by significant margins, with improvements of up to 13.2%, 14.5%, 11.1%, 10.5%, and 10.0% in terms of top-1, top-2, top-3, top-4, and top-5 accuracy, respectively. These findings underscore the transformative potential of Metro in revolutionizing the field of organic synthesis planning.  In essence, Metro represents a leap forward in retrosynthetic planning, offering a powerful tool for chemists to explore the vast landscape of organic synthesis with greater precision and efficiency."}
{"paper_id": 501, "abstract": "In the realm of artificial intelligence, where agents traverse vast landscapes and confront intricate challenges, the ability to explore efficiently is paramount. Traditional methods often falter when faced with the complexities of high-dimensional and multimodal spaces, relying heavily on extensive memory resources and lengthy training periods. Drawing inspiration from human cognition, particularly the brain's adeptness at managing and recalling episodic memories, we introduce a novel approach to exploration—Fragmentation-and-Recall. This innovative framework transforms the daunting task of global exploration into a series of manageable, local endeavors. By leveraging a carefully crafted combination of short-term and long-term memory, our method not only conserves valuable computational resources but also accelerates the exploration process. We meticulously validate our approach through rigorous experimentation, employing procedurally generated spatial environments and a diverse array of reinforcement learning benchmarks. The results are striking: our method outperforms conventional techniques, demonstrating its robustness and adaptability across a wide spectrum of tasks. With Fragmentation-and-Recall, we pave the way for more efficient and effective exploration in the ever-expanding universe of artificial intelligence."}
{"paper_id": 502, "abstract": "In the realm of heterogeneous image conversion, we embark on a journey to illuminate the mysteries of a scene captured through the lens of infrared, transforming it into a vivid visual spectacle. This endeavor hinges on the mastery of an image translation model, one that bridges the chasm between modalities. Yet, the quest is fraught with challenges, particularly when faced with limited datasets and the intricate interplay of semantics and structures within images.   To navigate these treacherous waters, we introduce a novel architecture—a four-layer multi-scale encoder-generator. This innovative design downscales images thrice, encoding and decoding them in ascending order, ensuring that the rich structural details of the input images are preserved intact.   In our exploration, we uncover two groundbreaking loss functions: the structure loss and the semantic loss. These losses serve as our guiding stars, illuminating the path to the optimal image translation model. The structure loss, crafted with the inception module as its cornerstone, adeptly captures the structural essence of an image, while the semantic loss leverages a pre-trained VGG-16 model to discern the semantic nuances. Together, they form a formidable alliance against the backdrop of L1 loss and adversarial loss, which often falter in their attempts to encapsulate the intricate tapestry of image structures.   Our empirical findings reveal that the image translation model we have developed outperforms existing methods in terms of both image structure and semantic content. This advancement not only pushes the boundaries of what is possible in image translation but also paves the way for future innovations in this dynamic field."}
{"paper_id": 503, "abstract": "In the realm of computational biology, the quest for novel protein structures holds the promise of unveiling groundbreaking medical treatments. Yet, this endeavor has long been fraught with challenges, particularly in generating diverse and realistic folds—a task that traditional methods often fail to accomplish. Enter our innovative approach: a denoising diffusion probabilistic model, meticulously crafted to traverse the intricate landscape of protein backbone configurations. By focusing on the inter-residue angles that define these structures, we eschew the complexities inherent in Cartesian atom coordinates, instead embracing a framework where each residue serves as an independent reference frame. This shift not only simplifies the learning process but also aligns more closely with the biological mechanisms that guide protein folding in nature. Our rigorous validation efforts reveal that our model not only adeptly captures the nuanced distribution of inter-residue angles found in real proteins but also yields overall structures rich with the structural motifs that characterize functional proteins. Furthermore, we demonstrate that our generated structures exhibit a remarkable diversity and are amenable to design, thereby underscoring their potential as viable candidates for future therapeutic innovations. In this way, our work illuminates a path forward, one that may lead to the discovery of new treatments for some of humanity's most pressing health challenges."}
{"paper_id": 504, "abstract": "Despite recent advances in deep learning, the generalization ability of modern neural networks remains largely unexplained, particularly in the context of reinforcement learning (RL) and meta-learning. To shed light on this phenomenon, we introduce a novel framework centered around the concept of \"features,\" which enables us to rigorously define and measure the difficulty of generalization across a wide range of learning scenarios, including supervised learning, RL, and meta-learning. At the heart of our approach lies a new metric: inductive bias complexity, which quantifies the information content of the inductive biases necessary for a model to perform well on a given task. This metric is grounded in information theory, providing a robust foundation for our exploration.   To facilitate empirical investigation, we propose a practical method for estimating inductive bias complexity, even in scenarios where the underlying features are unknown. Our experiments reveal intriguing insights: for example, we find that partially observed RL environments demand significantly more inductive bias compared to their fully observed counterparts. Furthermore, we demonstrate that simple few-shot meta-learning tasks can exhibit far greater inductive bias requirements than complex, real-world supervised learning tasks. These findings underscore the importance of inductive bias complexity as a critical factor in understanding the generalization capabilities of modern learning systems.   In this way, our work not only illuminates the challenges faced by current machine learning approaches but also paves the way for future advancements in the field. By providing a clear framework and actionable tools, we hope to inspire further research into the role of inductive biases in the quest for truly generalizable artificial intelligence."}
{"paper_id": 505, "abstract": "In the realm of deep generative modeling, the quest for disentanglement stands as a cornerstone, guiding us toward the creation of more interpretable and controllable representations. Yet, when we delve into the complex landscape of graph data, the disentanglement properties of these models remain largely unexplored. In this paper, we embark on a journey to unravel the mysteries surrounding disentanglement in deep generative models tailored for graph data. Our initial findings reveal that the latent spaces learned by these models are not entirely disentangled—a revelation that prompts us to confront the challenges of unsupervised graph controllable generation.   To address these issues, we introduce GraphCG, a groundbreaking framework designed to maximize the mutual information between the latent space and the graph attributes. This approach not only fosters disentanglement but also allows for the seamless manipulation of graph data through the traversal of latent spaces. We rigorously validate our framework across two distinct types of graph data—molecules and point clouds—demonstrating its effectiveness through qualitative evaluations. Additionally, for the molecular dataset, we unveil a novel metric: the sequence monotonic ratio (SMR), which quantifies the monotonicity of generated sequences. Our experimental results underscore the superior performance of GraphCG compared to existing methods, paving the way for new horizons in graph data generation and manipulation.   With GraphCG, we not only illuminate the path forward for disentanglement in graph data but also empower researchers and practitioners to harness the full potential of deep generative models in a myriad of applications."}
{"paper_id": 506, "abstract": "[Abstract]\n\nIn the ever-evolving landscape of visual recognition, the pursuit of enhanced accuracy often comes at the cost of introducing unwelcome changes—namely, negative flips, which occur when a model incorrectly classifies images previously handled correctly by its predecessor. This phenomenon, rooted in the intricate dynamics of deep neural networks, poses a significant challenge for practical deployment. To tackle this issue, we delve into the realm of positive-congruent training, a methodology designed to minimize negative flips alongside error rates. Our exploration reveals two pivotal insights: first, we uncover the surprising yet compelling relationship between deep ensembles and their effectiveness in reducing negative flips—a discovery that challenges conventional wisdom and opens new avenues for research. Second, we introduce a novel training objective that seeks to enhance both the accuracy and compatibility of deep ensembles. Through rigorous experimentation, we demonstrate that this innovative approach not only mitigates negative flips but also improves overall classification performance. In doing so, we lay the groundwork for more robust and reliable model updates, paving the way for a future where the evolution of visual recognition systems is both seamless and impactful."}
{"paper_id": 507, "abstract": "Optimal transport (OT) has become a powerful tool across numerous domains, yet its practical implementation often demands significant computational resources. To address this challenge, we introduce Meta Optimal Transport (Meta OT), a novel approach designed to streamline the process of solving multiple OT problems by leveraging amortized optimization techniques. Our framework employs machine learning models to predict the outcomes of OT problems, thereby bypassing the need for costly iterative optimization. This innovation allows for substantial savings in both computational time and the number of iterations required to achieve solutions. In our empirical studies, we demonstrate the robust performance of Meta OT, showcasing its ability to tackle both discrete and continuous OT problems effectively. Through rigorous analysis, we establish the convergence of our method under certain conditions, further validating its reliability. Additionally, we explore how Meta OT can enhance the efficiency of other OT-related tasks, such as predicting Sinkhorn distances and computing gradients of OT losses, thus broadening its applicability across various fields."}
{"paper_id": 508, "abstract": "In the realm of neural networks, attention has emerged as a powerful mechanism, reshaping the landscape of deep learning models. However, the attention mechanism, particularly the scaled dot-product attention, harbors a hidden vulnerability—it demands the entire sequence to compute a single attention, leading to a quadratic computational complexity. This inefficiency poses significant challenges when tackling long sequence tasks, as well as in edge inference environments. Moreover, the lack of statefulness in attention makes it ill-suited for tasks that require memory states, leaving many complex problems beyond its reach.   In this paper, we introduce Neural Attention Memory (NAM), a novel framework that transforms attention into a memory architecture within neural networks. At its core, NAM employs the familiar query-key-value structure, utilizing additive write operations to store key-value pairs in a memory matrix through outer products. Reading from this matrix is achieved through simple multiplication with a unit query vector. We delve into the mathematical foundations underpinning NAM’s read and write operations, providing rigorous theoretical proofs that establish their equivalence to attention in specific scenarios.   One of the standout features of NAM is its ability to facilitate efficient attention mechanisms. By relinquishing the erasure capability inherent in NAM’s write operation, we unveil Normalized Outer-Product Attention—a variant of NAM that mirrors linear attention in its simplicity and efficiency, boasting a linear computational complexity relative to the sequence length.   To showcase the prowess of NAM, we put it to the test in long-range sequence tasks, comparing it against both the base Transformer and Linear Transformer. The results reveal that NAM not only holds its own but also presents itself as a compelling alternative for those seeking efficiency.   But the true strength of NAM lies in its versatility as a building block for constructing memory-augmented neural networks (MANNs). Leveraging the power of NAM’s read and write primitives, we develop two innovative MANN architectures: Long Short-term Attention Memory (LSAM) and NAM Turing Machine (NAM-TM). LSAM emerges as a universal RNN architecture, replacing LSTM’s long-term cell state with a memory matrix. Instead of appending vectors to a cell state, LSAM utilizes NAM’s primitives to read from and write to the memory matrix. This elegant fusion of attention and RNN, coupled with the same computational complexity as LSTM, positions LSAM as a formidable contender.   NAM-TM, on the other hand, is designed specifically for algorithmic tasks. It leverages a Turing tape structure, where read and write"}
{"paper_id": 509, "abstract": "As the field of machine learning continues to evolve, the role of data has become increasingly central to progress. Yet, this reliance on vast datasets often comes with significant ethical challenges, particularly concerning data privacy. To address these concerns, a common practice has emerged: modifying datasets to eliminate sensitive attributes, thereby enhancing privacy while potentially sacrificing overall utility. However, this approach frequently falls short, as the very features that ensure robustness in downstream applications are often compromised in the process. This trade-off leaves a critical gap in the development of truly robust machine learning models. In this paper, we introduce Multi-Attribute Selective Suppression (MaSS), a groundbreaking framework designed to strike a delicate balance between privacy and utility. By selectively targeting specific attributes for suppression, MaSS ensures that sensitive information remains protected while preserving the integrity of other valuable features within the dataset. Our extensive evaluations across diverse multi-attribute datasets—ranging from images and audio to videos—demonstrate that MaSS not only achieves effective privacy protection but also maintains high levels of data utility, paving the way for more responsible and efficient machine learning practices."}
{"paper_id": 510, "abstract": "[Abstract]\n\nIn the realm of weakly-supervised object localization (WSOL), where only image-level labels guide the way, the challenge lies in distilling meaningful localization information from the intricate tapestry of feature maps woven by convolutional neural networks. This paper embarks on a journey through the landscape of existing heatmap-based XAI methods, revealing their unique capabilities and shortcomings in the WSOL arena. Our exploration encompasses the familiar Class Activation Mapping (CAM) and its lesser-known counterparts, such as GradCAM, Guided Backpropagation (GBP), and DeepLift—each offering a distinct lens through which we examine the localization prowess of CNNs.\n\nThe crux of our investigation hinges on the concept of normalization, a critical step often overlooked in previous studies. Herein, we propose a novel approach to normalize the heatmaps generated by these methods, uncovering the hidden potential for WSOL. Our findings reveal a fascinating hierarchy among these techniques, where certain methods exhibit superior performance when combined with the nuanced insights offered by Neural Backed Decision Trees (NBDTs). For instance, the saliency method, when meticulously tuned, emerges as a formidable contender, achieving impressive WSOL metrics. Conversely, other methods like GBP and DeepLift demonstrate remarkable localization abilities, particularly when applied to specific layers within the network.\n\nMoreover, we introduce a series of innovative modifications tailored to enhance the WSOL capabilities of these established methods. Notably, we explore the use of multi-layer heatmaps, where each layer's contributions are aggregated into a single, unified map. This approach not only amplifies the effectiveness of the original methods but also reveals the rich localization information encoded within deeper layers of the network. In addition, we delve into the impact of layer selection, uncovering the optimal layers for each method to achieve the highest WSOL scores.\n\nOur comprehensive analysis culminates in the revelation that, through the judicious application of CAM-like concepts, the heatmaps produced by the inner layers of existing XAI methods can significantly surpass those generated by traditional methods, such as CAM. This breakthrough underscores the transformative power of our proposed enhancements, offering a fresh perspective on the potential of heatmap-based XAI techniques in the quest for accurate WSOL. As we navigate the complexities of CNNs, our work illuminates new pathways for future research and innovation in this dynamic field."}
