{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSf_mE865VPO",
        "outputId": "0f8977a7-0320-4ad7-917e-6dc9fecb414d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diffusers==0.16.1\n",
            "  Downloading diffusers-0.16.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting huggingface_hub==0.19.4\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (3.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (8.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2.32.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (4.13.2)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m975.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.16.1) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Downloading diffusers-0.16.1-py3-none-any.whl (934 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.9/934.9 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, tokenizers, nvidia-cusolver-cu12, diffusers, transformers, accelerate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.31.4\n",
            "    Uninstalling huggingface-hub-0.31.4:\n",
            "      Successfully uninstalled huggingface-hub-0.31.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.33.1\n",
            "    Uninstalling diffusers-0.33.1:\n",
            "      Successfully uninstalled diffusers-0.33.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.21.0 diffusers-0.16.1 huggingface_hub-0.19.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.19.1 transformers-4.40.2\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers==0.16.1 transformers accelerate datasets accelerate==0.21.0 huggingface_hub==0.19.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbFWp6z08hLT",
        "outputId": "76127cd2-4cfa-4849-85ad-a73f6fdef0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: jax 0.5.2\n",
            "Uninstalling jax-0.5.2:\n",
            "  Successfully uninstalled jax-0.5.2\n",
            "Found existing installation: jaxlib 0.5.1\n",
            "Uninstalling jaxlib-0.5.1:\n",
            "  Successfully uninstalled jaxlib-0.5.1\n",
            "Found existing installation: flax 0.10.6\n",
            "Uninstalling flax-0.10.6:\n",
            "  Successfully uninstalled flax-0.10.6\n",
            "Found existing installation: diffusers 0.16.1\n",
            "Uninstalling diffusers-0.16.1:\n",
            "  Successfully uninstalled diffusers-0.16.1\n",
            "Found existing installation: huggingface-hub 0.19.4\n",
            "Uninstalling huggingface-hub-0.19.4:\n",
            "  Successfully uninstalled huggingface-hub-0.19.4\n",
            "Found existing installation: accelerate 0.21.0\n",
            "Uninstalling accelerate-0.21.0:\n",
            "  Successfully uninstalled accelerate-0.21.0\n",
            "Collecting diffusers==0.16.1\n",
            "  Using cached diffusers-0.16.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting huggingface_hub==0.19.4\n",
            "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting accelerate==0.21.0\n",
            "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (3.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (8.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.16.1) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (2025.3.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (4.13.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.19.4) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.16.1) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.16.1) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (3.0.2)\n",
            "Using cached diffusers-0.16.1-py3-none-any.whl (934 kB)\n",
            "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "Installing collected packages: huggingface_hub, diffusers, accelerate\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.19.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.21.0 diffusers-0.16.1 huggingface_hub-0.19.4\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y jax jaxlib flax\n",
        "!pip uninstall -y diffusers huggingface_hub accelerate\n",
        "!pip install diffusers==0.16.1 huggingface_hub==0.19.4 accelerate==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05kLViVa5Z-L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, image_dir):\n",
        "        self.image_paths = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.png')]\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        return self.transform(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M90MyB4Gq0_R",
        "outputId": "f4ca2d75-6e4a-4212-9a0e-a7475343bf34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:427: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:1301: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  new_forward = torch.cuda.amp.autocast(dtype=torch.float16)(model_forward_func)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Step 0 | Loss: 0.8482\n",
            "Epoch 0 | Step 50 | Loss: 0.5782\n",
            "Epoch 0 | Step 100 | Loss: 0.2393\n",
            "Epoch 0 | Step 150 | Loss: 0.1678\n",
            "Epoch 1 | Step 0 | Loss: 0.1916\n",
            "Epoch 1 | Step 50 | Loss: 0.1550\n",
            "Epoch 1 | Step 100 | Loss: 0.1411\n",
            "Epoch 1 | Step 150 | Loss: 0.1172\n",
            "Epoch 2 | Step 0 | Loss: 0.1214\n",
            "Epoch 2 | Step 50 | Loss: 0.1339\n",
            "Epoch 2 | Step 100 | Loss: 0.1356\n",
            "Epoch 2 | Step 150 | Loss: 0.1412\n",
            "Epoch 3 | Step 0 | Loss: 0.1137\n",
            "Epoch 3 | Step 50 | Loss: 0.1027\n",
            "Epoch 3 | Step 100 | Loss: 0.0924\n",
            "Epoch 3 | Step 150 | Loss: 0.0674\n",
            "Epoch 4 | Step 0 | Loss: 0.0871\n",
            "Epoch 4 | Step 50 | Loss: 0.0998\n",
            "Epoch 4 | Step 100 | Loss: 0.1040\n",
            "Epoch 4 | Step 150 | Loss: 0.0849\n",
            "Epoch 5 | Step 0 | Loss: 0.0828\n",
            "Epoch 5 | Step 50 | Loss: 0.0715\n",
            "Epoch 5 | Step 100 | Loss: 0.0779\n",
            "Epoch 5 | Step 150 | Loss: 0.0828\n",
            "Epoch 6 | Step 0 | Loss: 0.0858\n",
            "Epoch 6 | Step 50 | Loss: 0.0756\n",
            "Epoch 6 | Step 100 | Loss: 0.0803\n",
            "Epoch 6 | Step 150 | Loss: 0.0728\n",
            "Epoch 7 | Step 0 | Loss: 0.1236\n",
            "Epoch 7 | Step 50 | Loss: 0.0835\n",
            "Epoch 7 | Step 100 | Loss: 0.0883\n",
            "Epoch 7 | Step 150 | Loss: 0.0874\n",
            "Epoch 8 | Step 0 | Loss: 0.0799\n",
            "Epoch 8 | Step 50 | Loss: 0.0800\n",
            "Epoch 8 | Step 100 | Loss: 0.0711\n",
            "Epoch 8 | Step 150 | Loss: 0.0670\n",
            "Epoch 9 | Step 0 | Loss: 0.1346\n",
            "Epoch 9 | Step 50 | Loss: 0.0776\n",
            "Epoch 9 | Step 100 | Loss: 0.0888\n",
            "Epoch 9 | Step 150 | Loss: 0.1080\n",
            "Epoch 10 | Step 0 | Loss: 0.0714\n",
            "Epoch 10 | Step 50 | Loss: 0.1052\n",
            "Epoch 10 | Step 100 | Loss: 0.0776\n",
            "Epoch 10 | Step 150 | Loss: 0.0808\n",
            "Epoch 11 | Step 0 | Loss: 0.0746\n",
            "Epoch 11 | Step 50 | Loss: 0.0880\n",
            "Epoch 11 | Step 100 | Loss: 0.0728\n",
            "Epoch 11 | Step 150 | Loss: 0.0661\n",
            "Epoch 12 | Step 0 | Loss: 0.1042\n",
            "Epoch 12 | Step 50 | Loss: 0.0780\n",
            "Epoch 12 | Step 100 | Loss: 0.0833\n",
            "Epoch 12 | Step 150 | Loss: 0.0910\n",
            "Epoch 13 | Step 0 | Loss: 0.0805\n",
            "Epoch 13 | Step 50 | Loss: 0.0865\n",
            "Epoch 13 | Step 100 | Loss: 0.0927\n",
            "Epoch 13 | Step 150 | Loss: 0.0695\n",
            "Epoch 14 | Step 0 | Loss: 0.0982\n",
            "Epoch 14 | Step 50 | Loss: 0.0938\n",
            "Epoch 14 | Step 100 | Loss: 0.0936\n",
            "Epoch 14 | Step 150 | Loss: 0.0929\n",
            "Epoch 15 | Step 0 | Loss: 0.0937\n",
            "Epoch 15 | Step 50 | Loss: 0.0999\n",
            "Epoch 15 | Step 100 | Loss: 0.0741\n",
            "Epoch 15 | Step 150 | Loss: 0.0720\n",
            "Epoch 16 | Step 0 | Loss: 0.0628\n",
            "Epoch 16 | Step 50 | Loss: 0.0778\n",
            "Epoch 16 | Step 100 | Loss: 0.0799\n",
            "Epoch 16 | Step 150 | Loss: 0.1011\n",
            "Epoch 17 | Step 0 | Loss: 0.0818\n",
            "Epoch 17 | Step 50 | Loss: 0.0666\n",
            "Epoch 17 | Step 100 | Loss: 0.0936\n",
            "Epoch 17 | Step 150 | Loss: 0.0685\n",
            "Epoch 18 | Step 0 | Loss: 0.0868\n",
            "Epoch 18 | Step 50 | Loss: 0.0701\n",
            "Epoch 18 | Step 100 | Loss: 0.0620\n",
            "Epoch 18 | Step 150 | Loss: 0.0655\n",
            "Epoch 19 | Step 0 | Loss: 0.0846\n",
            "Epoch 19 | Step 50 | Loss: 0.0935\n",
            "Epoch 19 | Step 100 | Loss: 0.0874\n",
            "Epoch 19 | Step 150 | Loss: 0.0796\n",
            "Epoch 20 | Step 0 | Loss: 0.0728\n",
            "Epoch 20 | Step 50 | Loss: 0.0723\n",
            "Epoch 20 | Step 100 | Loss: 0.0716\n",
            "Epoch 20 | Step 150 | Loss: 0.0751\n",
            "Epoch 21 | Step 0 | Loss: 0.0774\n",
            "Epoch 21 | Step 50 | Loss: 0.0862\n",
            "Epoch 21 | Step 100 | Loss: 0.0584\n",
            "Epoch 21 | Step 150 | Loss: 0.0681\n",
            "Epoch 22 | Step 0 | Loss: 0.0741\n",
            "Epoch 22 | Step 50 | Loss: 0.0907\n",
            "Epoch 22 | Step 100 | Loss: 0.0787\n",
            "Epoch 22 | Step 150 | Loss: 0.0727\n",
            "Epoch 23 | Step 0 | Loss: 0.0717\n",
            "Epoch 23 | Step 50 | Loss: 0.0683\n",
            "Epoch 23 | Step 100 | Loss: 0.0696\n",
            "Epoch 23 | Step 150 | Loss: 0.0673\n",
            "Epoch 24 | Step 0 | Loss: 0.0714\n",
            "Epoch 24 | Step 50 | Loss: 0.0758\n",
            "Epoch 24 | Step 100 | Loss: 0.0819\n",
            "Epoch 24 | Step 150 | Loss: 0.0750\n",
            "Epoch 25 | Step 0 | Loss: 0.0791\n",
            "Epoch 25 | Step 50 | Loss: 0.0705\n",
            "Epoch 25 | Step 100 | Loss: 0.0734\n",
            "Epoch 25 | Step 150 | Loss: 0.0782\n",
            "Epoch 26 | Step 0 | Loss: 0.0666\n",
            "Epoch 26 | Step 50 | Loss: 0.0740\n",
            "Epoch 26 | Step 100 | Loss: 0.0747\n",
            "Epoch 26 | Step 150 | Loss: 0.0759\n",
            "Epoch 27 | Step 0 | Loss: 0.0677\n",
            "Epoch 27 | Step 50 | Loss: 0.0794\n",
            "Epoch 27 | Step 100 | Loss: 0.0802\n",
            "Epoch 27 | Step 150 | Loss: 0.0840\n",
            "Epoch 28 | Step 0 | Loss: 0.0908\n",
            "Epoch 28 | Step 50 | Loss: 0.0905\n",
            "Epoch 28 | Step 100 | Loss: 0.0850\n",
            "Epoch 28 | Step 150 | Loss: 0.0873\n",
            "Epoch 29 | Step 0 | Loss: 0.0747\n",
            "Epoch 29 | Step 50 | Loss: 0.0871\n",
            "Epoch 29 | Step 100 | Loss: 0.0695\n",
            "Epoch 29 | Step 150 | Loss: 0.0897\n",
            "Epoch 30 | Step 0 | Loss: 0.0683\n",
            "Epoch 30 | Step 50 | Loss: 0.0757\n",
            "Epoch 30 | Step 100 | Loss: 0.0843\n",
            "Epoch 30 | Step 150 | Loss: 0.0723\n",
            "Epoch 31 | Step 0 | Loss: 0.0850\n",
            "Epoch 31 | Step 50 | Loss: 0.0838\n",
            "Epoch 31 | Step 100 | Loss: 0.0754\n",
            "Epoch 31 | Step 150 | Loss: 0.0907\n",
            "Epoch 32 | Step 0 | Loss: 0.0675\n",
            "Epoch 32 | Step 50 | Loss: 0.0908\n",
            "Epoch 32 | Step 100 | Loss: 0.0840\n",
            "Epoch 32 | Step 150 | Loss: 0.0992\n",
            "Epoch 33 | Step 0 | Loss: 0.0631\n",
            "Epoch 33 | Step 50 | Loss: 0.0650\n",
            "Epoch 33 | Step 100 | Loss: 0.0907\n",
            "Epoch 33 | Step 150 | Loss: 0.0767\n",
            "Epoch 34 | Step 0 | Loss: 0.0831\n",
            "Epoch 34 | Step 50 | Loss: 0.0632\n",
            "Epoch 34 | Step 100 | Loss: 0.0744\n",
            "Epoch 34 | Step 150 | Loss: 0.0804\n",
            "Epoch 35 | Step 0 | Loss: 0.0783\n",
            "Epoch 35 | Step 50 | Loss: 0.0677\n",
            "Epoch 35 | Step 100 | Loss: 0.0668\n",
            "Epoch 35 | Step 150 | Loss: 0.0786\n",
            "Epoch 36 | Step 0 | Loss: 0.0662\n",
            "Epoch 36 | Step 50 | Loss: 0.0850\n",
            "Epoch 36 | Step 100 | Loss: 0.0749\n",
            "Epoch 36 | Step 150 | Loss: 0.0811\n",
            "Epoch 37 | Step 0 | Loss: 0.0795\n",
            "Epoch 37 | Step 50 | Loss: 0.0752\n",
            "Epoch 37 | Step 100 | Loss: 0.0767\n",
            "Epoch 37 | Step 150 | Loss: 0.0847\n",
            "Epoch 38 | Step 0 | Loss: 0.0800\n",
            "Epoch 38 | Step 50 | Loss: 0.0683\n",
            "Epoch 38 | Step 100 | Loss: 0.0637\n",
            "Epoch 38 | Step 150 | Loss: 0.0729\n",
            "Epoch 39 | Step 0 | Loss: 0.0661\n",
            "Epoch 39 | Step 50 | Loss: 0.0834\n",
            "Epoch 39 | Step 100 | Loss: 0.0740\n",
            "Epoch 39 | Step 150 | Loss: 0.0738\n",
            "Epoch 40 | Step 0 | Loss: 0.0684\n",
            "Epoch 40 | Step 50 | Loss: 0.0707\n",
            "Epoch 40 | Step 100 | Loss: 0.0595\n",
            "Epoch 40 | Step 150 | Loss: 0.0616\n",
            "Epoch 41 | Step 0 | Loss: 0.0747\n",
            "Epoch 41 | Step 50 | Loss: 0.0725\n",
            "Epoch 41 | Step 100 | Loss: 0.0881\n",
            "Epoch 41 | Step 150 | Loss: 0.0627\n",
            "Epoch 42 | Step 0 | Loss: 0.0671\n",
            "Epoch 42 | Step 50 | Loss: 0.0820\n",
            "Epoch 42 | Step 100 | Loss: 0.0881\n",
            "Epoch 42 | Step 150 | Loss: 0.0794\n",
            "Epoch 43 | Step 0 | Loss: 0.0763\n",
            "Epoch 43 | Step 50 | Loss: 0.0723\n",
            "Epoch 43 | Step 100 | Loss: 0.0683\n",
            "Epoch 43 | Step 150 | Loss: 0.0714\n",
            "Epoch 44 | Step 0 | Loss: 0.0793\n",
            "Epoch 44 | Step 50 | Loss: 0.0608\n",
            "Epoch 44 | Step 100 | Loss: 0.0717\n",
            "Epoch 44 | Step 150 | Loss: 0.0652\n",
            "Epoch 45 | Step 0 | Loss: 0.0742\n",
            "Epoch 45 | Step 50 | Loss: 0.0707\n",
            "Epoch 45 | Step 100 | Loss: 0.0950\n",
            "Epoch 45 | Step 150 | Loss: 0.0818\n",
            "Epoch 46 | Step 0 | Loss: 0.0802\n",
            "Epoch 46 | Step 50 | Loss: 0.0780\n",
            "Epoch 46 | Step 100 | Loss: 0.0806\n",
            "Epoch 46 | Step 150 | Loss: 0.0744\n",
            "Epoch 47 | Step 0 | Loss: 0.0668\n",
            "Epoch 47 | Step 50 | Loss: 0.0662\n",
            "Epoch 47 | Step 100 | Loss: 0.0616\n",
            "Epoch 47 | Step 150 | Loss: 0.0752\n",
            "Epoch 48 | Step 0 | Loss: 0.0895\n",
            "Epoch 48 | Step 50 | Loss: 0.0811\n",
            "Epoch 48 | Step 100 | Loss: 0.0678\n",
            "Epoch 48 | Step 150 | Loss: 0.0775\n",
            "Epoch 49 | Step 0 | Loss: 0.0721\n",
            "Epoch 49 | Step 50 | Loss: 0.0765\n",
            "Epoch 49 | Step 100 | Loss: 0.0841\n",
            "Epoch 49 | Step 150 | Loss: 0.0734\n",
            "Epoch 50 | Step 0 | Loss: 0.0958\n",
            "Epoch 50 | Step 50 | Loss: 0.0744\n",
            "Epoch 50 | Step 100 | Loss: 0.0822\n",
            "Epoch 50 | Step 150 | Loss: 0.0816\n",
            "Epoch 51 | Step 0 | Loss: 0.0906\n",
            "Epoch 51 | Step 50 | Loss: 0.0707\n",
            "Epoch 51 | Step 100 | Loss: 0.0802\n",
            "Epoch 51 | Step 150 | Loss: 0.0872\n",
            "Epoch 52 | Step 0 | Loss: 0.0598\n",
            "Epoch 52 | Step 50 | Loss: 0.0714\n",
            "Epoch 52 | Step 100 | Loss: 0.0700\n",
            "Epoch 52 | Step 150 | Loss: 0.0644\n",
            "Epoch 53 | Step 0 | Loss: 0.0630\n",
            "Epoch 53 | Step 50 | Loss: 0.0837\n",
            "Epoch 53 | Step 100 | Loss: 0.0637\n",
            "Epoch 53 | Step 150 | Loss: 0.0808\n",
            "Epoch 54 | Step 0 | Loss: 0.0771\n",
            "Epoch 54 | Step 50 | Loss: 0.0711\n",
            "Epoch 54 | Step 100 | Loss: 0.0820\n",
            "Epoch 54 | Step 150 | Loss: 0.0818\n",
            "Epoch 55 | Step 0 | Loss: 0.0857\n",
            "Epoch 55 | Step 50 | Loss: 0.0702\n",
            "Epoch 55 | Step 100 | Loss: 0.0596\n",
            "Epoch 55 | Step 150 | Loss: 0.0815\n",
            "Epoch 56 | Step 0 | Loss: 0.0716\n",
            "Epoch 56 | Step 50 | Loss: 0.0684\n",
            "Epoch 56 | Step 100 | Loss: 0.0657\n",
            "Epoch 56 | Step 150 | Loss: 0.0742\n",
            "Epoch 57 | Step 0 | Loss: 0.0860\n",
            "Epoch 57 | Step 50 | Loss: 0.0724\n",
            "Epoch 57 | Step 100 | Loss: 0.0641\n",
            "Epoch 57 | Step 150 | Loss: 0.0658\n",
            "Epoch 58 | Step 0 | Loss: 0.0774\n",
            "Epoch 58 | Step 50 | Loss: 0.0859\n",
            "Epoch 58 | Step 100 | Loss: 0.0719\n",
            "Epoch 58 | Step 150 | Loss: 0.0701\n",
            "Epoch 59 | Step 0 | Loss: 0.0748\n",
            "Epoch 59 | Step 50 | Loss: 0.0763\n",
            "Epoch 59 | Step 100 | Loss: 0.0972\n",
            "Epoch 59 | Step 150 | Loss: 0.0759\n",
            "Epoch 60 | Step 0 | Loss: 0.0619\n",
            "Epoch 60 | Step 50 | Loss: 0.0741\n",
            "Epoch 60 | Step 100 | Loss: 0.0701\n",
            "Epoch 60 | Step 150 | Loss: 0.0800\n",
            "Epoch 61 | Step 0 | Loss: 0.0723\n",
            "Epoch 61 | Step 50 | Loss: 0.0887\n",
            "Epoch 61 | Step 100 | Loss: 0.0554\n",
            "Epoch 61 | Step 150 | Loss: 0.0655\n",
            "Epoch 62 | Step 0 | Loss: 0.0841\n",
            "Epoch 62 | Step 50 | Loss: 0.0911\n",
            "Epoch 62 | Step 100 | Loss: 0.0835\n",
            "Epoch 62 | Step 150 | Loss: 0.0729\n",
            "Epoch 63 | Step 0 | Loss: 0.0634\n",
            "Epoch 63 | Step 50 | Loss: 0.0943\n",
            "Epoch 63 | Step 100 | Loss: 0.0725\n",
            "Epoch 63 | Step 150 | Loss: 0.0674\n",
            "Epoch 64 | Step 0 | Loss: 0.0889\n",
            "Epoch 64 | Step 50 | Loss: 0.0762\n",
            "Epoch 64 | Step 100 | Loss: 0.0798\n",
            "Epoch 64 | Step 150 | Loss: 0.0552\n",
            "Epoch 65 | Step 0 | Loss: 0.0668\n",
            "Epoch 65 | Step 50 | Loss: 0.0703\n",
            "Epoch 65 | Step 100 | Loss: 0.0844\n",
            "Epoch 65 | Step 150 | Loss: 0.0570\n",
            "Epoch 66 | Step 0 | Loss: 0.0642\n",
            "Epoch 66 | Step 50 | Loss: 0.0756\n",
            "Epoch 66 | Step 100 | Loss: 0.0818\n",
            "Epoch 66 | Step 150 | Loss: 0.0651\n",
            "Epoch 67 | Step 0 | Loss: 0.0795\n",
            "Epoch 67 | Step 50 | Loss: 0.0880\n",
            "Epoch 67 | Step 100 | Loss: 0.0666\n",
            "Epoch 67 | Step 150 | Loss: 0.0600\n",
            "Epoch 68 | Step 0 | Loss: 0.0741\n",
            "Epoch 68 | Step 50 | Loss: 0.0768\n",
            "Epoch 68 | Step 100 | Loss: 0.0684\n",
            "Epoch 68 | Step 150 | Loss: 0.0698\n",
            "Epoch 69 | Step 0 | Loss: 0.0785\n",
            "Epoch 69 | Step 50 | Loss: 0.0691\n",
            "Epoch 69 | Step 100 | Loss: 0.0575\n",
            "Epoch 69 | Step 150 | Loss: 0.0850\n",
            "Epoch 70 | Step 0 | Loss: 0.0721\n",
            "Epoch 70 | Step 50 | Loss: 0.0649\n",
            "Epoch 70 | Step 100 | Loss: 0.0749\n",
            "Epoch 70 | Step 150 | Loss: 0.0750\n",
            "Epoch 71 | Step 0 | Loss: 0.0760\n",
            "Epoch 71 | Step 50 | Loss: 0.0751\n",
            "Epoch 71 | Step 100 | Loss: 0.0756\n",
            "Epoch 71 | Step 150 | Loss: 0.0644\n",
            "Epoch 72 | Step 0 | Loss: 0.0672\n",
            "Epoch 72 | Step 50 | Loss: 0.0788\n",
            "Epoch 72 | Step 100 | Loss: 0.0823\n",
            "Epoch 72 | Step 150 | Loss: 0.0588\n",
            "Epoch 73 | Step 0 | Loss: 0.0635\n",
            "Epoch 73 | Step 50 | Loss: 0.0659\n",
            "Epoch 73 | Step 100 | Loss: 0.0583\n",
            "Epoch 73 | Step 150 | Loss: 0.0696\n",
            "Epoch 74 | Step 0 | Loss: 0.0797\n",
            "Epoch 74 | Step 50 | Loss: 0.0750\n",
            "Epoch 74 | Step 100 | Loss: 0.0771\n",
            "Epoch 74 | Step 150 | Loss: 0.0714\n",
            "Epoch 75 | Step 0 | Loss: 0.0793\n",
            "Epoch 75 | Step 50 | Loss: 0.0686\n",
            "Epoch 75 | Step 100 | Loss: 0.0813\n",
            "Epoch 75 | Step 150 | Loss: 0.0656\n",
            "Epoch 76 | Step 0 | Loss: 0.0742\n",
            "Epoch 76 | Step 50 | Loss: 0.0653\n",
            "Epoch 76 | Step 100 | Loss: 0.0642\n",
            "Epoch 76 | Step 150 | Loss: 0.0639\n",
            "Epoch 77 | Step 0 | Loss: 0.0681\n",
            "Epoch 77 | Step 50 | Loss: 0.0782\n",
            "Epoch 77 | Step 100 | Loss: 0.0870\n",
            "Epoch 77 | Step 150 | Loss: 0.0815\n",
            "Epoch 78 | Step 0 | Loss: 0.0618\n",
            "Epoch 78 | Step 50 | Loss: 0.0718\n",
            "Epoch 78 | Step 100 | Loss: 0.0769\n",
            "Epoch 78 | Step 150 | Loss: 0.0825\n",
            "Epoch 79 | Step 0 | Loss: 0.0885\n",
            "Epoch 79 | Step 50 | Loss: 0.0680\n",
            "Epoch 79 | Step 100 | Loss: 0.0818\n",
            "Epoch 79 | Step 150 | Loss: 0.0825\n",
            "Epoch 80 | Step 0 | Loss: 0.0747\n",
            "Epoch 80 | Step 50 | Loss: 0.0760\n",
            "Epoch 80 | Step 100 | Loss: 0.0622\n",
            "Epoch 80 | Step 150 | Loss: 0.0810\n",
            "Epoch 81 | Step 0 | Loss: 0.0832\n",
            "Epoch 81 | Step 50 | Loss: 0.0599\n",
            "Epoch 81 | Step 100 | Loss: 0.0750\n",
            "Epoch 81 | Step 150 | Loss: 0.0670\n",
            "Epoch 82 | Step 0 | Loss: 0.0630\n",
            "Epoch 82 | Step 50 | Loss: 0.0798\n",
            "Epoch 82 | Step 100 | Loss: 0.0777\n",
            "Epoch 82 | Step 150 | Loss: 0.0704\n",
            "Epoch 83 | Step 0 | Loss: 0.0738\n",
            "Epoch 83 | Step 50 | Loss: 0.0879\n",
            "Epoch 83 | Step 100 | Loss: 0.0664\n",
            "Epoch 83 | Step 150 | Loss: 0.0713\n",
            "Epoch 84 | Step 0 | Loss: 0.0663\n",
            "Epoch 84 | Step 50 | Loss: 0.0822\n",
            "Epoch 84 | Step 100 | Loss: 0.0752\n",
            "Epoch 84 | Step 150 | Loss: 0.0772\n",
            "Epoch 85 | Step 0 | Loss: 0.0729\n",
            "Epoch 85 | Step 50 | Loss: 0.0675\n",
            "Epoch 85 | Step 100 | Loss: 0.0812\n",
            "Epoch 85 | Step 150 | Loss: 0.0847\n",
            "Epoch 86 | Step 0 | Loss: 0.0848\n",
            "Epoch 86 | Step 50 | Loss: 0.0989\n",
            "Epoch 86 | Step 100 | Loss: 0.0739\n",
            "Epoch 86 | Step 150 | Loss: 0.0658\n",
            "Epoch 87 | Step 0 | Loss: 0.0741\n",
            "Epoch 87 | Step 50 | Loss: 0.0744\n",
            "Epoch 87 | Step 100 | Loss: 0.0799\n",
            "Epoch 87 | Step 150 | Loss: 0.0748\n",
            "Epoch 88 | Step 0 | Loss: 0.0635\n",
            "Epoch 88 | Step 50 | Loss: 0.0562\n",
            "Epoch 88 | Step 100 | Loss: 0.0685\n",
            "Epoch 88 | Step 150 | Loss: 0.0685\n",
            "Epoch 89 | Step 0 | Loss: 0.0767\n",
            "Epoch 89 | Step 50 | Loss: 0.0632\n",
            "Epoch 89 | Step 100 | Loss: 0.0748\n",
            "Epoch 89 | Step 150 | Loss: 0.0587\n",
            "Epoch 90 | Step 0 | Loss: 0.0627\n",
            "Epoch 90 | Step 50 | Loss: 0.0625\n",
            "Epoch 90 | Step 100 | Loss: 0.0785\n",
            "Epoch 90 | Step 150 | Loss: 0.0755\n",
            "Epoch 91 | Step 0 | Loss: 0.0668\n",
            "Epoch 91 | Step 50 | Loss: 0.0707\n",
            "Epoch 91 | Step 100 | Loss: 0.0701\n",
            "Epoch 91 | Step 150 | Loss: 0.0887\n",
            "Epoch 92 | Step 0 | Loss: 0.0810\n",
            "Epoch 92 | Step 50 | Loss: 0.0600\n",
            "Epoch 92 | Step 100 | Loss: 0.0570\n",
            "Epoch 92 | Step 150 | Loss: 0.0741\n",
            "Epoch 93 | Step 0 | Loss: 0.0637\n",
            "Epoch 93 | Step 50 | Loss: 0.0638\n",
            "Epoch 93 | Step 100 | Loss: 0.0807\n",
            "Epoch 93 | Step 150 | Loss: 0.0754\n",
            "Epoch 94 | Step 0 | Loss: 0.0748\n",
            "Epoch 94 | Step 50 | Loss: 0.0846\n",
            "Epoch 94 | Step 100 | Loss: 0.0738\n",
            "Epoch 94 | Step 150 | Loss: 0.0824\n",
            "Epoch 95 | Step 0 | Loss: 0.0791\n",
            "Epoch 95 | Step 50 | Loss: 0.0771\n",
            "Epoch 95 | Step 100 | Loss: 0.0757\n",
            "Epoch 95 | Step 150 | Loss: 0.0709\n",
            "Epoch 96 | Step 0 | Loss: 0.0735\n",
            "Epoch 96 | Step 50 | Loss: 0.0668\n",
            "Epoch 96 | Step 100 | Loss: 0.0748\n",
            "Epoch 96 | Step 150 | Loss: 0.0583\n",
            "Epoch 97 | Step 0 | Loss: 0.0680\n",
            "Epoch 97 | Step 50 | Loss: 0.0568\n",
            "Epoch 97 | Step 100 | Loss: 0.0767\n",
            "Epoch 97 | Step 150 | Loss: 0.0785\n",
            "Epoch 98 | Step 0 | Loss: 0.0872\n",
            "Epoch 98 | Step 50 | Loss: 0.0797\n",
            "Epoch 98 | Step 100 | Loss: 0.0729\n",
            "Epoch 98 | Step 150 | Loss: 0.0745\n",
            "Epoch 99 | Step 0 | Loss: 0.0906\n",
            "Epoch 99 | Step 50 | Loss: 0.0612\n",
            "Epoch 99 | Step 100 | Loss: 0.0801\n",
            "Epoch 99 | Step 150 | Loss: 0.0776\n",
            "[INFO] Saved model at epoch 100\n",
            "Epoch 100 | Step 0 | Loss: 0.0712\n",
            "Epoch 100 | Step 50 | Loss: 0.0698\n",
            "Epoch 100 | Step 100 | Loss: 0.0748\n",
            "Epoch 100 | Step 150 | Loss: 0.0546\n",
            "Epoch 101 | Step 0 | Loss: 0.0770\n",
            "Epoch 101 | Step 50 | Loss: 0.0638\n",
            "Epoch 101 | Step 100 | Loss: 0.0613\n",
            "Epoch 101 | Step 150 | Loss: 0.0535\n",
            "Epoch 102 | Step 0 | Loss: 0.0736\n",
            "Epoch 102 | Step 50 | Loss: 0.0731\n",
            "Epoch 102 | Step 100 | Loss: 0.0626\n",
            "Epoch 102 | Step 150 | Loss: 0.0723\n",
            "Epoch 103 | Step 0 | Loss: 0.0675\n",
            "Epoch 103 | Step 50 | Loss: 0.0623\n",
            "Epoch 103 | Step 100 | Loss: 0.0807\n",
            "Epoch 103 | Step 150 | Loss: 0.0714\n",
            "Epoch 104 | Step 0 | Loss: 0.0922\n",
            "Epoch 104 | Step 50 | Loss: 0.0753\n",
            "Epoch 104 | Step 100 | Loss: 0.0779\n",
            "Epoch 104 | Step 150 | Loss: 0.0665\n",
            "Epoch 105 | Step 0 | Loss: 0.0689\n",
            "Epoch 105 | Step 50 | Loss: 0.0655\n",
            "Epoch 105 | Step 100 | Loss: 0.0712\n",
            "Epoch 105 | Step 150 | Loss: 0.0683\n",
            "Epoch 106 | Step 0 | Loss: 0.0760\n",
            "Epoch 106 | Step 50 | Loss: 0.0756\n",
            "Epoch 106 | Step 100 | Loss: 0.0629\n",
            "Epoch 106 | Step 150 | Loss: 0.0750\n",
            "Epoch 107 | Step 0 | Loss: 0.0654\n",
            "Epoch 107 | Step 50 | Loss: 0.0721\n",
            "Epoch 107 | Step 100 | Loss: 0.0779\n",
            "Epoch 107 | Step 150 | Loss: 0.0717\n",
            "Epoch 108 | Step 0 | Loss: 0.0739\n",
            "Epoch 108 | Step 50 | Loss: 0.0720\n",
            "Epoch 108 | Step 100 | Loss: 0.0598\n",
            "Epoch 108 | Step 150 | Loss: 0.0771\n",
            "Epoch 109 | Step 0 | Loss: 0.0733\n",
            "Epoch 109 | Step 50 | Loss: 0.0734\n",
            "Epoch 109 | Step 100 | Loss: 0.0672\n",
            "Epoch 109 | Step 150 | Loss: 0.0702\n",
            "Epoch 110 | Step 0 | Loss: 0.0791\n",
            "Epoch 110 | Step 50 | Loss: 0.0629\n",
            "Epoch 110 | Step 100 | Loss: 0.0688\n",
            "Epoch 110 | Step 150 | Loss: 0.0636\n",
            "Epoch 111 | Step 0 | Loss: 0.0613\n",
            "Epoch 111 | Step 50 | Loss: 0.0601\n",
            "Epoch 111 | Step 100 | Loss: 0.0720\n",
            "Epoch 111 | Step 150 | Loss: 0.0659\n",
            "Epoch 112 | Step 0 | Loss: 0.0649\n",
            "Epoch 112 | Step 50 | Loss: 0.0710\n",
            "Epoch 112 | Step 100 | Loss: 0.0647\n",
            "Epoch 112 | Step 150 | Loss: 0.0634\n",
            "Epoch 113 | Step 0 | Loss: 0.0781\n",
            "Epoch 113 | Step 50 | Loss: 0.0705\n",
            "Epoch 113 | Step 100 | Loss: 0.0762\n",
            "Epoch 113 | Step 150 | Loss: 0.0682\n",
            "Epoch 114 | Step 0 | Loss: 0.0704\n",
            "Epoch 114 | Step 50 | Loss: 0.0590\n",
            "Epoch 114 | Step 100 | Loss: 0.0717\n",
            "Epoch 114 | Step 150 | Loss: 0.0904\n",
            "Epoch 115 | Step 0 | Loss: 0.0827\n",
            "Epoch 115 | Step 50 | Loss: 0.0854\n",
            "Epoch 115 | Step 100 | Loss: 0.0849\n",
            "Epoch 115 | Step 150 | Loss: 0.0688\n",
            "Epoch 116 | Step 0 | Loss: 0.0628\n",
            "Epoch 116 | Step 50 | Loss: 0.0629\n",
            "Epoch 116 | Step 100 | Loss: 0.0667\n",
            "Epoch 116 | Step 150 | Loss: 0.0683\n",
            "Epoch 117 | Step 0 | Loss: 0.0589\n",
            "Epoch 117 | Step 50 | Loss: 0.0638\n",
            "Epoch 117 | Step 100 | Loss: 0.0776\n",
            "Epoch 117 | Step 150 | Loss: 0.0662\n",
            "Epoch 118 | Step 0 | Loss: 0.0734\n",
            "Epoch 118 | Step 50 | Loss: 0.0744\n",
            "Epoch 118 | Step 100 | Loss: 0.0975\n",
            "Epoch 118 | Step 150 | Loss: 0.0664\n",
            "Epoch 119 | Step 0 | Loss: 0.0663\n",
            "Epoch 119 | Step 50 | Loss: 0.0596\n",
            "Epoch 119 | Step 100 | Loss: 0.0849\n",
            "Epoch 119 | Step 150 | Loss: 0.0733\n",
            "Epoch 120 | Step 0 | Loss: 0.0670\n",
            "Epoch 120 | Step 50 | Loss: 0.0720\n",
            "Epoch 120 | Step 100 | Loss: 0.0767\n",
            "Epoch 120 | Step 150 | Loss: 0.0578\n",
            "Epoch 121 | Step 0 | Loss: 0.0683\n",
            "Epoch 121 | Step 50 | Loss: 0.0620\n",
            "Epoch 121 | Step 100 | Loss: 0.0699\n",
            "Epoch 121 | Step 150 | Loss: 0.0535\n",
            "Epoch 122 | Step 0 | Loss: 0.0753\n",
            "Epoch 122 | Step 50 | Loss: 0.0609\n",
            "Epoch 122 | Step 100 | Loss: 0.0891\n",
            "Epoch 122 | Step 150 | Loss: 0.0627\n",
            "Epoch 123 | Step 0 | Loss: 0.0776\n",
            "Epoch 123 | Step 50 | Loss: 0.0722\n",
            "Epoch 123 | Step 100 | Loss: 0.0708\n",
            "Epoch 123 | Step 150 | Loss: 0.0691\n",
            "Epoch 124 | Step 0 | Loss: 0.0620\n",
            "Epoch 124 | Step 50 | Loss: 0.0654\n",
            "Epoch 124 | Step 100 | Loss: 0.0571\n",
            "Epoch 124 | Step 150 | Loss: 0.0704\n",
            "Epoch 125 | Step 0 | Loss: 0.0692\n",
            "Epoch 125 | Step 50 | Loss: 0.0608\n",
            "Epoch 125 | Step 100 | Loss: 0.0728\n",
            "Epoch 125 | Step 150 | Loss: 0.0690\n",
            "Epoch 126 | Step 0 | Loss: 0.0675\n",
            "Epoch 126 | Step 50 | Loss: 0.0716\n",
            "Epoch 126 | Step 100 | Loss: 0.0640\n",
            "Epoch 126 | Step 150 | Loss: 0.0765\n",
            "Epoch 127 | Step 0 | Loss: 0.0655\n",
            "Epoch 127 | Step 50 | Loss: 0.0733\n",
            "Epoch 127 | Step 100 | Loss: 0.0673\n",
            "Epoch 127 | Step 150 | Loss: 0.0677\n",
            "Epoch 128 | Step 0 | Loss: 0.0574\n",
            "Epoch 128 | Step 50 | Loss: 0.0764\n",
            "Epoch 128 | Step 100 | Loss: 0.0773\n",
            "Epoch 128 | Step 150 | Loss: 0.0800\n",
            "Epoch 129 | Step 0 | Loss: 0.0702\n",
            "Epoch 129 | Step 50 | Loss: 0.0840\n",
            "Epoch 129 | Step 100 | Loss: 0.0712\n",
            "Epoch 129 | Step 150 | Loss: 0.0575\n",
            "Epoch 130 | Step 0 | Loss: 0.0641\n",
            "Epoch 130 | Step 50 | Loss: 0.0724\n",
            "Epoch 130 | Step 100 | Loss: 0.0777\n",
            "Epoch 130 | Step 150 | Loss: 0.0848\n",
            "Epoch 131 | Step 0 | Loss: 0.0632\n",
            "Epoch 131 | Step 50 | Loss: 0.0705\n",
            "Epoch 131 | Step 100 | Loss: 0.0762\n",
            "Epoch 131 | Step 150 | Loss: 0.0761\n",
            "Epoch 132 | Step 0 | Loss: 0.0790\n",
            "Epoch 132 | Step 50 | Loss: 0.0698\n",
            "Epoch 132 | Step 100 | Loss: 0.0656\n",
            "Epoch 132 | Step 150 | Loss: 0.0741\n",
            "Epoch 133 | Step 0 | Loss: 0.0701\n",
            "Epoch 133 | Step 50 | Loss: 0.0693\n",
            "Epoch 133 | Step 100 | Loss: 0.0664\n",
            "Epoch 133 | Step 150 | Loss: 0.0733\n",
            "Epoch 134 | Step 0 | Loss: 0.0611\n",
            "Epoch 134 | Step 50 | Loss: 0.0735\n",
            "Epoch 134 | Step 100 | Loss: 0.0655\n",
            "Epoch 134 | Step 150 | Loss: 0.0662\n",
            "Epoch 135 | Step 0 | Loss: 0.0738\n",
            "Epoch 135 | Step 50 | Loss: 0.0657\n",
            "Epoch 135 | Step 100 | Loss: 0.0695\n",
            "Epoch 135 | Step 150 | Loss: 0.0628\n",
            "Epoch 136 | Step 0 | Loss: 0.0706\n",
            "Epoch 136 | Step 50 | Loss: 0.0666\n",
            "Epoch 136 | Step 100 | Loss: 0.0672\n",
            "Epoch 136 | Step 150 | Loss: 0.0769\n",
            "Epoch 137 | Step 0 | Loss: 0.0804\n",
            "Epoch 137 | Step 50 | Loss: 0.0851\n",
            "Epoch 137 | Step 100 | Loss: 0.0656\n",
            "Epoch 137 | Step 150 | Loss: 0.0715\n",
            "Epoch 138 | Step 0 | Loss: 0.0804\n",
            "Epoch 138 | Step 50 | Loss: 0.0639\n",
            "Epoch 138 | Step 100 | Loss: 0.0742\n",
            "Epoch 138 | Step 150 | Loss: 0.0798\n",
            "Epoch 139 | Step 0 | Loss: 0.0862\n",
            "Epoch 139 | Step 50 | Loss: 0.0614\n",
            "Epoch 139 | Step 100 | Loss: 0.0794\n",
            "Epoch 139 | Step 150 | Loss: 0.0835\n",
            "Epoch 140 | Step 0 | Loss: 0.0659\n",
            "Epoch 140 | Step 50 | Loss: 0.0672\n",
            "Epoch 140 | Step 100 | Loss: 0.0664\n",
            "Epoch 140 | Step 150 | Loss: 0.0707\n",
            "Epoch 141 | Step 0 | Loss: 0.0578\n",
            "Epoch 141 | Step 50 | Loss: 0.0691\n",
            "Epoch 141 | Step 100 | Loss: 0.0598\n",
            "Epoch 141 | Step 150 | Loss: 0.0609\n",
            "Epoch 142 | Step 0 | Loss: 0.0706\n",
            "Epoch 142 | Step 50 | Loss: 0.0792\n",
            "Epoch 142 | Step 100 | Loss: 0.0636\n",
            "Epoch 142 | Step 150 | Loss: 0.0675\n",
            "Epoch 143 | Step 0 | Loss: 0.0677\n",
            "Epoch 143 | Step 50 | Loss: 0.0738\n",
            "Epoch 143 | Step 100 | Loss: 0.0744\n",
            "Epoch 143 | Step 150 | Loss: 0.0876\n",
            "Epoch 144 | Step 0 | Loss: 0.0737\n",
            "Epoch 144 | Step 50 | Loss: 0.0764\n",
            "Epoch 144 | Step 100 | Loss: 0.0556\n",
            "Epoch 144 | Step 150 | Loss: 0.0818\n",
            "Epoch 145 | Step 0 | Loss: 0.0668\n",
            "Epoch 145 | Step 50 | Loss: 0.0666\n",
            "Epoch 145 | Step 100 | Loss: 0.0612\n",
            "Epoch 145 | Step 150 | Loss: 0.0611\n",
            "Epoch 146 | Step 0 | Loss: 0.0784\n",
            "Epoch 146 | Step 50 | Loss: 0.0682\n",
            "Epoch 146 | Step 100 | Loss: 0.0546\n",
            "Epoch 146 | Step 150 | Loss: 0.0791\n",
            "Epoch 147 | Step 0 | Loss: 0.0722\n",
            "Epoch 147 | Step 50 | Loss: 0.0647\n",
            "Epoch 147 | Step 100 | Loss: 0.0674\n",
            "Epoch 147 | Step 150 | Loss: 0.0733\n",
            "Epoch 148 | Step 0 | Loss: 0.0681\n",
            "Epoch 148 | Step 50 | Loss: 0.0654\n",
            "Epoch 148 | Step 100 | Loss: 0.0714\n",
            "Epoch 148 | Step 150 | Loss: 0.0565\n",
            "Epoch 149 | Step 0 | Loss: 0.0858\n",
            "Epoch 149 | Step 50 | Loss: 0.0757\n",
            "Epoch 149 | Step 100 | Loss: 0.0695\n",
            "Epoch 149 | Step 150 | Loss: 0.0665\n",
            "Epoch 150 | Step 0 | Loss: 0.0693\n",
            "Epoch 150 | Step 50 | Loss: 0.0652\n",
            "Epoch 150 | Step 100 | Loss: 0.0696\n",
            "Epoch 150 | Step 150 | Loss: 0.0578\n",
            "Epoch 151 | Step 0 | Loss: 0.0677\n",
            "Epoch 151 | Step 50 | Loss: 0.0726\n",
            "Epoch 151 | Step 100 | Loss: 0.0768\n",
            "Epoch 151 | Step 150 | Loss: 0.0651\n",
            "Epoch 152 | Step 0 | Loss: 0.0597\n",
            "Epoch 152 | Step 50 | Loss: 0.0744\n",
            "Epoch 152 | Step 100 | Loss: 0.0683\n",
            "Epoch 152 | Step 150 | Loss: 0.0648\n",
            "Epoch 153 | Step 0 | Loss: 0.0751\n",
            "Epoch 153 | Step 50 | Loss: 0.0713\n",
            "Epoch 153 | Step 100 | Loss: 0.0683\n",
            "Epoch 153 | Step 150 | Loss: 0.0651\n",
            "Epoch 154 | Step 0 | Loss: 0.0800\n",
            "Epoch 154 | Step 50 | Loss: 0.0452\n",
            "Epoch 154 | Step 100 | Loss: 0.0975\n",
            "Epoch 154 | Step 150 | Loss: 0.0634\n",
            "Epoch 155 | Step 0 | Loss: 0.0853\n",
            "Epoch 155 | Step 50 | Loss: 0.0844\n",
            "Epoch 155 | Step 100 | Loss: 0.0629\n",
            "Epoch 155 | Step 150 | Loss: 0.0769\n",
            "Epoch 156 | Step 0 | Loss: 0.0667\n",
            "Epoch 156 | Step 50 | Loss: 0.0692\n",
            "Epoch 156 | Step 100 | Loss: 0.0665\n",
            "Epoch 156 | Step 150 | Loss: 0.0576\n",
            "Epoch 157 | Step 0 | Loss: 0.0724\n",
            "Epoch 157 | Step 50 | Loss: 0.0679\n",
            "Epoch 157 | Step 100 | Loss: 0.0735\n",
            "Epoch 157 | Step 150 | Loss: 0.0678\n",
            "Epoch 158 | Step 0 | Loss: 0.0744\n",
            "Epoch 158 | Step 50 | Loss: 0.0782\n",
            "Epoch 158 | Step 100 | Loss: 0.0644\n",
            "Epoch 158 | Step 150 | Loss: 0.0720\n",
            "Epoch 159 | Step 0 | Loss: 0.0514\n",
            "Epoch 159 | Step 50 | Loss: 0.0640\n",
            "Epoch 159 | Step 100 | Loss: 0.0565\n",
            "Epoch 159 | Step 150 | Loss: 0.0740\n",
            "Epoch 160 | Step 0 | Loss: 0.0661\n",
            "Epoch 160 | Step 50 | Loss: 0.0775\n",
            "Epoch 160 | Step 100 | Loss: 0.0690\n",
            "Epoch 160 | Step 150 | Loss: 0.0744\n",
            "Epoch 161 | Step 0 | Loss: 0.0698\n",
            "Epoch 161 | Step 50 | Loss: 0.0729\n",
            "Epoch 161 | Step 100 | Loss: 0.0838\n",
            "Epoch 161 | Step 150 | Loss: 0.0601\n",
            "Epoch 162 | Step 0 | Loss: 0.0694\n",
            "Epoch 162 | Step 50 | Loss: 0.0751\n",
            "Epoch 162 | Step 100 | Loss: 0.0668\n",
            "Epoch 162 | Step 150 | Loss: 0.0797\n",
            "Epoch 163 | Step 0 | Loss: 0.0651\n",
            "Epoch 163 | Step 50 | Loss: 0.0510\n",
            "Epoch 163 | Step 100 | Loss: 0.0781\n",
            "Epoch 163 | Step 150 | Loss: 0.0643\n",
            "Epoch 164 | Step 0 | Loss: 0.0622\n",
            "Epoch 164 | Step 50 | Loss: 0.0757\n",
            "Epoch 164 | Step 100 | Loss: 0.0668\n",
            "Epoch 164 | Step 150 | Loss: 0.0711\n",
            "Epoch 165 | Step 0 | Loss: 0.0759\n",
            "Epoch 165 | Step 50 | Loss: 0.0689\n",
            "Epoch 165 | Step 100 | Loss: 0.0663\n",
            "Epoch 165 | Step 150 | Loss: 0.0596\n",
            "Epoch 166 | Step 0 | Loss: 0.0676\n",
            "Epoch 166 | Step 50 | Loss: 0.0668\n",
            "Epoch 166 | Step 100 | Loss: 0.0953\n",
            "Epoch 166 | Step 150 | Loss: 0.0748\n",
            "Epoch 167 | Step 0 | Loss: 0.0669\n",
            "Epoch 167 | Step 50 | Loss: 0.0768\n",
            "Epoch 167 | Step 100 | Loss: 0.0656\n",
            "Epoch 167 | Step 150 | Loss: 0.0753\n",
            "Epoch 168 | Step 0 | Loss: 0.0719\n",
            "Epoch 168 | Step 50 | Loss: 0.0758\n",
            "Epoch 168 | Step 100 | Loss: 0.0593\n",
            "Epoch 168 | Step 150 | Loss: 0.0754\n",
            "Epoch 169 | Step 0 | Loss: 0.0617\n",
            "Epoch 169 | Step 50 | Loss: 0.0633\n",
            "Epoch 169 | Step 100 | Loss: 0.0601\n",
            "Epoch 169 | Step 150 | Loss: 0.0567\n",
            "Epoch 170 | Step 0 | Loss: 0.0645\n",
            "Epoch 170 | Step 50 | Loss: 0.0743\n",
            "Epoch 170 | Step 100 | Loss: 0.0693\n",
            "Epoch 170 | Step 150 | Loss: 0.0695\n",
            "Epoch 171 | Step 0 | Loss: 0.0813\n",
            "Epoch 171 | Step 50 | Loss: 0.0719\n",
            "Epoch 171 | Step 100 | Loss: 0.0893\n",
            "Epoch 171 | Step 150 | Loss: 0.0640\n",
            "Epoch 172 | Step 0 | Loss: 0.0734\n",
            "Epoch 172 | Step 50 | Loss: 0.0771\n",
            "Epoch 172 | Step 100 | Loss: 0.0552\n",
            "Epoch 172 | Step 150 | Loss: 0.0633\n",
            "Epoch 173 | Step 0 | Loss: 0.0772\n",
            "Epoch 173 | Step 50 | Loss: 0.0503\n",
            "Epoch 173 | Step 100 | Loss: 0.0750\n",
            "Epoch 173 | Step 150 | Loss: 0.0633\n",
            "Epoch 174 | Step 0 | Loss: 0.0608\n",
            "Epoch 174 | Step 50 | Loss: 0.0735\n",
            "Epoch 174 | Step 100 | Loss: 0.0743\n",
            "Epoch 174 | Step 150 | Loss: 0.0650\n",
            "Epoch 175 | Step 0 | Loss: 0.0627\n",
            "Epoch 175 | Step 50 | Loss: 0.0740\n",
            "Epoch 175 | Step 100 | Loss: 0.0629\n",
            "Epoch 175 | Step 150 | Loss: 0.0781\n",
            "Epoch 176 | Step 0 | Loss: 0.0621\n",
            "Epoch 176 | Step 50 | Loss: 0.0830\n",
            "Epoch 176 | Step 100 | Loss: 0.0588\n",
            "Epoch 176 | Step 150 | Loss: 0.0572\n",
            "Epoch 177 | Step 0 | Loss: 0.0686\n",
            "Epoch 177 | Step 50 | Loss: 0.0557\n",
            "Epoch 177 | Step 100 | Loss: 0.0571\n",
            "Epoch 177 | Step 150 | Loss: 0.0426\n",
            "Epoch 178 | Step 0 | Loss: 0.0763\n",
            "Epoch 178 | Step 50 | Loss: 0.0672\n",
            "Epoch 178 | Step 100 | Loss: 0.0642\n",
            "Epoch 178 | Step 150 | Loss: 0.0820\n",
            "Epoch 179 | Step 0 | Loss: 0.0754\n",
            "Epoch 179 | Step 50 | Loss: 0.0690\n",
            "Epoch 179 | Step 100 | Loss: 0.0728\n",
            "Epoch 179 | Step 150 | Loss: 0.0687\n",
            "Epoch 180 | Step 0 | Loss: 0.0620\n",
            "Epoch 180 | Step 50 | Loss: 0.0665\n",
            "Epoch 180 | Step 100 | Loss: 0.0577\n",
            "Epoch 180 | Step 150 | Loss: 0.0557\n",
            "Epoch 181 | Step 0 | Loss: 0.0738\n",
            "Epoch 181 | Step 50 | Loss: 0.0601\n",
            "Epoch 181 | Step 100 | Loss: 0.0620\n",
            "Epoch 181 | Step 150 | Loss: 0.0649\n",
            "Epoch 182 | Step 0 | Loss: 0.0679\n",
            "Epoch 182 | Step 50 | Loss: 0.0617\n",
            "Epoch 182 | Step 100 | Loss: 0.0624\n",
            "Epoch 182 | Step 150 | Loss: 0.0664\n",
            "Epoch 183 | Step 0 | Loss: 0.0746\n",
            "Epoch 183 | Step 50 | Loss: 0.0581\n",
            "Epoch 183 | Step 100 | Loss: 0.0713\n",
            "Epoch 183 | Step 150 | Loss: 0.0661\n",
            "Epoch 184 | Step 0 | Loss: 0.0854\n",
            "Epoch 184 | Step 50 | Loss: 0.0943\n",
            "Epoch 184 | Step 100 | Loss: 0.0659\n",
            "Epoch 184 | Step 150 | Loss: 0.0814\n",
            "Epoch 185 | Step 0 | Loss: 0.0687\n",
            "Epoch 185 | Step 50 | Loss: 0.0721\n",
            "Epoch 185 | Step 100 | Loss: 0.0686\n",
            "Epoch 185 | Step 150 | Loss: 0.0654\n",
            "Epoch 186 | Step 0 | Loss: 0.0632\n",
            "Epoch 186 | Step 50 | Loss: 0.0824\n",
            "Epoch 186 | Step 100 | Loss: 0.0602\n",
            "Epoch 186 | Step 150 | Loss: 0.0815\n",
            "Epoch 187 | Step 0 | Loss: 0.0559\n",
            "Epoch 187 | Step 50 | Loss: 0.0714\n",
            "Epoch 187 | Step 100 | Loss: 0.0664\n",
            "Epoch 187 | Step 150 | Loss: 0.0713\n",
            "Epoch 188 | Step 0 | Loss: 0.0607\n",
            "Epoch 188 | Step 50 | Loss: 0.0635\n",
            "Epoch 188 | Step 100 | Loss: 0.0595\n",
            "Epoch 188 | Step 150 | Loss: 0.0757\n",
            "Epoch 189 | Step 0 | Loss: 0.0631\n",
            "Epoch 189 | Step 50 | Loss: 0.0745\n",
            "Epoch 189 | Step 100 | Loss: 0.0783\n",
            "Epoch 189 | Step 150 | Loss: 0.0819\n",
            "Epoch 190 | Step 0 | Loss: 0.0716\n",
            "Epoch 190 | Step 50 | Loss: 0.0658\n",
            "Epoch 190 | Step 100 | Loss: 0.0731\n",
            "Epoch 190 | Step 150 | Loss: 0.0647\n",
            "Epoch 191 | Step 0 | Loss: 0.0625\n",
            "Epoch 191 | Step 50 | Loss: 0.0594\n",
            "Epoch 191 | Step 100 | Loss: 0.0551\n",
            "Epoch 191 | Step 150 | Loss: 0.0683\n",
            "Epoch 192 | Step 0 | Loss: 0.0565\n",
            "Epoch 192 | Step 50 | Loss: 0.0620\n",
            "Epoch 192 | Step 100 | Loss: 0.0609\n",
            "Epoch 192 | Step 150 | Loss: 0.0636\n",
            "Epoch 193 | Step 0 | Loss: 0.0655\n",
            "Epoch 193 | Step 50 | Loss: 0.0663\n",
            "Epoch 193 | Step 100 | Loss: 0.0652\n",
            "Epoch 193 | Step 150 | Loss: 0.0679\n",
            "Epoch 194 | Step 0 | Loss: 0.0595\n",
            "Epoch 194 | Step 50 | Loss: 0.0769\n",
            "Epoch 194 | Step 100 | Loss: 0.0669\n",
            "Epoch 194 | Step 150 | Loss: 0.0662\n",
            "Epoch 195 | Step 0 | Loss: 0.0504\n",
            "Epoch 195 | Step 50 | Loss: 0.0722\n",
            "Epoch 195 | Step 100 | Loss: 0.0474\n",
            "Epoch 195 | Step 150 | Loss: 0.0526\n",
            "Epoch 196 | Step 0 | Loss: 0.0610\n",
            "Epoch 196 | Step 50 | Loss: 0.0623\n",
            "Epoch 196 | Step 100 | Loss: 0.0608\n",
            "Epoch 196 | Step 150 | Loss: 0.0662\n",
            "Epoch 197 | Step 0 | Loss: 0.0719\n",
            "Epoch 197 | Step 50 | Loss: 0.0629\n",
            "Epoch 197 | Step 100 | Loss: 0.0613\n",
            "Epoch 197 | Step 150 | Loss: 0.0593\n",
            "Epoch 198 | Step 0 | Loss: 0.0678\n",
            "Epoch 198 | Step 50 | Loss: 0.0688\n",
            "Epoch 198 | Step 100 | Loss: 0.0804\n",
            "Epoch 198 | Step 150 | Loss: 0.0751\n",
            "Epoch 199 | Step 0 | Loss: 0.0602\n",
            "Epoch 199 | Step 50 | Loss: 0.0744\n",
            "Epoch 199 | Step 100 | Loss: 0.0782\n",
            "Epoch 199 | Step 150 | Loss: 0.0736\n",
            "[INFO] Saved model at epoch 200\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from accelerate import Accelerator\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "# from dataset import FaceDataset  # Ensure this points to your custom dataset class\n",
        "\n",
        "# ----- Config -----\n",
        "data_path = \"/content/drive/MyDrive/GAI_HW5/selected_10000/\"\n",
        "model_ckpt_dir = \"/content/drive/MyDrive/GAI_HW5/model_v2\"\n",
        "os.makedirs(model_ckpt_dir, exist_ok=True)\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 200\n",
        "start_epoch = 0  # resume from epoch_50\n",
        "\n",
        "# ----- Accelerator & Device -----\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "device = accelerator.device\n",
        "\n",
        "# ----- Dataset & DataLoader -----\n",
        "dataset = FaceDataset(data_path)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# ----- Model -----\n",
        "model = UNet2DModel(\n",
        "    sample_size=64,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512, 512, 1024),\n",
        "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
        "    up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
        ")\n",
        "\n",
        "# ----- Scheduler & Optimizer -----\n",
        "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: min((step + 1) / 500, 1))\n",
        "\n",
        "# ----- Load checkpoint if exists -----\n",
        "# if os.path.exists(f\"{model_ckpt_dir}/epoch_{start_epoch}\"):\n",
        "#     print(f\"[INFO] Loading model from epoch {start_epoch}\")\n",
        "#     model = UNet2DModel.from_pretrained(f\"{model_ckpt_dir}/epoch_{start_epoch}\")\n",
        "#     model.to(device)\n",
        "#     if os.path.exists(f\"{model_ckpt_dir}/optimizer_epoch_{start_epoch}.pt\"):\n",
        "#         optimizer.load_state_dict(torch.load(f\"{model_ckpt_dir}/optimizer_epoch_{start_epoch}.pt\"))\n",
        "\n",
        "\n",
        "# ----- Accelerator prepare -----\n",
        "model, optimizer, dataloader, lr_scheduler = accelerator.prepare(model, optimizer, dataloader, lr_scheduler)\n",
        "\n",
        "# ----- Training loop -----\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        noise = torch.randn_like(batch)\n",
        "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch.size(0),), device=device).long()\n",
        "\n",
        "        noisy_images = scheduler.add_noise(batch, noise, timesteps)\n",
        "        noise_pred = model(noisy_images, timesteps).sample\n",
        "\n",
        "        loss = nn.functional.l1_loss(noise_pred, noise)  # L1 Loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Save model & optimizer\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "      if accelerator.is_main_process:\n",
        "          model.save_pretrained(f\"{model_ckpt_dir}/epoch_{epoch+1}\")\n",
        "          torch.save(optimizer.state_dict(), f\"{model_ckpt_dir}/optimizer_epoch_{epoch+1}.pt\")\n",
        "          torch.save(lr_scheduler.state_dict(), f\"{model_ckpt_dir}/lr_scheduler_epoch_{epoch+1}.pt\")\n",
        "          print(f\"[INFO] Saved model at epoch {epoch+1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5OlArEUnk7H",
        "outputId": "efccb074-3743-4133-b5c1-2d2c6441c7ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating images: 100%|██████████| 100/100 [3:39:54<00:00, 131.95s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Load your trained model ---\n",
        "model = UNet2DModel.from_pretrained(\"/content/drive/MyDrive/GAI_HW5/model_v2/epoch_200\").to(\"cuda\").eval()\n",
        "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "\n",
        "# --- Output directory ---\n",
        "os.makedirs(\"/content/drive/MyDrive/GAI_HW5/generated_images_v2\", exist_ok=True)\n",
        "\n",
        "# --- Generate 10,000 images in batches ---\n",
        "batch_size = 100\n",
        "total = 10000\n",
        "steps = total // batch_size\n",
        "\n",
        "for step in tqdm(range(steps), desc=\"Generating images\"):\n",
        "    x = torch.randn(batch_size, 3, 64, 64).to(\"cuda\")\n",
        "\n",
        "    for t in reversed(range(scheduler.config.num_train_timesteps)):\n",
        "        t_scalar = torch.tensor(t, dtype=torch.long, device=\"cpu\")  # for scheduler.step\n",
        "        t_tensor = torch.full((batch_size,), t, device=\"cuda\", dtype=torch.long)  # for model\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(x, t_tensor).sample\n",
        "        x = scheduler.step(noise_pred, t_scalar, x).prev_sample\n",
        "\n",
        "\n",
        "    # Post-process and save\n",
        "    x = (x.clamp(-1, 1) + 1) / 2  # [-1, 1] → [0, 1]\n",
        "    for i in range(batch_size):\n",
        "        save_image(x[i], f\"/content/drive/MyDrive/GAI_HW5/generated_images/{step * batch_size + i:05d}.png\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
